---
apiVersion: v1
items:
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:08Z"
  involvedObject:
    apiVersion: coordination.k8s.io/v1
    kind: Lease
    name: openshift-cluster-kube-scheduler-operator-lock
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5065"
    uid: a60d8eb8-7b12-4181-abe9-6c2b35d1c633
  kind: Event
  lastTimestamp: "2023-12-06T09:21:08Z"
  message: openshift-kube-scheduler-operator-6d599fd966-qznfb_1ae09f01-941c-4477-b367-6e553bea533c
    became leader
  metadata:
    creationTimestamp: "2023-12-06T09:21:08Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:08Z"
    name: openshift-cluster-kube-scheduler-operator-lock.179e339f63cb2060
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5066"
    uid: 4eed6df5-d168-4b21-b123-c7790f96afc3
  reason: LeaderElection
  reportingComponent: openshift-cluster-kube-scheduler-operator
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:25Z"
  involvedObject:
    apiVersion: coordination.k8s.io/v1
    kind: Lease
    name: openshift-cluster-kube-scheduler-operator-lock
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "10473"
    uid: a60d8eb8-7b12-4181-abe9-6c2b35d1c633
  kind: Event
  lastTimestamp: "2023-12-06T09:22:25Z"
  message: openshift-kube-scheduler-operator-6d599fd966-qznfb_6ec36d81-7315-49c3-a5c0-8ccdd36d53f1
    became leader
  metadata:
    creationTimestamp: "2023-12-06T09:22:25Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:22:25Z"
    name: openshift-cluster-kube-scheduler-operator-lock.179e33b1214e524b
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "10474"
    uid: cb5c2599-0cb2-428a-8e16-d90d38571128
  reason: LeaderElection
  reportingComponent: openshift-cluster-kube-scheduler-operator
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:18:45Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: openshift-kube-scheduler-operator-6d599fd966-qznfb
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "2535"
    uid: 8de2c05b-b3de-43ce-8a04-5f8ad4b10eb8
  kind: Event
  lastTimestamp: "2023-12-06T09:18:45Z"
  message: '0/2 nodes are available: 2 node(s) had untolerated taint {node.cloudprovider.kubernetes.io/uninitialized:
    true}. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..'
  metadata:
    creationTimestamp: "2023-12-06T09:18:45Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: kube-scheduler
      operation: Update
      time: "2023-12-06T09:18:45Z"
    name: openshift-kube-scheduler-operator-6d599fd966-qznfb.179e337e042284a5
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "2538"
    uid: 4dbe39a7-4f31-4a24-af2d-ea044c3ab5bc
  reason: FailedScheduling
  reportingComponent: default-scheduler
  reportingInstance: ""
  source:
    component: default-scheduler
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:20:49Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: openshift-kube-scheduler-operator-6d599fd966-qznfb
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "2539"
    uid: 8de2c05b-b3de-43ce-8a04-5f8ad4b10eb8
  kind: Event
  lastTimestamp: "2023-12-06T09:20:49Z"
  message: Successfully assigned openshift-kube-scheduler-operator/openshift-kube-scheduler-operator-6d599fd966-qznfb
    to ip-10-0-21-63.us-west-1.compute.internal
  metadata:
    creationTimestamp: "2023-12-06T09:20:49Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: kube-scheduler
      operation: Update
      time: "2023-12-06T09:20:49Z"
    name: openshift-kube-scheduler-operator-6d599fd966-qznfb.179e339ae08e7365
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "4452"
    uid: 21304108-ad2c-4499-9dc9-702363098e3f
  reason: Scheduled
  reportingComponent: default-scheduler
  reportingInstance: ""
  source:
    component: default-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:20:50Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: openshift-kube-scheduler-operator-6d599fd966-qznfb
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "4408"
    uid: 8de2c05b-b3de-43ce-8a04-5f8ad4b10eb8
  kind: Event
  lastTimestamp: "2023-12-06T09:20:50Z"
  message: Add eth0 [10.129.0.18/23] from ovn-kubernetes
  metadata:
    creationTimestamp: "2023-12-06T09:20:50Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: multus-daemon
      operation: Update
      time: "2023-12-06T09:20:50Z"
    name: openshift-kube-scheduler-operator-6d599fd966-qznfb.179e339b26518e6d
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "4529"
    uid: 859a9ffe-bd74-4751-a164-c107cd598d71
  reason: AddedInterface
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: multus
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:20:50Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-scheduler-operator-container}
    kind: Pod
    name: openshift-kube-scheduler-operator-6d599fd966-qznfb
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "4378"
    uid: 8de2c05b-b3de-43ce-8a04-5f8ad4b10eb8
  kind: Event
  lastTimestamp: "2023-12-06T09:20:50Z"
  message: Pulling image "registry.build03.ci.openshift.org/ci-op-2j285qtr/stable@sha256:f0931615ccf5c633bbe370a21f3a97d4285858f81085c8ea3c21a053a0236e29"
  metadata:
    creationTimestamp: "2023-12-06T09:20:50Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:reportingInstance: {}
        f:source:
          f:component: {}
          f:host: {}
        f:type: {}
      manager: kubelet
      operation: Update
      time: "2023-12-06T09:20:50Z"
    name: openshift-kube-scheduler-operator-6d599fd966-qznfb.179e339b27a7b7c4
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "4532"
    uid: 6904cddb-52d6-428f-bd24-d4cb08efc423
  reason: Pulling
  reportingComponent: kubelet
  reportingInstance: ip-10-0-21-63.us-west-1.compute.internal
  source:
    component: kubelet
    host: ip-10-0-21-63.us-west-1.compute.internal
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:03Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-scheduler-operator-container}
    kind: Pod
    name: openshift-kube-scheduler-operator-6d599fd966-qznfb
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "4378"
    uid: 8de2c05b-b3de-43ce-8a04-5f8ad4b10eb8
  kind: Event
  lastTimestamp: "2023-12-06T09:21:03Z"
  message: Successfully pulled image "registry.build03.ci.openshift.org/ci-op-2j285qtr/stable@sha256:f0931615ccf5c633bbe370a21f3a97d4285858f81085c8ea3c21a053a0236e29"
    in 13.117s (13.117s including waiting)
  metadata:
    creationTimestamp: "2023-12-06T09:21:03Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:reportingInstance: {}
        f:source:
          f:component: {}
          f:host: {}
        f:type: {}
      manager: kubelet
      operation: Update
      time: "2023-12-06T09:21:03Z"
    name: openshift-kube-scheduler-operator-6d599fd966-qznfb.179e339e35835902
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "4726"
    uid: 48c3f7f4-bc43-4aae-8605-b2b00cf58498
  reason: Pulled
  reportingComponent: kubelet
  reportingInstance: ip-10-0-21-63.us-west-1.compute.internal
  source:
    component: kubelet
    host: ip-10-0-21-63.us-west-1.compute.internal
  type: Normal
- apiVersion: v1
  count: 2
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:06Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-scheduler-operator-container}
    kind: Pod
    name: openshift-kube-scheduler-operator-6d599fd966-qznfb
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "4378"
    uid: 8de2c05b-b3de-43ce-8a04-5f8ad4b10eb8
  kind: Event
  lastTimestamp: "2023-12-06T09:22:24Z"
  message: Created container kube-scheduler-operator-container
  metadata:
    creationTimestamp: "2023-12-06T09:21:06Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:reportingInstance: {}
        f:source:
          f:component: {}
          f:host: {}
        f:type: {}
      manager: kubelet
      operation: Update
      time: "2023-12-06T09:22:24Z"
    name: openshift-kube-scheduler-operator-6d599fd966-qznfb.179e339ede61f6ee
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "10464"
    uid: 6644e4f6-6ac0-4c32-868a-a8cc087bc308
  reason: Created
  reportingComponent: kubelet
  reportingInstance: ip-10-0-21-63.us-west-1.compute.internal
  source:
    component: kubelet
    host: ip-10-0-21-63.us-west-1.compute.internal
  type: Normal
- apiVersion: v1
  count: 2
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:06Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-scheduler-operator-container}
    kind: Pod
    name: openshift-kube-scheduler-operator-6d599fd966-qznfb
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "4378"
    uid: 8de2c05b-b3de-43ce-8a04-5f8ad4b10eb8
  kind: Event
  lastTimestamp: "2023-12-06T09:22:24Z"
  message: Started container kube-scheduler-operator-container
  metadata:
    creationTimestamp: "2023-12-06T09:21:06Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:reportingInstance: {}
        f:source:
          f:component: {}
          f:host: {}
        f:type: {}
      manager: kubelet
      operation: Update
      time: "2023-12-06T09:22:24Z"
    name: openshift-kube-scheduler-operator-6d599fd966-qznfb.179e339ee3e1d023
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "10468"
    uid: af0af1a9-5b25-455c-adad-624c6c7cca33
  reason: Started
  reportingComponent: kubelet
  reportingInstance: ip-10-0-21-63.us-west-1.compute.internal
  source:
    component: kubelet
    host: ip-10-0-21-63.us-west-1.compute.internal
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:07Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: openshift-kube-scheduler-operator-6d599fd966-qznfb
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "4593"
    uid: 8de2c05b-b3de-43ce-8a04-5f8ad4b10eb8
  kind: Event
  lastTimestamp: "2023-12-06T09:21:07Z"
  message: 'failed to add remote pod openshift-kube-scheduler-operator/openshift-kube-scheduler-operator-6d599fd966-qznfb
    to namespace: failed add ips to address set default-network-controller:Namespace:openshift-kube-scheduler-operator:
    (error in transact with ops [{Op:mutate Table:Address_Set Row:map[] Rows:[] Columns:[]
    Mutations:[{Column:addresses Mutator:insert Value:{GoSet:[10.129.0.18]}}] Timeout:<nil>
    Where:[where column _uuid == {84c83901-c68f-43ab-b747-1efcb0831224}] Until: Durable:<nil>
    Comment:<nil> Lock:<nil> UUID: UUIDName:}]: context deadline exceeded: while awaiting
    reconnection)'
  metadata:
    creationTimestamp: "2023-12-06T09:21:07Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: ip-10-0-106-212
      operation: Update
      time: "2023-12-06T09:21:07Z"
    name: openshift-kube-scheduler-operator-6d599fd966-qznfb.179e339f17c510fa
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "4972"
    uid: cf75f192-5481-46e1-a1d5-3f5ea0456a2a
  reason: ErrorUpdatingResource
  reportingComponent: controlplane
  reportingInstance: ""
  source:
    component: controlplane
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:24Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-scheduler-operator-container}
    kind: Pod
    name: openshift-kube-scheduler-operator-6d599fd966-qznfb
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "4378"
    uid: 8de2c05b-b3de-43ce-8a04-5f8ad4b10eb8
  kind: Event
  lastTimestamp: "2023-12-06T09:22:24Z"
  message: Container image "registry.build03.ci.openshift.org/ci-op-2j285qtr/stable@sha256:f0931615ccf5c633bbe370a21f3a97d4285858f81085c8ea3c21a053a0236e29"
    already present on machine
  metadata:
    creationTimestamp: "2023-12-06T09:22:24Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:reportingInstance: {}
        f:source:
          f:component: {}
          f:host: {}
        f:type: {}
      manager: kubelet
      operation: Update
      time: "2023-12-06T09:22:24Z"
    name: openshift-kube-scheduler-operator-6d599fd966-qznfb.179e33b0ecc2d3f2
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "10447"
    uid: 31bf4483-a418-46cf-95de-2263a474aa50
  reason: Pulled
  reportingComponent: kubelet
  reportingInstance: ip-10-0-21-63.us-west-1.compute.internal
  source:
    component: kubelet
    host: ip-10-0-21-63.us-west-1.compute.internal
  type: Normal
- apiVersion: v1
  count: 13
  eventTime: null
  firstTimestamp: "2023-12-06T09:18:04Z"
  involvedObject:
    apiVersion: apps/v1
    kind: ReplicaSet
    name: openshift-kube-scheduler-operator-6d599fd966
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "1389"
    uid: cbd6cfa4-e1d7-4a13-a4a9-53ec3fd7976e
  kind: Event
  lastTimestamp: "2023-12-06T09:18:25Z"
  message: 'Error creating: pods "openshift-kube-scheduler-operator-6d599fd966-" is
    forbidden: autoscaling.openshift.io/ManagementCPUsOverride the cluster does not
    have any nodes'
  metadata:
    creationTimestamp: "2023-12-06T09:18:04Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: kube-controller-manager
      operation: Update
      time: "2023-12-06T09:18:25Z"
    name: openshift-kube-scheduler-operator-6d599fd966.179e3374796921e5
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "2271"
    uid: 576bfaf4-8047-43d4-b4e4-d5b29325ebc2
  reason: FailedCreate
  reportingComponent: replicaset-controller
  reportingInstance: ""
  source:
    component: replicaset-controller
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:18:45Z"
  involvedObject:
    apiVersion: apps/v1
    kind: ReplicaSet
    name: openshift-kube-scheduler-operator-6d599fd966
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "1393"
    uid: cbd6cfa4-e1d7-4a13-a4a9-53ec3fd7976e
  kind: Event
  lastTimestamp: "2023-12-06T09:18:45Z"
  message: 'Created pod: openshift-kube-scheduler-operator-6d599fd966-qznfb'
  metadata:
    creationTimestamp: "2023-12-06T09:18:45Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: kube-controller-manager
      operation: Update
      time: "2023-12-06T09:18:45Z"
    name: openshift-kube-scheduler-operator-6d599fd966.179e337e0412dba8
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "2536"
    uid: 23bef82f-0459-4019-9758-60ed69b52d05
  reason: SuccessfulCreate
  reportingComponent: replicaset-controller
  reportingInstance: ""
  source:
    component: replicaset-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:18:04Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "1388"
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:18:04Z"
  message: Scaled up replica set openshift-kube-scheduler-operator-6d599fd966 to 1
  metadata:
    creationTimestamp: "2023-12-06T09:18:04Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: kube-controller-manager
      operation: Update
      time: "2023-12-06T09:18:04Z"
    name: openshift-kube-scheduler-operator.179e3374792ce491
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "1390"
    uid: 4bcedd56-74ea-465e-960a-c74edf134d8d
  reason: ScalingReplicaSet
  reportingComponent: deployment-controller
  reportingInstance: ""
  source:
    component: deployment-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:08Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:08Z"
  message: FeatureGates updated to featuregates.Features{Enabled:[]v1.FeatureGateName{"AlibabaPlatform",
    "AzureWorkloadIdentity", "BuildCSIVolumes", "CloudDualStackNodeIPs", "ExternalCloudProvider",
    "ExternalCloudProviderAzure", "ExternalCloudProviderExternal", "ExternalCloudProviderGCP",
    "OpenShiftPodSecurityAdmission", "PrivateHostedZoneAWS"}, Disabled:[]v1.FeatureGateName{"AdminNetworkPolicy",
    "AutomatedEtcdBackup", "CSIDriverSharedResource", "ClusterAPIInstall", "DNSNameResolver",
    "DisableKubeletCloudCredentialProviders", "DynamicResourceAllocation", "EventedPLEG",
    "GCPClusterHostedDNS", "GCPLabelsTags", "GatewayAPI", "InsightsConfigAPI", "InstallAlternateInfrastructureAWS",
    "MachineAPIOperatorDisableMachineHealthCheckController", "MachineAPIProviderOpenStack",
    "MachineConfigNodes", "ManagedBootImages", "MaxUnavailableStatefulSet", "MetricsServer",
    "MixedCPUsAllocation", "NetworkLiveMigration", "NodeSwap", "RouteExternalCertificate",
    "SigstoreImageVerification", "VSphereControlPlaneMachineSet", "VSphereStaticIPs",
    "ValidatingAdmissionPolicy"}}
  metadata:
    creationTimestamp: "2023-12-06T09:21:08Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:08Z"
    name: openshift-kube-scheduler-operator.179e339f64237325
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5070"
    uid: 59ecb272-fdff-4cd8-a77d-2c704bae5ab1
  reason: FeatureGatesInitialized
  reportingComponent: openshift-cluster-kube-scheduler-operator
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:09Z"
  message: clusteroperator/kube-scheduler version "raw-internal" changed from "" to
    "4.15.0-0.ci.test-2023-12-06-090630-ci-op-2j285qtr-latest"
  metadata:
    creationTimestamp: "2023-12-06T09:21:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:09Z"
    name: openshift-kube-scheduler-operator.179e339f6c437902
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5084"
    uid: c9066e24-9b3d-4833-9726-d32c960b0d29
  reason: OperatorVersionChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:09Z"
  message: Observed new master node ip-10-0-21-63.us-west-1.compute.internal
  metadata:
    creationTimestamp: "2023-12-06T09:21:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:09Z"
    name: openshift-kube-scheduler-operator.179e339f6c57ef7e
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5083"
    uid: 1fe523ed-ddea-4b21-8388-fe8e097fb554
  reason: MasterNodeObserved
  reportingComponent: openshift-cluster-kube-scheduler-operator-nodecontroller
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-nodecontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:09Z"
  message: Observed new master node ip-10-0-94-160.us-west-1.compute.internal
  metadata:
    creationTimestamp: "2023-12-06T09:21:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:09Z"
    name: openshift-kube-scheduler-operator.179e339f6c580a5e
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5088"
    uid: 30af7a46-ee05-47a0-9e8d-5d93a4a9d4d2
  reason: MasterNodeObserved
  reportingComponent: openshift-cluster-kube-scheduler-operator-nodecontroller
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-nodecontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:09Z"
  message: Observed new master node ip-10-0-106-212.us-west-1.compute.internal
  metadata:
    creationTimestamp: "2023-12-06T09:21:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:09Z"
    name: openshift-kube-scheduler-operator.179e339f6c5814e0
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5100"
    uid: 6a85a62a-84b3-4e6c-8050-330ebfaa9a1d
  reason: MasterNodeObserved
  reportingComponent: openshift-cluster-kube-scheduler-operator-nodecontroller
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-nodecontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:09Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded set to Unknown
    (""),Progressing set to Unknown (""),Available set to Unknown (""),Upgradeable
    set to Unknown (""),status.relatedObjects changed from [{"operator.openshift.io"
    "kubeschedulers" "" "cluster"} {"config.openshift.io" "schedulers" "" ""} {""
    "namespaces" "" "openshift-config"} {"" "namespaces" "" "openshift-config-managed"}
    {"" "namespaces" "" "openshift-kube-scheduler-operator"} {"" "namespaces" "" "openshift-kube-scheduler"}
    {"controlplane.operator.openshift.io" "podnetworkconnectivitychecks" "openshift-kube-scheduler"
    ""}] to [{"operator.openshift.io" "kubeschedulers" "" "cluster"} {"config.openshift.io"
    "schedulers" "" ""} {"" "namespaces" "" "openshift-config"} {"" "namespaces" ""
    "openshift-config-managed"} {"" "namespaces" "" "openshift-kube-scheduler"} {""
    "namespaces" "" "openshift-kube-scheduler-operator"} {"controlplane.operator.openshift.io"
    "podnetworkconnectivitychecks" "openshift-kube-apiserver" ""}],status.versions
    changed from [] to [{"raw-internal" "4.15.0-0.ci.test-2023-12-06-090630-ci-op-2j285qtr-latest"}]'
  metadata:
    creationTimestamp: "2023-12-06T09:21:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:09Z"
    name: openshift-kube-scheduler-operator.179e339f6e4843a7
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5090"
    uid: 42d97b67-3562-4bf6-b829-eb3133d585f1
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:09Z"
  message: minTLSVersion changed to VersionTLS12
  metadata:
    creationTimestamp: "2023-12-06T09:21:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:09Z"
    name: openshift-kube-scheduler-operator.179e339f731f8759
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5102"
    uid: c89119cb-6325-4517-9544-8f093459cf80
  reason: ObserveTLSSecurityProfile
  reportingComponent: openshift-cluster-kube-scheduler-operator-config-observer-configobserver
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-config-observer-configobserver
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:09Z"
  message: cipherSuites changed to ["TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256" "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256"
    "TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384" "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384"
    "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256" "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256"]
  metadata:
    creationTimestamp: "2023-12-06T09:21:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:09Z"
    name: openshift-kube-scheduler-operator.179e339f73205bf9
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5108"
    uid: a9b67d2f-84b1-4996-baa7-659cc3c07e09
  reason: ObserveTLSSecurityProfile
  reportingComponent: openshift-cluster-kube-scheduler-operator-config-observer-configobserver
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-config-observer-configobserver
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:09Z"
  message: "Writing updated observed config:   map[string]any{\n+ \t\"servingInfo\":
    map[string]any{\n+ \t\t\"cipherSuites\": []any{\n+ \t\t\tstring(\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\"),\n+ \t\t\tstring(\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\"),\n+ \t\t\tstring(\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\"),\n+ \t\t\tstring(\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\"),\n+ \t\t\tstring(\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\"),\n+ \t\t\tstring(\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"),\n+ \t\t},\n+ \t\t\"minTLSVersion\":
    string(\"VersionTLS12\"),\n+ \t},\n  }\n"
  metadata:
    creationTimestamp: "2023-12-06T09:21:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:09Z"
    name: openshift-kube-scheduler-operator.179e339f7324093f
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5112"
    uid: 7b21c923-b505-4472-a944-764835b6cc3c
  reason: ObservedConfigChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-config-observer-configobserver
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-config-observer-configobserver
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:09Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded changed from
    Unknown to False ("NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)")'
  metadata:
    creationTimestamp: "2023-12-06T09:21:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:09Z"
    name: openshift-kube-scheduler-operator.179e339f752333bd
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5111"
    uid: 73c762f7-0fcd-473d-905e-7f9accb85b9f
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:09Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)" to "NodeControllerDegraded: The master nodes
    not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\" not ready since
    2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container runtime network
    not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin
    returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your
    network provider started?), node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:09Z"
    name: openshift-kube-scheduler-operator.179e339f76dca090
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5116"
    uid: 55dcdf3a-0c26-4dd0-bf18-f4fc7e516f82
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 2
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:11Z"
  message: 'secrets: kube-scheduler-client-cert-key, configmaps: config-0,kube-scheduler-cert-syncer-kubeconfig-0,kube-scheduler-pod-0,scheduler-kubeconfig-0,serviceaccount-ca-0,
    secrets: localhost-recovery-client-token-0'
  metadata:
    creationTimestamp: "2023-12-06T09:21:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:11Z"
    name: openshift-kube-scheduler-operator.179e339f78bb76f6
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5645"
    uid: 74f7d4c7-4453-43aa-8986-6f9795a8fd15
  reason: RequiredInstallerResourcesMissing
  reportingComponent: openshift-cluster-kube-scheduler-operator-installer-controller
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-installer-controller
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:09Z"
  message: Created PodDisruptionBudget.policy/openshift-kube-scheduler-guard-pdb -n
    openshift-kube-scheduler because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:09Z"
    name: openshift-kube-scheduler-operator.179e339f798aea96
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5184"
    uid: f4126ac3-522d-48ab-b1d4-81191a8c11e3
  reason: PodDisruptionBudgetCreated
  reportingComponent: openshift-cluster-kube-scheduler-operator-guardcontroller
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-guardcontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:09Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)" to "NodeControllerDegraded: The master nodes
    not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\" not ready since
    2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container runtime network
    not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin
    returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/. Has your
    network provider started?), node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:09Z"
    name: openshift-kube-scheduler-operator.179e339f971bfc0f
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5218"
    uid: f82f26e7-a29d-4ba5-ac8f-d2e10bfbc2e6
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:10Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:10Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Upgradeable changed
    from Unknown to True ("All is well")'
  metadata:
    creationTimestamp: "2023-12-06T09:21:10Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:10Z"
    name: openshift-kube-scheduler-operator.179e339fbc490461
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5376"
    uid: 6f3ea7b0-5661-4577-bd28-a2283401f412
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:11Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:11Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal" to "NodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal\nRevisionControllerDegraded:
    configmaps \"kube-scheduler-pod\" not found"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:11Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:11Z"
    name: openshift-kube-scheduler-operator.179e339fea15f51c
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5573"
    uid: f6446d0f-e9a2-4d60-bf30-e7c73b721149
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:11Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:11Z"
  message: Updated Namespace/openshift-kube-scheduler because it changed
  metadata:
    creationTimestamp: "2023-12-06T09:21:11Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:11Z"
    name: openshift-kube-scheduler-operator.179e33a00d0b0045
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5659"
    uid: 6a6f1d31-67ac-4bf6-8ef4-11ed014fbdbc
  reason: NamespaceUpdated
  reportingComponent: openshift-cluster-kube-scheduler-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:11Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:11Z"
  message: Created ConfigMap/config -n openshift-kube-scheduler because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:11Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:11Z"
    name: openshift-kube-scheduler-operator.179e33a018efccbe
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5672"
    uid: ac73a376-45e6-496a-a6fe-24d5c1732204
  reason: ConfigMapCreated
  reportingComponent: openshift-cluster-kube-scheduler-operator-targetconfigcontroller
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-targetconfigcontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:12Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:12Z"
  message: Created ServiceAccount/installer-sa -n openshift-kube-scheduler because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:12Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:12Z"
    name: openshift-kube-scheduler-operator.179e33a024ce146d
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5699"
    uid: 23ff1522-bcdd-40b3-9be7-be62929c55e5
  reason: ServiceAccountCreated
  reportingComponent: openshift-cluster-kube-scheduler-operator-backingresourcecontroller-backingresourcecontroller
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-backingresourcecontroller-backingresourcecontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:12Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:12Z"
  message: Created ClusterRoleBinding.rbac.authorization.k8s.io/system:openshift:operator:openshift-kube-scheduler-installer
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:12Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:12Z"
    name: openshift-kube-scheduler-operator.179e33a025642442
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5701"
    uid: 115258af-2ee1-4d32-959e-b86e154f3e1a
  reason: ClusterRoleBindingCreated
  reportingComponent: openshift-cluster-kube-scheduler-operator-backingresourcecontroller-backingresourcecontroller
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-backingresourcecontroller-backingresourcecontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:12Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:12Z"
  message: Created ConfigMap/kube-scheduler-cert-syncer-kubeconfig -n openshift-kube-scheduler
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:12Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:12Z"
    name: openshift-kube-scheduler-operator.179e33a03ccf66e8
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5741"
    uid: 7bd8d665-fef3-458c-9d68-ca0d2138c932
  reason: ConfigMapCreated
  reportingComponent: openshift-cluster-kube-scheduler-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:12Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:12Z"
  message: Created RoleBinding.rbac.authorization.k8s.io/system:openshift:leader-locking-kube-scheduler
    -n kube-system because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:12Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:12Z"
    name: openshift-kube-scheduler-operator.179e33a03da8a71c
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5747"
    uid: 7400eaf8-41b7-4406-a1c8-adfa966f9a9b
  reason: RoleBindingCreated
  reportingComponent: openshift-cluster-kube-scheduler-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:12Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:12Z"
  message: Created ClusterRoleBinding.rbac.authorization.k8s.io/system:openshift:operator:kube-scheduler:public-2
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:12Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:12Z"
    name: openshift-kube-scheduler-operator.179e33a03dfc8e04
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5751"
    uid: 29b740c5-b559-44e6-8674-417cd6ca98ba
  reason: ClusterRoleBindingCreated
  reportingComponent: openshift-cluster-kube-scheduler-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:12Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:12Z"
  message: Created Role.rbac.authorization.k8s.io/system:openshift:sa-listing-configmaps
    -n openshift-kube-scheduler because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:12Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:12Z"
    name: openshift-kube-scheduler-operator.179e33a03e5c2a7e
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5754"
    uid: 56aa165f-e7f6-462e-a5fa-a0818aa7e1e0
  reason: RoleCreated
  reportingComponent: openshift-cluster-kube-scheduler-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:12Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:12Z"
  message: Created RoleBinding.rbac.authorization.k8s.io/system:openshift:sa-listing-configmaps
    -n openshift-kube-scheduler because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:12Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:12Z"
    name: openshift-kube-scheduler-operator.179e33a03ee1d082
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5757"
    uid: 9f691207-7aba-4e3b-8c93-b67fde51482f
  reason: RoleBindingCreated
  reportingComponent: openshift-cluster-kube-scheduler-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:13Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:13Z"
  message: Created ConfigMap/serviceaccount-ca -n openshift-kube-scheduler because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:13Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:13Z"
    name: openshift-kube-scheduler-operator.179e33a06c5c1a51
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5855"
    uid: 7952c98e-889f-4b88-8e57-d655c1f52bae
  reason: ConfigMapCreated
  reportingComponent: openshift-cluster-kube-scheduler-operator-targetconfigcontroller
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-targetconfigcontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:13Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:13Z"
  message: Created Service/scheduler -n openshift-kube-scheduler because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:13Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:13Z"
    name: openshift-kube-scheduler-operator.179e33a0905f8a8a
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "5944"
    uid: 3e296590-1ed5-44d6-ae42-c3c053123c02
  reason: ServiceCreated
  reportingComponent: openshift-cluster-kube-scheduler-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:15Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:15Z"
  message: Created ServiceAccount/openshift-kube-scheduler-sa -n openshift-kube-scheduler
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:15Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:15Z"
    name: openshift-kube-scheduler-operator.179e33a0ef656bf4
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "6120"
    uid: 3d12a14b-131a-4728-87c7-aa94c4bcf30f
  reason: ServiceAccountCreated
  reportingComponent: openshift-cluster-kube-scheduler-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:15Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:15Z"
  message: Created ClusterRoleBinding.rbac.authorization.k8s.io/system:openshift:operator:kube-scheduler-recovery
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:15Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:15Z"
    name: openshift-kube-scheduler-operator.179e33a0efbdc738
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "6122"
    uid: fef8a223-7cb4-4c97-9c1e-df9823fbd5b7
  reason: ClusterRoleBindingCreated
  reportingComponent: openshift-cluster-kube-scheduler-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 22
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:15Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:32Z"
  message: 'secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1'
  metadata:
    creationTimestamp: "2023-12-06T09:21:15Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:32Z"
    name: openshift-kube-scheduler-operator.179e33a0efcf81a6
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "7440"
    uid: 9b18eb23-76da-47a4-8eea-6123f491e7fe
  reason: RequiredInstallerResourcesMissing
  reportingComponent: openshift-cluster-kube-scheduler-operator-installer-controller
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-installer-controller
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:15Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:15Z"
  message: Created ConfigMap/scheduler-kubeconfig -n openshift-kube-scheduler because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:15Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:15Z"
    name: openshift-kube-scheduler-operator.179e33a0fba625a2
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "6152"
    uid: e864b491-cb8d-49da-a007-8ed091e2598e
  reason: ConfigMapCreated
  reportingComponent: openshift-cluster-kube-scheduler-operator-targetconfigcontroller
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-targetconfigcontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:15Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:15Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal\nRevisionControllerDegraded:
    configmaps \"kube-scheduler-pod\" not found" to "NodeControllerDegraded: The master
    nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\" not ready
    since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container runtime
    network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal\nRevisionControllerDegraded:
    configmaps \"kube-scheduler-pod\" not found"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:15Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:15Z"
    name: openshift-kube-scheduler-operator.179e33a0fc63fee6
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "6159"
    uid: 2a3dc476-2c2f-4159-9ad9-0a73a2b7ceec
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:16Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:16Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal\nRevisionControllerDegraded:
    configmaps \"kube-scheduler-pod\" not found" to "NodeControllerDegraded: The master
    nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\" not ready
    since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container runtime
    network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal\nRevisionControllerDegraded:
    configmaps \"kube-scheduler-pod\" not found"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:16Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:16Z"
    name: openshift-kube-scheduler-operator.179e33a11435320d
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "6252"
    uid: a61d848d-e11a-48bd-9841-efeb2c45b081
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:16Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:16Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal\nRevisionControllerDegraded:
    configmaps \"kube-scheduler-pod\" not found" to "NodeControllerDegraded: The master
    nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\" not ready
    since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container runtime
    network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal\nRevisionControllerDegraded:
    configmaps \"kube-scheduler-pod\" not found\nInstallerControllerDegraded: missing
    required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]",Progressing changed from Unknown
    to True ("NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved
    new revision 1"),Available changed from Unknown to False ("StaticPodsAvailable:
    0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision
    1")'
  metadata:
    creationTimestamp: "2023-12-06T09:21:16Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:16Z"
    name: openshift-kube-scheduler-operator.179e33a147979698
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "6335"
    uid: 4546aa71-084c-4f5e-bfa1-f4a52cb36216
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:17Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:17Z"
  message: Created ServiceAccount/localhost-recovery-client -n openshift-kube-scheduler
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:17Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:17Z"
    name: openshift-kube-scheduler-operator.179e33a17e7acad7
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "6468"
    uid: 485f56f1-7051-4df2-8f6e-2030e9607e62
  reason: ServiceAccountCreated
  reportingComponent: openshift-cluster-kube-scheduler-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:19Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:19Z"
  message: Created Secret/localhost-recovery-client-token -n openshift-kube-scheduler
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:19Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:19Z"
    name: openshift-kube-scheduler-operator.179e33a1c6086c21
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "6624"
    uid: ed83083a-dca0-4d37-a266-461be299f53f
  reason: SecretCreated
  reportingComponent: openshift-cluster-kube-scheduler-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:19Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:19Z"
  message: Created ConfigMap/kube-scheduler-pod -n openshift-kube-scheduler because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:19Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:19Z"
    name: openshift-kube-scheduler-operator.179e33a1d1f4ae3c
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "6648"
    uid: 0302bfc2-2c90-44ec-ae1e-45e0d6978ea4
  reason: ConfigMapCreated
  reportingComponent: openshift-cluster-kube-scheduler-operator-targetconfigcontroller
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-targetconfigcontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:19Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:19Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal\nRevisionControllerDegraded:
    configmaps \"kube-scheduler-pod\" not found\nInstallerControllerDegraded: missing
    required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]" to "NodeControllerDegraded: The master
    nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\" not ready
    since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container runtime
    network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal\nRevisionControllerDegraded:
    configmaps \"kube-scheduler-pod\" not found\nInstallerControllerDegraded: missing
    required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nTargetConfigControllerDegraded: \"serviceaccount/localhost-recovery-client\":
    serviceaccounts \"localhost-recovery-client\" not found"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:19Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:19Z"
    name: openshift-kube-scheduler-operator.179e33a1d2dad951
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "6653"
    uid: dfd75daa-dc8e-4858-8701-8afd6709bcfb
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:20Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:20Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal\nRevisionControllerDegraded:
    configmaps \"kube-scheduler-pod\" not found\nInstallerControllerDegraded: missing
    required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nTargetConfigControllerDegraded: \"serviceaccount/localhost-recovery-client\":
    serviceaccounts \"localhost-recovery-client\" not found" to "NodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal\nRevisionControllerDegraded:
    configmaps \"kube-scheduler-pod\" not found\nInstallerControllerDegraded: missing
    required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nTargetConfigControllerDegraded: \"serviceaccount/localhost-recovery-client\":
    serviceaccounts \"localhost-recovery-client\" not found"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:20Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:20Z"
    name: openshift-kube-scheduler-operator.179e33a1fd410bdd
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "6712"
    uid: 3341bf43-e620-42f4-9656-ffdfa94f7187
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:20Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:20Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal\nRevisionControllerDegraded:
    configmaps \"kube-scheduler-pod\" not found\nInstallerControllerDegraded: missing
    required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nTargetConfigControllerDegraded: \"serviceaccount/localhost-recovery-client\":
    serviceaccounts \"localhost-recovery-client\" not found" to "NodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal\nRevisionControllerDegraded:
    configmaps \"kube-scheduler-pod\" not found\nInstallerControllerDegraded: missing
    required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nTargetConfigControllerDegraded: \"serviceaccount/localhost-recovery-client\":
    serviceaccounts \"localhost-recovery-client\" not found"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:20Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:20Z"
    name: openshift-kube-scheduler-operator.179e33a1fdccf89f
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "6715"
    uid: caead54f-5f1f-469c-8e37-8a6bde917155
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 26
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:20Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:22:22Z"
  message: 'Failed to create ConfigMap/kube-scheduler-pod-2 -n openshift-kube-scheduler:
    ConfigMap "kube-scheduler-pod-2" is invalid: metadata.ownerReferences.uid: Invalid
    value: "": uid must not be empty'
  metadata:
    creationTimestamp: "2023-12-06T09:21:20Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:22:22Z"
    name: openshift-kube-scheduler-operator.179e33a2313b1f47
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "10401"
    uid: 2ac020ed-c350-49c6-bb14-2fa8df91540f
  reason: ConfigMapCreateFailed
  reportingComponent: openshift-cluster-kube-scheduler-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-revisioncontroller
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:20Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:20Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal\nRevisionControllerDegraded:
    configmaps \"kube-scheduler-pod\" not found\nInstallerControllerDegraded: missing
    required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nTargetConfigControllerDegraded: \"serviceaccount/localhost-recovery-client\":
    serviceaccounts \"localhost-recovery-client\" not found" to "NodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nTargetConfigControllerDegraded: \"serviceaccount/localhost-recovery-client\":
    serviceaccounts \"localhost-recovery-client\" not found"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:20Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:20Z"
    name: openshift-kube-scheduler-operator.179e33a23231029a
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "6807"
    uid: b14ecd40-cde4-4a16-8942-a01986205944
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:25Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:25Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nTargetConfigControllerDegraded: \"serviceaccount/localhost-recovery-client\":
    serviceaccounts \"localhost-recovery-client\" not found" to "NodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nTargetConfigControllerDegraded: \"serviceaccount/localhost-recovery-client\":
    serviceaccounts \"localhost-recovery-client\" not found"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:25Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:25Z"
    name: openshift-kube-scheduler-operator.179e33a34ee3ce18
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "7066"
    uid: c7265c11-683d-4daa-a20b-ef8d6766739d
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:25Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:25Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nTargetConfigControllerDegraded: \"serviceaccount/localhost-recovery-client\":
    serviceaccounts \"localhost-recovery-client\" not found" to "NodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: [Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nTargetConfigControllerDegraded: \"serviceaccount/localhost-recovery-client\":
    serviceaccounts \"localhost-recovery-client\" not found"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:25Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:25Z"
    name: openshift-kube-scheduler-operator.179e33a35310e575
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "7073"
    uid: 2274e2bc-8b15-4d16-bdf2-dddae5b51c4f
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:25Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:25Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: [Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nTargetConfigControllerDegraded: \"serviceaccount/localhost-recovery-client\":
    serviceaccounts \"localhost-recovery-client\" not found" to "NodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: [Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand on node ip-10-0-21-63.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nTargetConfigControllerDegraded: \"serviceaccount/localhost-recovery-client\":
    serviceaccounts \"localhost-recovery-client\" not found"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:25Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:25Z"
    name: openshift-kube-scheduler-operator.179e33a3546abb75
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "7078"
    uid: 905613da-5a56-4b6e-b4d0-3e789a82c220
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:27Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:27Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: [Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand on node ip-10-0-21-63.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nTargetConfigControllerDegraded: \"serviceaccount/localhost-recovery-client\":
    serviceaccounts \"localhost-recovery-client\" not found" to "NodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: [Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand on node ip-10-0-21-63.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:27Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:27Z"
    name: openshift-kube-scheduler-operator.179e33a3afa5c6ab
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "7138"
    uid: 7eaaf507-450b-4afb-a3b3-c6600ef71529
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:27Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:27Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: [Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand on node ip-10-0-21-63.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]" to "NodeControllerDegraded: The master
    nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\" not ready
    since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container runtime
    network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: [Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:27Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:27Z"
    name: openshift-kube-scheduler-operator.179e33a3d39dca0e
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "7175"
    uid: 2e67d2c8-daff-4a1b-b9cb-787079bbb8c0
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:32Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:32Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nGuardControllerDegraded: [Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]" to "NodeControllerDegraded: All master
    nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-21-63.us-west-1.compute.internal,
    Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:32Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:32Z"
    name: openshift-kube-scheduler-operator.179e33a4d8c2dbf2
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "7402"
    uid: b4539ffc-f9f3-4c7a-9e29-0d841c8477bc
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:32Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:32Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]" to "NodeControllerDegraded: All master
    nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:32Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:32Z"
    name: openshift-kube-scheduler-operator.179e33a4d9e1e41a
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "7406"
    uid: 5ec2b12c-278c-4d7b-a661-3af595a3fb13
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:32Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:32Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]" to "NodeControllerDegraded: All master
    nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-21-63.us-west-1.compute.internal,
    Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:32Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:32Z"
    name: openshift-kube-scheduler-operator.179e33a4fe13fc4a
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "7442"
    uid: 66b479fd-e07a-4471-b401-4f6cb9684374
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:36Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:36Z"
  message: Created Secret/kube-scheduler-client-cert-key -n openshift-kube-scheduler
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:36Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:36Z"
    name: openshift-kube-scheduler-operator.179e33a5bb80c687
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "7573"
    uid: da78ef5b-c8d8-4e33-8e7b-55afea6963db
  reason: SecretCreated
  reportingComponent: openshift-cluster-kube-scheduler-operator-resource-sync-controller-resourcesynccontroller
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-resource-sync-controller-resourcesynccontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:36Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:36Z"
  message: |-
    Updated ConfigMap/kube-scheduler-pod -n openshift-kube-scheduler:
    cause by changes in data.pod.yaml
  metadata:
    creationTimestamp: "2023-12-06T09:21:36Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:36Z"
    name: openshift-kube-scheduler-operator.179e33a5c749dc5f
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "7579"
    uid: fad3dc6e-8422-4ed7-80b8-46eb08d52e55
  reason: ConfigMapUpdated
  reportingComponent: openshift-cluster-kube-scheduler-operator-targetconfigcontroller
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-targetconfigcontroller
  type: Normal
- apiVersion: v1
  count: 17
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:40Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:22:23Z"
  message: 'configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1'
  metadata:
    creationTimestamp: "2023-12-06T09:21:40Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:22:23Z"
    name: openshift-kube-scheduler-operator.179e33a6b46f7691
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "10413"
    uid: cff32b63-2f3b-41ce-838b-9e17d334f5b7
  reason: RequiredInstallerResourcesMissing
  reportingComponent: openshift-cluster-kube-scheduler-operator-installer-controller
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-installer-controller
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:40Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:40Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand on node ip-10-0-106-212.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]" to "NodeControllerDegraded: All master
    nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:40Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:40Z"
    name: openshift-kube-scheduler-operator.179e33a6b4e76afc
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "7720"
    uid: 5e13fbd5-1ca1-4fb7-9f0f-271e66727b7e
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:40Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:40Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand on node ip-10-0-21-63.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]" to "NodeControllerDegraded: All master
    nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:40Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:40Z"
    name: openshift-kube-scheduler-operator.179e33a6b56d100e
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "7723"
    uid: 47cc191e-e1d2-406e-9008-d94fb591798e
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 2
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:42Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:22:23Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand on node ip-10-0-21-63.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]" to "NodeControllerDegraded: All master
    nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-21-63.us-west-1.compute.internal,
    Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:42Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:22:23Z"
    name: openshift-kube-scheduler-operator.179e33a7364086e5
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "10415"
    uid: 02c6b3d4-cf41-467d-acfb-2016c4051a9e
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:46Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:46Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand on node ip-10-0-106-212.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]" to "NodeControllerDegraded: All master
    nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:46Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:46Z"
    name: openshift-kube-scheduler-operator.179e33a817340834
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "8228"
    uid: 02991edb-da2f-4ce7-afea-f5327dd83c5f
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 2
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:48Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:21:51Z"
  message: |-
    Updated ConfigMap/serviceaccount-ca -n openshift-kube-scheduler:
    cause by changes in data.ca-bundle.crt
  metadata:
    creationTimestamp: "2023-12-06T09:21:48Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:21:51Z"
    name: openshift-kube-scheduler-operator.179e33a8928eacdd
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "8444"
    uid: 3356f42b-185b-4e4b-86be-099058a1888d
  reason: ConfigMapUpdated
  reportingComponent: openshift-cluster-kube-scheduler-operator-targetconfigcontroller
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-targetconfigcontroller
  type: Normal
- apiVersion: v1
  count: 4
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:52Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:22:10Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand on node ip-10-0-21-63.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]" to "NodeControllerDegraded: All master
    nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:52Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:22:10Z"
    name: openshift-kube-scheduler-operator.179e33a9959aec32
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "9442"
    uid: b76c5368-ad53-484f-a73f-3a4780329245
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 4
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:53Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:22:21Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]" to "NodeControllerDegraded: All master
    nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:53Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:22:21Z"
    name: openshift-kube-scheduler-operator.179e33a9b85bbe55
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "10356"
    uid: b197ee22-47d7-4d57-90c0-ba8acc912786
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:12Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:22:12Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]" to "NodeControllerDegraded: All master
    nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-21-63.us-west-1.compute.internal,
    Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:12Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:22:12Z"
    name: openshift-kube-scheduler-operator.179e33ae3177d5eb
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "9515"
    uid: 5dea8bf4-9022-422f-ad93-34b67e0c7e6c
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:13Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:22:13Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand on node ip-10-0-106-212.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]" to "NodeControllerDegraded: All master
    nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:13Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:22:13Z"
    name: openshift-kube-scheduler-operator.179e33ae5b4b7afa
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "9618"
    uid: c8a6cb2d-5bf7-4072-af38-784d698aa4e9
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:23Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:22:23Z"
  message: 'Failed to create revision 2: client rate limiter Wait returned an error:
    context canceled'
  metadata:
    creationTimestamp: "2023-12-06T09:22:23Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:22:23Z"
    name: openshift-kube-scheduler-operator.179e33b0b9e8c5f4
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "10417"
    uid: 86968515-945b-414b-8250-01766c762df2
  reason: RevisionCreateFailed
  reportingComponent: openshift-cluster-kube-scheduler-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-revisioncontroller
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:25Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:22:25Z"
  message: FeatureGates updated to featuregates.Features{Enabled:[]v1.FeatureGateName{"AlibabaPlatform",
    "AzureWorkloadIdentity", "BuildCSIVolumes", "CloudDualStackNodeIPs", "ExternalCloudProvider",
    "ExternalCloudProviderAzure", "ExternalCloudProviderExternal", "ExternalCloudProviderGCP",
    "OpenShiftPodSecurityAdmission", "PrivateHostedZoneAWS"}, Disabled:[]v1.FeatureGateName{"AdminNetworkPolicy",
    "AutomatedEtcdBackup", "CSIDriverSharedResource", "ClusterAPIInstall", "DNSNameResolver",
    "DisableKubeletCloudCredentialProviders", "DynamicResourceAllocation", "EventedPLEG",
    "GCPClusterHostedDNS", "GCPLabelsTags", "GatewayAPI", "InsightsConfigAPI", "InstallAlternateInfrastructureAWS",
    "MachineAPIOperatorDisableMachineHealthCheckController", "MachineAPIProviderOpenStack",
    "MachineConfigNodes", "ManagedBootImages", "MaxUnavailableStatefulSet", "MetricsServer",
    "MixedCPUsAllocation", "NetworkLiveMigration", "NodeSwap", "RouteExternalCertificate",
    "SigstoreImageVerification", "VSphereControlPlaneMachineSet", "VSphereStaticIPs",
    "ValidatingAdmissionPolicy"}}
  metadata:
    creationTimestamp: "2023-12-06T09:22:25Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:22:25Z"
    name: openshift-kube-scheduler-operator.179e33b121739ea8
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "10476"
    uid: af2f90e3-6051-4748-87ad-959675f15a49
  reason: FeatureGatesInitialized
  reportingComponent: openshift-cluster-kube-scheduler-operator
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator
  type: Normal
- apiVersion: v1
  count: 567
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:25Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:58:04Z"
  message: 'configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1'
  metadata:
    creationTimestamp: "2023-12-06T09:22:25Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:58:04Z"
    name: openshift-kube-scheduler-operator.179e33b12820ddc0
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "47840"
    uid: 0fc76484-1a97-4eab-af2c-50c6540e4336
  reason: RequiredInstallerResourcesMissing
  reportingComponent: openshift-cluster-kube-scheduler-operator-installer-controller
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-installer-controller
  type: Warning
- apiVersion: v1
  count: 2
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:25Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:22:26Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand on node ip-10-0-106-212.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]" to "NodeControllerDegraded: All master
    nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:25Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:22:26Z"
    name: openshift-kube-scheduler-operator.179e33b135c500fc
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "10575"
    uid: 914804b4-002d-4edc-a155-027e4732e950
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 2
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:26Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:23:02Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]" to "NodeControllerDegraded: All master
    nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:26Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:23:02Z"
    name: openshift-kube-scheduler-operator.179e33b15d3265cd
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "11762"
    uid: a72433fb-94a5-4d8c-89f2-10f510adb8e2
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:26Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:22:26Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand on node ip-10-0-21-63.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]" to "NodeControllerDegraded: All master
    nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-21-63.us-west-1.compute.internal,
    Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:26Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:22:26Z"
    name: openshift-kube-scheduler-operator.179e33b15e31f5b8
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "10554"
    uid: 4b0b9c39-bed7-410d-86aa-53d50b98c85a
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 521
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:27Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:58:07Z"
  message: 'Failed to create ConfigMap/kube-scheduler-pod-2 -n openshift-kube-scheduler:
    ConfigMap "kube-scheduler-pod-2" is invalid: metadata.ownerReferences.uid: Invalid
    value: "": uid must not be empty'
  metadata:
    creationTimestamp: "2023-12-06T09:22:27Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:58:07Z"
    name: openshift-kube-scheduler-operator.179e33b1b11c2858
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "47871"
    uid: 1db7e45f-a1b2-459e-94a3-617c7fc646a0
  reason: ConfigMapCreateFailed
  reportingComponent: openshift-cluster-kube-scheduler-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-revisioncontroller
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:07Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:23:07Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand on node ip-10-0-21-63.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]" to "NodeControllerDegraded: All master
    nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]\nRevisionControllerDegraded:
    ConfigMap \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid
    value: \"\": uid must not be empty\nInstallerControllerDegraded: missing required
    resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]"'
  metadata:
    creationTimestamp: "2023-12-06T09:23:07Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:23:07Z"
    name: openshift-kube-scheduler-operator.179e33bafd65258b
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "11905"
    uid: 0982f0df-7947-4f9a-a494-20e353814131
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:23:09Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded changed from
    False to True ("GuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]\nInstallerControllerDegraded:
    missing required resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nRevisionControllerDegraded: ConfigMap
    \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid value:
    \"\": uid must not be empty")'
  metadata:
    creationTimestamp: "2023-12-06T09:23:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:23:09Z"
    name: openshift-kube-scheduler-operator.179e33bb9102f218
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "11942"
    uid: 942fe310-70c6-4f0e-ad57-56cdfa7366b1
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 124
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:12Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:57:30Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "GuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]\nInstallerControllerDegraded:
    missing required resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nRevisionControllerDegraded: ConfigMap
    \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid value:
    \"\": uid must not be empty" to "GuardControllerDegraded: [Missing operand on
    node ip-10-0-94-160.us-west-1.compute.internal, Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal]\nInstallerControllerDegraded:
    missing required resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nRevisionControllerDegraded: ConfigMap
    \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid value:
    \"\": uid must not be empty"'
  metadata:
    creationTimestamp: "2023-12-06T09:23:12Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:57:30Z"
    name: openshift-kube-scheduler-operator.179e33bc235b317a
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "47401"
    uid: 0e746ac0-7e95-4a0d-8fc7-da500dfc7714
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 130
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:14Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:57:33Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "GuardControllerDegraded: [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal]\nInstallerControllerDegraded:
    missing required resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nRevisionControllerDegraded: ConfigMap
    \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid value:
    \"\": uid must not be empty" to "GuardControllerDegraded: [Missing operand on
    node ip-10-0-106-212.us-west-1.compute.internal, Missing operand on node ip-10-0-21-63.us-west-1.compute.internal,
    Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]\nInstallerControllerDegraded:
    missing required resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nRevisionControllerDegraded: ConfigMap
    \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid value:
    \"\": uid must not be empty"'
  metadata:
    creationTimestamp: "2023-12-06T09:23:14Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:57:33Z"
    name: openshift-kube-scheduler-operator.179e33bc95a71b19
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "47455"
    uid: ced37ca9-36b5-4ed8-99f2-04c49ed252d4
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 127
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:17Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:58:03Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "GuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]\nInstallerControllerDegraded:
    missing required resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nRevisionControllerDegraded: ConfigMap
    \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid value:
    \"\": uid must not be empty" to "GuardControllerDegraded: [Missing operand on
    node ip-10-0-21-63.us-west-1.compute.internal, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal]\nInstallerControllerDegraded:
    missing required resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nRevisionControllerDegraded: ConfigMap
    \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid value:
    \"\": uid must not be empty"'
  metadata:
    creationTimestamp: "2023-12-06T09:23:17Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:58:03Z"
    name: openshift-kube-scheduler-operator.179e33bd6975a475
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "47837"
    uid: 2d84fee0-2809-4d0f-a248-7c1d2886b7db
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 26
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:22Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:56:37Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "GuardControllerDegraded: [Missing operand on node ip-10-0-21-63.us-west-1.compute.internal,
    Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal]\nInstallerControllerDegraded:
    missing required resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nRevisionControllerDegraded: ConfigMap
    \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid value:
    \"\": uid must not be empty" to "GuardControllerDegraded: [Missing operand on
    node ip-10-0-94-160.us-west-1.compute.internal, Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal]\nInstallerControllerDegraded:
    missing required resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nRevisionControllerDegraded: ConfigMap
    \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid value:
    \"\": uid must not be empty"'
  metadata:
    creationTimestamp: "2023-12-06T09:23:22Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:56:37Z"
    name: openshift-kube-scheduler-operator.179e33be7dd63d3f
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "46361"
    uid: 1fbb2e40-a032-46d6-ba0a-5e9924a7c93b
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 121
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:38Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:58:04Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "GuardControllerDegraded: [Missing operand on node ip-10-0-21-63.us-west-1.compute.internal,
    Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal]\nInstallerControllerDegraded:
    missing required resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nRevisionControllerDegraded: ConfigMap
    \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid value:
    \"\": uid must not be empty" to "GuardControllerDegraded: [Missing operand on
    node ip-10-0-106-212.us-west-1.compute.internal, Missing operand on node ip-10-0-21-63.us-west-1.compute.internal,
    Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]\nInstallerControllerDegraded:
    missing required resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nRevisionControllerDegraded: ConfigMap
    \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid value:
    \"\": uid must not be empty"'
  metadata:
    creationTimestamp: "2023-12-06T09:23:38Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:58:04Z"
    name: openshift-kube-scheduler-operator.179e33c22b749d36
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "47842"
    uid: 224db90f-3091-48a4-8d8c-46436a196f12
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
- apiVersion: v1
  count: 20
  eventTime: null
  firstTimestamp: "2023-12-06T09:25:26Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: openshift-kube-scheduler-operator
    namespace: openshift-kube-scheduler-operator
    uid: cb379d58-d7cd-40c3-b312-cec02846e6c7
  kind: Event
  lastTimestamp: "2023-12-06T09:54:55Z"
  message: 'Status for clusteroperator/kube-scheduler changed: Degraded message changed
    from "GuardControllerDegraded: [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal]\nInstallerControllerDegraded:
    missing required resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nRevisionControllerDegraded: ConfigMap
    \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid value:
    \"\": uid must not be empty" to "GuardControllerDegraded: [Missing operand on
    node ip-10-0-21-63.us-west-1.compute.internal, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal]\nInstallerControllerDegraded:
    missing required resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1]\nRevisionControllerDegraded: ConfigMap
    \"kube-scheduler-pod-2\" is invalid: metadata.ownerReferences.uid: Invalid value:
    \"\": uid must not be empty"'
  metadata:
    creationTimestamp: "2023-12-06T09:25:26Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-scheduler-operator
      operation: Update
      time: "2023-12-06T09:54:55Z"
    name: openshift-kube-scheduler-operator.179e33db6f66420b
    namespace: openshift-kube-scheduler-operator
    resourceVersion: "45230"
    uid: 364de269-1289-474d-bd8b-c37e143cf1fd
  reason: OperatorStatusChanged
  reportingComponent: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  reportingInstance: ""
  source:
    component: openshift-cluster-kube-scheduler-operator-status-controller-statussyncer_kube-scheduler
  type: Normal
kind: EventList
metadata:
  resourceVersion: "47902"
