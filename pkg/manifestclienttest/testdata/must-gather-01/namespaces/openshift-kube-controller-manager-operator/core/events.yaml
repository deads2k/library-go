---
apiVersion: v1
items:
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:18:45Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: kube-controller-manager-operator-db597848b-9bhz6
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "2527"
    uid: 4bd1850e-ab80-47f4-81eb-d66550b55d88
  kind: Event
  lastTimestamp: "2023-12-06T09:18:45Z"
  message: '0/2 nodes are available: 2 node(s) had untolerated taint {node.cloudprovider.kubernetes.io/uninitialized:
    true}. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..'
  metadata:
    creationTimestamp: "2023-12-06T09:18:45Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: kube-scheduler
      operation: Update
      time: "2023-12-06T09:18:45Z"
    name: kube-controller-manager-operator-db597848b-9bhz6.179e337e02786875
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "2529"
    uid: 94b3bfaa-bea2-4385-9f02-7390586d37cd
  reason: FailedScheduling
  reportingComponent: default-scheduler
  reportingInstance: ""
  source:
    component: default-scheduler
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:20:49Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: kube-controller-manager-operator-db597848b-9bhz6
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "2531"
    uid: 4bd1850e-ab80-47f4-81eb-d66550b55d88
  kind: Event
  lastTimestamp: "2023-12-06T09:20:49Z"
  message: Successfully assigned openshift-kube-controller-manager-operator/kube-controller-manager-operator-db597848b-9bhz6
    to ip-10-0-21-63.us-west-1.compute.internal
  metadata:
    creationTimestamp: "2023-12-06T09:20:49Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: kube-scheduler
      operation: Update
      time: "2023-12-06T09:20:49Z"
    name: kube-controller-manager-operator-db597848b-9bhz6.179e339ae0896a70
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "4447"
    uid: 4d780a58-4179-44d5-acf2-d1cbe12b2a88
  reason: Scheduled
  reportingComponent: default-scheduler
  reportingInstance: ""
  source:
    component: default-scheduler
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:20:50Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: kube-controller-manager-operator-db597848b-9bhz6
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "4403"
    uid: 4bd1850e-ab80-47f4-81eb-d66550b55d88
  kind: Event
  lastTimestamp: "2023-12-06T09:20:50Z"
  message: Add eth0 [10.129.0.15/23] from ovn-kubernetes
  metadata:
    creationTimestamp: "2023-12-06T09:20:50Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: multus-daemon
      operation: Update
      time: "2023-12-06T09:20:50Z"
    name: kube-controller-manager-operator-db597848b-9bhz6.179e339b215e3368
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "4523"
    uid: e1c4ecc3-6a55-4461-a49c-5957a675e339
  reason: AddedInterface
  reportingComponent: ""
  reportingInstance: ""
  source:
    component: multus
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:20:50Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-controller-manager-operator}
    kind: Pod
    name: kube-controller-manager-operator-db597848b-9bhz6
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "4377"
    uid: 4bd1850e-ab80-47f4-81eb-d66550b55d88
  kind: Event
  lastTimestamp: "2023-12-06T09:20:50Z"
  message: Pulling image "registry.build03.ci.openshift.org/ci-op-2j285qtr/stable@sha256:c936b6297ca6b08bc72aa2b56c865ea18cb04838f10f843997c5f38745354be4"
  metadata:
    creationTimestamp: "2023-12-06T09:20:50Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:reportingInstance: {}
        f:source:
          f:component: {}
          f:host: {}
        f:type: {}
      manager: kubelet
      operation: Update
      time: "2023-12-06T09:20:50Z"
    name: kube-controller-manager-operator-db597848b-9bhz6.179e339b22b9e2a5
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "4525"
    uid: 33d6657d-66b0-436f-9a39-b57e127d8251
  reason: Pulling
  reportingComponent: kubelet
  reportingInstance: ip-10-0-21-63.us-west-1.compute.internal
  source:
    component: kubelet
    host: ip-10-0-21-63.us-west-1.compute.internal
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:05Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-controller-manager-operator}
    kind: Pod
    name: kube-controller-manager-operator-db597848b-9bhz6
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "4377"
    uid: 4bd1850e-ab80-47f4-81eb-d66550b55d88
  kind: Event
  lastTimestamp: "2023-12-06T09:21:05Z"
  message: Successfully pulled image "registry.build03.ci.openshift.org/ci-op-2j285qtr/stable@sha256:c936b6297ca6b08bc72aa2b56c865ea18cb04838f10f843997c5f38745354be4"
    in 14.862s (14.862s including waiting)
  metadata:
    creationTimestamp: "2023-12-06T09:21:05Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:reportingInstance: {}
        f:source:
          f:component: {}
          f:host: {}
        f:type: {}
      manager: kubelet
      operation: Update
      time: "2023-12-06T09:21:05Z"
    name: kube-controller-manager-operator-db597848b-9bhz6.179e339e98933116
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "4849"
    uid: de08c422-5319-40ae-8692-149fa53849d1
  reason: Pulled
  reportingComponent: kubelet
  reportingInstance: ip-10-0-21-63.us-west-1.compute.internal
  source:
    component: kubelet
    host: ip-10-0-21-63.us-west-1.compute.internal
  type: Normal
- apiVersion: v1
  count: 2
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:06Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-controller-manager-operator}
    kind: Pod
    name: kube-controller-manager-operator-db597848b-9bhz6
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "4377"
    uid: 4bd1850e-ab80-47f4-81eb-d66550b55d88
  kind: Event
  lastTimestamp: "2023-12-06T09:22:29Z"
  message: Created container kube-controller-manager-operator
  metadata:
    creationTimestamp: "2023-12-06T09:21:06Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:reportingInstance: {}
        f:source:
          f:component: {}
          f:host: {}
        f:type: {}
      manager: kubelet
      operation: Update
      time: "2023-12-06T09:22:29Z"
    name: kube-controller-manager-operator-db597848b-9bhz6.179e339edbfe19e5
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "10695"
    uid: d58020bf-1d21-420e-9866-5009711b18d5
  reason: Created
  reportingComponent: kubelet
  reportingInstance: ip-10-0-21-63.us-west-1.compute.internal
  source:
    component: kubelet
    host: ip-10-0-21-63.us-west-1.compute.internal
  type: Normal
- apiVersion: v1
  count: 2
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:06Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-controller-manager-operator}
    kind: Pod
    name: kube-controller-manager-operator-db597848b-9bhz6
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "4377"
    uid: 4bd1850e-ab80-47f4-81eb-d66550b55d88
  kind: Event
  lastTimestamp: "2023-12-06T09:22:29Z"
  message: Started container kube-controller-manager-operator
  metadata:
    creationTimestamp: "2023-12-06T09:21:06Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:reportingInstance: {}
        f:source:
          f:component: {}
          f:host: {}
        f:type: {}
      manager: kubelet
      operation: Update
      time: "2023-12-06T09:22:29Z"
    name: kube-controller-manager-operator-db597848b-9bhz6.179e339ee0a99d18
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "10696"
    uid: cc891c33-4e8b-42f5-ba81-2370208561f0
  reason: Started
  reportingComponent: kubelet
  reportingInstance: ip-10-0-21-63.us-west-1.compute.internal
  source:
    component: kubelet
    host: ip-10-0-21-63.us-west-1.compute.internal
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:07Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: kube-controller-manager-operator-db597848b-9bhz6
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "4579"
    uid: 4bd1850e-ab80-47f4-81eb-d66550b55d88
  kind: Event
  lastTimestamp: "2023-12-06T09:21:07Z"
  message: 'failed to add remote pod openshift-kube-controller-manager-operator/kube-controller-manager-operator-db597848b-9bhz6
    to namespace: failed add ips to address set default-network-controller:Namespace:openshift-kube-controller-manager-operator:
    (error in transact with ops [{Op:mutate Table:Address_Set Row:map[] Rows:[] Columns:[]
    Mutations:[{Column:addresses Mutator:insert Value:{GoSet:[10.129.0.15]}}] Timeout:<nil>
    Where:[where column _uuid == {d6cef1a9-7411-41f6-8d53-edea94e2d8a6}] Until: Durable:<nil>
    Comment:<nil> Lock:<nil> UUID: UUIDName:}]: context deadline exceeded: while awaiting
    reconnection)'
  metadata:
    creationTimestamp: "2023-12-06T09:21:07Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: ip-10-0-94-160
      operation: Update
      time: "2023-12-06T09:21:07Z"
    name: kube-controller-manager-operator-db597848b-9bhz6.179e339f17b4462b
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "4971"
    uid: d2f10f64-dd80-46ca-8f9d-98bc2c0727ed
  reason: ErrorUpdatingResource
  reportingComponent: controlplane
  reportingInstance: ""
  source:
    component: controlplane
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:29Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-controller-manager-operator}
    kind: Pod
    name: kube-controller-manager-operator-db597848b-9bhz6
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "4377"
    uid: 4bd1850e-ab80-47f4-81eb-d66550b55d88
  kind: Event
  lastTimestamp: "2023-12-06T09:22:29Z"
  message: Container image "registry.build03.ci.openshift.org/ci-op-2j285qtr/stable@sha256:c936b6297ca6b08bc72aa2b56c865ea18cb04838f10f843997c5f38745354be4"
    already present on machine
  metadata:
    creationTimestamp: "2023-12-06T09:22:29Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:reportingInstance: {}
        f:source:
          f:component: {}
          f:host: {}
        f:type: {}
      manager: kubelet
      operation: Update
      time: "2023-12-06T09:22:29Z"
    name: kube-controller-manager-operator-db597848b-9bhz6.179e33b21d8914fb
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "10685"
    uid: c7eb48b3-1688-41a2-9002-6da26ce3ac6a
  reason: Pulled
  reportingComponent: kubelet
  reportingInstance: ip-10-0-21-63.us-west-1.compute.internal
  source:
    component: kubelet
    host: ip-10-0-21-63.us-west-1.compute.internal
  type: Normal
- apiVersion: v1
  count: 13
  eventTime: null
  firstTimestamp: "2023-12-06T09:18:04Z"
  involvedObject:
    apiVersion: apps/v1
    kind: ReplicaSet
    name: kube-controller-manager-operator-db597848b
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "1379"
    uid: 290a49f2-ba63-411e-adb8-987d7e00c064
  kind: Event
  lastTimestamp: "2023-12-06T09:18:25Z"
  message: 'Error creating: pods "kube-controller-manager-operator-db597848b-" is
    forbidden: autoscaling.openshift.io/ManagementCPUsOverride the cluster does not
    have any nodes'
  metadata:
    creationTimestamp: "2023-12-06T09:18:04Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: kube-controller-manager
      operation: Update
      time: "2023-12-06T09:18:25Z"
    name: kube-controller-manager-operator-db597848b.179e337477b3a85c
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "2270"
    uid: 98e30331-1fd4-48e0-9784-ae8aa1d8c3e4
  reason: FailedCreate
  reportingComponent: replicaset-controller
  reportingInstance: ""
  source:
    component: replicaset-controller
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:18:45Z"
  involvedObject:
    apiVersion: apps/v1
    kind: ReplicaSet
    name: kube-controller-manager-operator-db597848b
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "1383"
    uid: 290a49f2-ba63-411e-adb8-987d7e00c064
  kind: Event
  lastTimestamp: "2023-12-06T09:18:45Z"
  message: 'Created pod: kube-controller-manager-operator-db597848b-9bhz6'
  metadata:
    creationTimestamp: "2023-12-06T09:18:45Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: kube-controller-manager
      operation: Update
      time: "2023-12-06T09:18:45Z"
    name: kube-controller-manager-operator-db597848b.179e337e026eda28
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "2528"
    uid: 4769e7ed-2959-462d-adcf-cc6d08897a59
  reason: SuccessfulCreate
  reportingComponent: replicaset-controller
  reportingInstance: ""
  source:
    component: replicaset-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:09Z"
  involvedObject:
    apiVersion: coordination.k8s.io/v1
    kind: Lease
    name: kube-controller-manager-operator-lock
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "5201"
    uid: f461ae7a-b38e-428f-b42e-e9d339d13d80
  kind: Event
  lastTimestamp: "2023-12-06T09:21:09Z"
  message: kube-controller-manager-operator-db597848b-9bhz6_6eab3edc-99f7-4b64-bc77-43d7d74e66da
    became leader
  metadata:
    creationTimestamp: "2023-12-06T09:21:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:09Z"
    name: kube-controller-manager-operator-lock.179e339f8e01f5ee
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "5202"
    uid: 391b434a-c183-400d-9837-a17a719924fe
  reason: LeaderElection
  reportingComponent: kube-controller-manager-operator
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:29Z"
  involvedObject:
    apiVersion: coordination.k8s.io/v1
    kind: Lease
    name: kube-controller-manager-operator-lock
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "10752"
    uid: f461ae7a-b38e-428f-b42e-e9d339d13d80
  kind: Event
  lastTimestamp: "2023-12-06T09:22:29Z"
  message: kube-controller-manager-operator-db597848b-9bhz6_4168a79d-19f5-4983-8c4e-a060e5d7354c
    became leader
  metadata:
    creationTimestamp: "2023-12-06T09:22:29Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:29Z"
    name: kube-controller-manager-operator-lock.179e33b24551bf37
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "10756"
    uid: dff2fab6-a55c-4845-943f-10e67e37125b
  reason: LeaderElection
  reportingComponent: kube-controller-manager-operator
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:18:04Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "1377"
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:18:04Z"
  message: Scaled up replica set kube-controller-manager-operator-db597848b to 1
  metadata:
    creationTimestamp: "2023-12-06T09:18:04Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: kube-controller-manager
      operation: Update
      time: "2023-12-06T09:18:04Z"
    name: kube-controller-manager-operator.179e33747785e2f5
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "1380"
    uid: f944178b-4b1c-4928-9844-a19eee530ed2
  reason: ScalingReplicaSet
  reportingComponent: deployment-controller
  reportingInstance: ""
  source:
    component: deployment-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:09Z"
  message: FeatureGates updated to featuregates.Features{Enabled:[]v1.FeatureGateName{"AlibabaPlatform",
    "AzureWorkloadIdentity", "BuildCSIVolumes", "CloudDualStackNodeIPs", "ExternalCloudProvider",
    "ExternalCloudProviderAzure", "ExternalCloudProviderExternal", "ExternalCloudProviderGCP",
    "OpenShiftPodSecurityAdmission", "PrivateHostedZoneAWS"}, Disabled:[]v1.FeatureGateName{"AdminNetworkPolicy",
    "AutomatedEtcdBackup", "CSIDriverSharedResource", "ClusterAPIInstall", "DNSNameResolver",
    "DisableKubeletCloudCredentialProviders", "DynamicResourceAllocation", "EventedPLEG",
    "GCPClusterHostedDNS", "GCPLabelsTags", "GatewayAPI", "InsightsConfigAPI", "InstallAlternateInfrastructureAWS",
    "MachineAPIOperatorDisableMachineHealthCheckController", "MachineAPIProviderOpenStack",
    "MachineConfigNodes", "ManagedBootImages", "MaxUnavailableStatefulSet", "MetricsServer",
    "MixedCPUsAllocation", "NetworkLiveMigration", "NodeSwap", "RouteExternalCertificate",
    "SigstoreImageVerification", "VSphereControlPlaneMachineSet", "VSphereStaticIPs",
    "ValidatingAdmissionPolicy"}}
  metadata:
    creationTimestamp: "2023-12-06T09:21:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:09Z"
    name: kube-controller-manager-operator.179e339f8e32fece
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "5204"
    uid: 58696483-6bbb-4407-9dd2-599540fc5b0a
  reason: FeatureGatesInitialized
  reportingComponent: kube-controller-manager-operator
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator
  type: Normal
- apiVersion: v1
  count: 2
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:09Z"
  message: clusteroperator/kube-controller-manager version "raw-internal" changed
    from "" to "4.15.0-0.ci.test-2023-12-06-090630-ci-op-2j285qtr-latest"
  metadata:
    creationTimestamp: "2023-12-06T09:21:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:09Z"
    name: kube-controller-manager-operator.179e339f97704504
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "5231"
    uid: 4024efa0-6019-4cae-9143-0bd318fe3bba
  reason: OperatorVersionChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:09Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded set
    to Unknown (""),Progressing set to Unknown (""),Available set to Unknown (""),Upgradeable
    set to Unknown (""),status.relatedObjects changed from [{"operator.openshift.io"
    "kubecontrollermanagers" "" "cluster"} {"" "namespaces" "" "openshift-config"}
    {"" "namespaces" "" "openshift-config-managed"} {"" "namespaces" "" "openshift-kube-controller-manager"}
    {"" "namespaces" "" "openshift-kube-controller-manager-operator"} {"" "namespaces"
    "" "kube-system"} {"" "nodes" "" ""} {"certificates.k8s.io" "certificatesigningrequests"
    "" ""}] to [{"operator.openshift.io" "kubecontrollermanagers" "" "cluster"} {""
    "namespaces" "" "openshift-config"} {"" "namespaces" "" "openshift-config-managed"}
    {"" "namespaces" "" "openshift-kube-controller-manager"} {"" "namespaces" "" "openshift-kube-controller-manager-operator"}
    {"" "namespaces" "" "kube-system"} {"certificates.k8s.io" "certificatesigningrequests"
    "" ""} {"" "nodes" "" ""} {"config.openshift.io" "nodes" "" "cluster"}],status.versions
    changed from [] to [{"raw-internal" "4.15.0-0.ci.test-2023-12-06-090630-ci-op-2j285qtr-latest"}]'
  metadata:
    creationTimestamp: "2023-12-06T09:21:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:09Z"
    name: kube-controller-manager-operator.179e339f98936f08
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "5226"
    uid: 42e12417-a90f-403f-a0ae-5c90247af6c8
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:09Z"
  message: new revision 1 triggered by "configmap \"kube-controller-manager-pod\"
    not found"
  metadata:
    creationTimestamp: "2023-12-06T09:21:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:09Z"
    name: kube-controller-manager-operator.179e339f98c24082
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "5227"
    uid: 3eedba13-e317-4d88-82f9-f4491998b522
  reason: RevisionTriggered
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 16
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:15Z"
  message: 'configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key,
    configmaps: cluster-policy-controller-config-0,config-0,controller-manager-kubeconfig-0,kube-controller-cert-syncer-kubeconfig-0,kube-controller-manager-pod-0,recycler-config-0,service-ca-0,serviceaccount-ca-0,
    secrets: localhost-recovery-client-token-0,service-account-private-key-0'
  metadata:
    creationTimestamp: "2023-12-06T09:21:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:16Z"
    name: kube-controller-manager-operator.179e339f98cb76c5
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6334"
    uid: 8421962d-2df6-4fe9-9d15-4acf67f1a128
  reason: RequiredInstallerResourcesMissing
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:09Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded changed
    from Unknown to False ("InstallerControllerDegraded: missing required resources:
    [configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key,
    configmaps: cluster-policy-controller-config-0,config-0,controller-manager-kubeconfig-0,kube-controller-cert-syncer-kubeconfig-0,kube-controller-manager-pod-0,recycler-config-0,service-ca-0,serviceaccount-ca-0,
    secrets: localhost-recovery-client-token-0,service-account-private-key-0]"),Progressing
    changed from Unknown to False ("All is well"),Available changed from Unknown to
    False ("StaticPodsAvailable: 0 nodes are active; "),Upgradeable changed from Unknown
    to True ("All is well")'
  metadata:
    creationTimestamp: "2023-12-06T09:21:10Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:10Z"
    name: kube-controller-manager-operator.179e339fa65157da
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "5297"
    uid: b41edd34-a3a3-4c52-b68a-c9b6a0652beb
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:11Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:11Z"
  message: Observed new master node ip-10-0-21-63.us-west-1.compute.internal
  metadata:
    creationTimestamp: "2023-12-06T09:21:11Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:11Z"
    name: kube-controller-manager-operator.179e339ff16fa03b
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "5631"
    uid: 1278a97a-7ef7-4712-b36e-25d3647e2b51
  reason: MasterNodeObserved
  reportingComponent: kube-controller-manager-operator-nodecontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-nodecontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:11Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:11Z"
  message: Observed new master node ip-10-0-94-160.us-west-1.compute.internal
  metadata:
    creationTimestamp: "2023-12-06T09:21:12Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:12Z"
    name: kube-controller-manager-operator.179e339ff16fd46b
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "5714"
    uid: 86e08707-6a1f-477e-bf6c-34084e89b4e8
  reason: MasterNodeObserved
  reportingComponent: kube-controller-manager-operator-nodecontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-nodecontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:11Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:11Z"
  message: Observed new master node ip-10-0-106-212.us-west-1.compute.internal
  metadata:
    creationTimestamp: "2023-12-06T09:21:12Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:12Z"
    name: kube-controller-manager-operator.179e339ff16fe13c
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "5823"
    uid: 12f9dc01-af9a-41d4-8248-daff8489c767
  reason: MasterNodeObserved
  reportingComponent: kube-controller-manager-operator-nodecontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-nodecontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:11Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:11Z"
  message: Created PodDisruptionBudget.policy/kube-controller-manager-guard-pdb -n
    openshift-kube-controller-manager because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:11Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:11Z"
    name: kube-controller-manager-operator.179e339ff26e7b75
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "5651"
    uid: a0db0cda-089e-4644-a2c9-acd47a5d31a2
  reason: PodDisruptionBudgetCreated
  reportingComponent: kube-controller-manager-operator-guardcontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-guardcontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:11Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:11Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key,
    configmaps: cluster-policy-controller-config-0,config-0,controller-manager-kubeconfig-0,kube-controller-cert-syncer-kubeconfig-0,kube-controller-manager-pod-0,recycler-config-0,service-ca-0,serviceaccount-ca-0,
    secrets: localhost-recovery-client-token-0,service-account-private-key-0]" to
    "InstallerControllerDegraded: missing required resources: [configmaps: aggregator-client-ca,client-ca,
    secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps: cluster-policy-controller-config-0,config-0,controller-manager-kubeconfig-0,kube-controller-cert-syncer-kubeconfig-0,kube-controller-manager-pod-0,recycler-config-0,service-ca-0,serviceaccount-ca-0,
    secrets: localhost-recovery-client-token-0,service-account-private-key-0]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:11Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:11Z"
    name: kube-controller-manager-operator.179e339ff34d1165
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "5683"
    uid: 15362aae-cd7f-4ab6-87be-c0cdef646013
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 4
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:11Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:41Z"
  message: minTLSVersion changed to VersionTLS12
  metadata:
    creationTimestamp: "2023-12-06T09:21:12Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:45Z"
    name: kube-controller-manager-operator.179e339ffcc846bf
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8211"
    uid: d0592b57-4eb9-4f32-a1d1-343e682c09a6
  reason: ObserveTLSSecurityProfile
  reportingComponent: kube-controller-manager-operator-config-observer-configobserver
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-config-observer-configobserver
  type: Normal
- apiVersion: v1
  count: 4
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:11Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:41Z"
  message: cipherSuites changed to ["TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256" "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256"
    "TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384" "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384"
    "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256" "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256"]
  metadata:
    creationTimestamp: "2023-12-06T09:21:13Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:46Z"
    name: kube-controller-manager-operator.179e339ffcc8e22e
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8229"
    uid: 87d644ca-8484-4906-9678-43f4aac2c9e2
  reason: ObserveTLSSecurityProfile
  reportingComponent: kube-controller-manager-operator-config-observer-configobserver
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-config-observer-configobserver
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:11Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:11Z"
  message: CloudProvider config file changed to /etc/kubernetes/static-pod-resources/configmaps/cloud-config/config
  metadata:
    creationTimestamp: "2023-12-06T09:21:14Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:14Z"
    name: kube-controller-manager-operator.179e339ffcce8c19
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "5967"
    uid: 25cc95ea-74bd-4dd6-b2b7-8a90a9189c96
  reason: ObserveCloudProviderNamesChanges
  reportingComponent: kube-controller-manager-operator-config-observer-configobserver
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-config-observer-configobserver
  type: Normal
- apiVersion: v1
  count: 4
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:11Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:41Z"
  message: Updated featureGates to AdminNetworkPolicy=false,AlibabaPlatform=true,AutomatedEtcdBackup=false,AzureWorkloadIdentity=true,BuildCSIVolumes=true,CSIDriverSharedResource=false,CloudDualStackNodeIPs=true,ClusterAPIInstall=false,DNSNameResolver=false,DisableKubeletCloudCredentialProviders=false,DynamicResourceAllocation=false,EventedPLEG=false,ExternalCloudProvider=true,ExternalCloudProviderAzure=true,ExternalCloudProviderExternal=true,ExternalCloudProviderGCP=true,GCPClusterHostedDNS=false,GCPLabelsTags=false,GatewayAPI=false,InsightsConfigAPI=false,InstallAlternateInfrastructureAWS=false,MachineAPIOperatorDisableMachineHealthCheckController=false,MachineAPIProviderOpenStack=false,MachineConfigNodes=false,ManagedBootImages=false,MaxUnavailableStatefulSet=false,MetricsServer=false,MixedCPUsAllocation=false,NetworkLiveMigration=false,NodeSwap=false,OpenShiftPodSecurityAdmission=true,PrivateHostedZoneAWS=true,RouteExternalCertificate=false,SigstoreImageVerification=false,VSphereControlPlaneMachineSet=false,VSphereStaticIPs=false,ValidatingAdmissionPolicy=false
  metadata:
    creationTimestamp: "2023-12-06T09:21:15Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:41Z"
    name: kube-controller-manager-operator.179e339ffcd3287a
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7797"
    uid: bfe9480f-f09c-4a07-ace1-2b0a6008e2f7
  reason: ObserveFeatureFlagsUpdated
  reportingComponent: kube-controller-manager-operator-config-observer-configobserver
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-config-observer-configobserver
  type: Normal
- apiVersion: v1
  count: 4
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:11Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:41Z"
  message: Updated extendedArguments.feature-gates to AdminNetworkPolicy=false,AlibabaPlatform=true,AutomatedEtcdBackup=false,AzureWorkloadIdentity=true,BuildCSIVolumes=true,CSIDriverSharedResource=false,CloudDualStackNodeIPs=true,ClusterAPIInstall=false,DNSNameResolver=false,DisableKubeletCloudCredentialProviders=false,DynamicResourceAllocation=false,EventedPLEG=false,ExternalCloudProviderAzure=true,ExternalCloudProviderExternal=true,ExternalCloudProviderGCP=true,GCPClusterHostedDNS=false,GCPLabelsTags=false,GatewayAPI=false,InsightsConfigAPI=false,InstallAlternateInfrastructureAWS=false,MachineAPIOperatorDisableMachineHealthCheckController=false,MachineAPIProviderOpenStack=false,MachineConfigNodes=false,ManagedBootImages=false,MaxUnavailableStatefulSet=false,MetricsServer=false,MixedCPUsAllocation=false,NetworkLiveMigration=false,NodeSwap=false,OpenShiftPodSecurityAdmission=true,PrivateHostedZoneAWS=true,RouteExternalCertificate=false,SigstoreImageVerification=false,VSphereControlPlaneMachineSet=false,VSphereStaticIPs=false,ValidatingAdmissionPolicy=false
  metadata:
    creationTimestamp: "2023-12-06T09:21:17Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:44Z"
    name: kube-controller-manager-operator.179e339ffcd5d84e
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8168"
    uid: 2b914b27-514e-4274-957e-4d1e287c1044
  reason: ObserveFeatureFlagsUpdated
  reportingComponent: kube-controller-manager-operator-config-observer-configobserver
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-config-observer-configobserver
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:11Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:11Z"
  message: "Writing updated observed config:   map[string]any{\n+ \t\"extendedArguments\":
    map[string]any{\n+ \t\t\"cloud-config\":   []any{string(\"/etc/kubernetes/static-pod-resources/configmaps/cloud-config/config\")},\n+ \t\t\"cloud-provider\":
    []any{string(\"external\")},\n+ \t\t\"cluster-cidr\":   []any{string(\"10.128.0.0/14\")},\n+ \t\t\"cluster-name\":
    \  []any{string(\"ci-op-2j285qtr-234c7-w84r8\")},\n+ \t\t\"feature-gates\": []any{\n+ \t\t\tstring(\"AdminNetworkPolicy=false\"),
    string(\"AlibabaPlatform=true\"),\n+ \t\t\tstring(\"AutomatedEtcdBackup=false\"),
    string(\"AzureWorkloadIdentity=true\"),\n+ \t\t\tstring(\"BuildCSIVolumes=true\"),
    string(\"CSIDriverSharedResource=false\"),\n+ \t\t\tstring(\"CloudDualStackNodeIPs=true\"),
    string(\"ClusterAPIInstall=false\"), ...,\n+ \t\t},\n+ \t\t\"service-cluster-ip-range\":
    []any{string(\"172.30.0.0/16\")},\n+ \t},\n+ \t\"featureGates\": []any{\n+ \t\tstring(\"AdminNetworkPolicy=false\"),
    string(\"AlibabaPlatform=true\"),\n+ \t\tstring(\"AutomatedEtcdBackup=false\"),
    string(\"AzureWorkloadIdentity=true\"),\n+ \t\tstring(\"BuildCSIVolumes=true\"),
    string(\"CSIDriverSharedResource=false\"),\n+ \t\tstring(\"CloudDualStackNodeIPs=true\"),
    string(\"ClusterAPIInstall=false\"),\n+ \t\tstring(\"DNSNameResolver=false\"),\n+ \t\tstring(\"DisableKubeletCloudCredentialProviders=false\"),\n+ \t\tstring(\"DynamicResourceAllocation=false\"),
    string(\"EventedPLEG=false\"),\n+ \t\tstring(\"ExternalCloudProvider=true\"),\n+ \t\tstring(\"ExternalCloudProviderAzure=true\"),\n+ \t\tstring(\"ExternalCloudProviderExternal=true\"),\n+ \t\tstring(\"ExternalCloudProviderGCP=true\"),
    ...,\n+ \t},\n+ \t\"servingInfo\": map[string]any{\n+ \t\t\"cipherSuites\": []any{\n+ \t\t\tstring(\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\"),\n+ \t\t\tstring(\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\"),\n+ \t\t\tstring(\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\"),\n+ \t\t\tstring(\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\"),\n+ \t\t\tstring(\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\"),\n+ \t\t\tstring(\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"),\n+ \t\t},\n+ \t\t\"minTLSVersion\":
    string(\"VersionTLS12\"),\n+ \t},\n  }\n"
  metadata:
    creationTimestamp: "2023-12-06T09:21:19Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:19Z"
    name: kube-controller-manager-operator.179e339ffcdcb5c5
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6635"
    uid: bbe856a3-3f93-4801-b5fc-52d8675faa58
  reason: ObservedConfigChanged
  reportingComponent: kube-controller-manager-operator-config-observer-configobserver
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-config-observer-configobserver
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:11Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:11Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key,
    configmaps: cluster-policy-controller-config-0,config-0,controller-manager-kubeconfig-0,kube-controller-cert-syncer-kubeconfig-0,kube-controller-manager-pod-0,recycler-config-0,service-ca-0,serviceaccount-ca-0,
    secrets: localhost-recovery-client-token-0,service-account-private-key-0]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)" to "InstallerControllerDegraded: missing
    required resources: [configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key,
    configmaps: cluster-policy-controller-config-0,config-0,controller-manager-kubeconfig-0,kube-controller-cert-syncer-kubeconfig-0,kube-controller-manager-pod-0,recycler-config-0,service-ca-0,serviceaccount-ca-0,
    secrets: localhost-recovery-client-token-0,service-account-private-key-0]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:12Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:12Z"
    name: kube-controller-manager-operator.179e33a00965ca5b
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "5769"
    uid: cdce9676-0f32-4304-944b-3e942d442138
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:12Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:12Z"
  message: Deleted Secret/csr-signer-signer -n openshift-kube-controller-manager-operator
  metadata:
    creationTimestamp: "2023-12-06T09:21:12Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:12Z"
    name: kube-controller-manager-operator.179e33a01f3a0793
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "5808"
    uid: dd8c0a44-f55d-4f86-b732-7807817eb86e
  reason: SecretDeleted
  reportingComponent: kube-controller-manager-operator
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:12Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:12Z"
  message: Created ConfigMap/revision-status-1 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:13Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:13Z"
    name: kube-controller-manager-operator.179e33a02b2e7ca9
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "5845"
    uid: dd81b682-cce2-4714-9513-6f81b6e6929f
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 23
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:12Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:42Z"
  message: no observedConfig
  metadata:
    creationTimestamp: "2023-12-06T09:21:13Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:42Z"
    name: kube-controller-manager-operator.179e33a03872ee22
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7903"
    uid: 9ba84953-62a7-427a-a45a-a2fbe2ad0062
  reason: ConfigMissing
  reportingComponent: kube-controller-manager-operator-targetconfigcontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-targetconfigcontroller
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:12Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:12Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Progressing
    message changed from "All is well" to "NodeInstallerProgressing: 3 nodes are at
    revision 0",Available message changed from "StaticPodsAvailable: 0 nodes are active;
    " to "StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:13Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:13Z"
    name: kube-controller-manager-operator.179e33a045dc57bb
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "5950"
    uid: 25302482-e12c-4a2c-86ae-93f7e477f463
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:13Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:13Z"
  message: Created ConfigMap/cloud-config -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:14Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:14Z"
    name: kube-controller-manager-operator.179e33a072b0ee6c
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "5980"
    uid: a599c098-1d6b-45d6-bf96-e0db3c3ca684
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-resource-sync-controller-resourcesynccontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-resource-sync-controller-resourcesynccontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:13Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:13Z"
  message: Created Secret/csr-signer-signer -n openshift-kube-controller-manager-operator
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:14Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:14Z"
    name: kube-controller-manager-operator.179e33a07ea9afe6
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6005"
    uid: 6da78587-2b4e-42c6-953a-6673e0fa9c26
  reason: SecretCreated
  reportingComponent: kube-controller-manager-operator
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:13Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:13Z"
  message: '"csr-controller-signer-ca" in "openshift-kube-controller-manager-operator"
    requires a new cert'
  metadata:
    creationTimestamp: "2023-12-06T09:21:16Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:16Z"
    name: kube-controller-manager-operator.179e33a07eb04022
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6263"
    uid: ffe7960b-053c-4dec-844c-847b1e79ca90
  reason: CABundleUpdateRequired
  reportingComponent: kube-controller-manager-operator
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:13Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:13Z"
  message: Created ServiceAccount/installer-sa -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:14Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:14Z"
    name: kube-controller-manager-operator.179e33a08a7f1ef8
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6053"
    uid: b9f88c80-956b-4cb1-9104-7f4266507ceb
  reason: ServiceAccountCreated
  reportingComponent: kube-controller-manager-operator-backingresourcecontroller-backingresourcecontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-backingresourcecontroller-backingresourcecontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:13Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:13Z"
  message: Created ClusterRoleBinding.rbac.authorization.k8s.io/system:openshift:operator:openshift-kube-controller-manager-installer
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:16Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:16Z"
    name: kube-controller-manager-operator.179e33a08ad1128b
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6317"
    uid: 1eccf1e8-0a2e-4db6-ae9a-10c6d88c69bc
  reason: ClusterRoleBindingCreated
  reportingComponent: kube-controller-manager-operator-backingresourcecontroller-backingresourcecontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-backingresourcecontroller-backingresourcecontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:13Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:13Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key,
    configmaps: cluster-policy-controller-config-0,config-0,controller-manager-kubeconfig-0,kube-controller-cert-syncer-kubeconfig-0,kube-controller-manager-pod-0,recycler-config-0,service-ca-0,serviceaccount-ca-0,
    secrets: localhost-recovery-client-token-0,service-account-private-key-0]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)" to "InstallerControllerDegraded: missing
    required resources: [configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key,
    configmaps: cluster-policy-controller-config-0,config-0,controller-manager-kubeconfig-0,kube-controller-cert-syncer-kubeconfig-0,kube-controller-manager-pod-0,recycler-config-0,service-ca-0,serviceaccount-ca-0,
    secrets: localhost-recovery-client-token-0,service-account-private-key-0]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:15Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:15Z"
    name: kube-controller-manager-operator.179e33a08cb91db2
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6092"
    uid: 0607c70f-658c-4a17-a26c-f4399049071c
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:14Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:14Z"
  message: Updated Namespace/openshift-kube-controller-manager because it changed
  metadata:
    creationTimestamp: "2023-12-06T09:21:15Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:15Z"
    name: kube-controller-manager-operator.179e33a0966c45bf
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6130"
    uid: 6b075502-510c-47ca-9885-513c32fce4bd
  reason: NamespaceUpdated
  reportingComponent: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 40
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:14Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:28Z"
  message: 'unexpected addresses: 10.0.154.98'
  metadata:
    creationTimestamp: "2023-12-06T09:21:15Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:28Z"
    name: kube-controller-manager-operator.179e33a0ae33f31b
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "10643"
    uid: 6d4f73b8-5694-4ebe-ac4a-444d0b8c75b2
  reason: SATokenSignerControllerStuck
  reportingComponent: kube-controller-manager-operator-satokensignercontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-satokensignercontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:14Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:14Z"
  message: Deleted target configmap openshift-config-managed/csr-controller-ca because
    source config does not exist
  metadata:
    creationTimestamp: "2023-12-06T09:21:16Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:16Z"
    name: kube-controller-manager-operator.179e33a0ba372134
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6281"
    uid: 4430ab41-8d2e-4a23-bbfc-53d881256bbc
  reason: TargetConfigDeleted
  reportingComponent: kube-controller-manager-operator-resource-sync-controller-resourcesynccontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-resource-sync-controller-resourcesynccontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:14Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:14Z"
  message: Created ConfigMap/csr-controller-signer-ca -n openshift-kube-controller-manager-operator
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:17Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:17Z"
    name: kube-controller-manager-operator.179e33a0c61f243f
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6475"
    uid: 0b163209-b4f7-4225-8f80-62fe79adec80
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:14Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:14Z"
  message: '"csr-signer" in "openshift-kube-controller-manager-operator" requires
    a new target cert/key pair: missing notAfter'
  metadata:
    creationTimestamp: "2023-12-06T09:21:19Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:19Z"
    name: kube-controller-manager-operator.179e33a0c6224804
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6656"
    uid: 7cf96599-ce1c-49bb-8458-14a70ff34873
  reason: TargetUpdateRequired
  reportingComponent: kube-controller-manager-operator
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:15Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:15Z"
  message: Created ConfigMap/kube-controller-cert-syncer-kubeconfig -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:17Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:17Z"
    name: kube-controller-manager-operator.179e33a0d2035c91
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6401"
    uid: 75c477ea-1da1-4adb-b5a4-40d36757101f
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:15Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:15Z"
  message: Created RoleBinding.rbac.authorization.k8s.io/system:openshift:leader-locking-kube-controller-manager
    -n kube-system because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:18Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:18Z"
    name: kube-controller-manager-operator.179e33a0d25d9223
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6611"
    uid: 8bbfeefc-bc1c-40a1-b8d2-3144668dbfff
  reason: RoleBindingCreated
  reportingComponent: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:15Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:15Z"
  message: Created Role.rbac.authorization.k8s.io/system:openshift:leader-election-lock-cluster-policy-controller
    -n openshift-kube-controller-manager because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:20Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:20Z"
    name: kube-controller-manager-operator.179e33a0d29a49d7
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6758"
    uid: 19288814-9f70-4589-aaf6-033c91cc388c
  reason: RoleCreated
  reportingComponent: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:15Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:15Z"
  message: Created RoleBinding.rbac.authorization.k8s.io/system:openshift:leader-election-lock-cluster-policy-controller
    -n openshift-kube-controller-manager because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:21Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:21Z"
    name: kube-controller-manager-operator.179e33a0d2eb0042
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6831"
    uid: cdbfb7eb-cacd-4c7a-9f6f-34ec4a9662a7
  reason: RoleBindingCreated
  reportingComponent: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:15Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:15Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key,
    configmaps: cluster-policy-controller-config-0,config-0,controller-manager-kubeconfig-0,kube-controller-cert-syncer-kubeconfig-0,kube-controller-manager-pod-0,recycler-config-0,service-ca-0,serviceaccount-ca-0,
    secrets: localhost-recovery-client-token-0,service-account-private-key-0]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)" to "InstallerControllerDegraded: missing
    required resources: [configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key,
    configmaps: cluster-policy-controller-config-0,config-0,controller-manager-kubeconfig-0,kube-controller-cert-syncer-kubeconfig-0,kube-controller-manager-pod-0,recycler-config-0,service-ca-0,serviceaccount-ca-0,
    secrets: localhost-recovery-client-token-0,service-account-private-key-0]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:17Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:17Z"
    name: kube-controller-manager-operator.179e33a0dfec2cec
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6375"
    uid: 7dedc439-9406-4e74-8510-17c2cf929d4b
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:15Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:15Z"
  message: Created Secret/service-account-private-key -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:17Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:17Z"
    name: kube-controller-manager-operator.179e33a0e9e4e651
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6458"
    uid: a9b87882-4794-4191-8f15-0ef8e071f578
  reason: SecretCreated
  reportingComponent: kube-controller-manager-operator-satokensignercontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-satokensignercontroller
  type: Normal
- apiVersion: v1
  count: 3
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:15Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:41Z"
  message: 'Failed to write observed config: Operation cannot be fulfilled on kubecontrollermanagers.operator.openshift.io
    "cluster": the object has been modified; please apply your changes to the latest
    version and try again'
  metadata:
    creationTimestamp: "2023-12-06T09:21:20Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:41Z"
    name: kube-controller-manager-operator.179e33a0eb47a438
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7752"
    uid: 00376077-5cbe-4119-adf2-2fbfb548766e
  reason: ObservedConfigWriteError
  reportingComponent: kube-controller-manager-operator-config-observer-configobserver
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-config-observer-configobserver
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:15Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:15Z"
  message: Created ConfigMap/service-ca -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:18Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:18Z"
    name: kube-controller-manager-operator.179e33a0f5c8e7c1
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6503"
    uid: eb716c19-2b9f-4443-b76e-09eb27a27c65
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-resource-sync-controller-resourcesynccontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-resource-sync-controller-resourcesynccontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:15Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:15Z"
  message: Created Secret/csr-signer -n openshift-kube-controller-manager-operator
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:20Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:20Z"
    name: kube-controller-manager-operator.179e33a1022f38f3
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6793"
    uid: 5645fe4b-3fab-4fa5-9d99-489c54ed33af
  reason: SecretCreated
  reportingComponent: kube-controller-manager-operator
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator
  type: Normal
- apiVersion: v1
  count: 5
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:16Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:22Z"
  message: 'configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key,
    configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1'
  metadata:
    creationTimestamp: "2023-12-06T09:21:18Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:23Z"
    name: kube-controller-manager-operator.179e33a127143d94
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6983"
    uid: c3a04884-ec86-4318-8c07-cc0f7813c150
  reason: RequiredInstallerResourcesMissing
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Warning
- apiVersion: v1
  count: 33
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:16Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:00Z"
  message: new revision 2 triggered by "configmap \"kube-controller-manager-pod\"
    not found"
  metadata:
    creationTimestamp: "2023-12-06T09:21:18Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:00Z"
    name: kube-controller-manager-operator.179e33a12716870f
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8999"
    uid: d9426b15-945b-4958-9458-03c75c7b78e0
  reason: RevisionTriggered
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:17Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:17Z"
  message: Created ConfigMap/revision-status-2 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:19Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:19Z"
    name: kube-controller-manager-operator.179e33a149647927
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6683"
    uid: 6b29ac88-e5f8-4384-90eb-c91493648418
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:17Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:17Z"
  message: Created Service/kube-controller-manager -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:22Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:22Z"
    name: kube-controller-manager-operator.179e33a1614b2ba4
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6940"
    uid: c4bed677-a4c8-4f96-baa6-46a4f3396538
  reason: ServiceCreated
  reportingComponent: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:17Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:17Z"
  message: 'Failed to create ConfigMap/revision-status-2 -n openshift-kube-controller-manager:
    configmaps "revision-status-2" already exists'
  metadata:
    creationTimestamp: "2023-12-06T09:21:21Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:21Z"
    name: kube-controller-manager-operator.179e33a16d05edb7
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6912"
    uid: 5bd2cc0b-295e-471c-b708-621e983cd50c
  reason: ConfigMapCreateFailed
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:18Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:18Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key,
    configmaps: cluster-policy-controller-config-0,config-0,controller-manager-kubeconfig-0,kube-controller-cert-syncer-kubeconfig-0,kube-controller-manager-pod-0,recycler-config-0,service-ca-0,serviceaccount-ca-0,
    secrets: localhost-recovery-client-token-0,service-account-private-key-0]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found" to "InstallerControllerDegraded: missing required resources: [configmaps:
    aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key,
    configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found",Progressing changed from False to True ("NodeInstallerProgressing:
    3 nodes are at revision 0; 0 nodes have achieved new revision 1"),Available message
    changed from "StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision
    0" to "StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes
    have achieved new revision 1"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:19Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:19Z"
    name: kube-controller-manager-operator.179e33a19ee1eb94
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6694"
    uid: 768651e0-d07c-4140-bf4e-a05c258ea5b7
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:18Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:18Z"
  message: Created ServiceAccount/kube-controller-manager-sa -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:22Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:22Z"
    name: kube-controller-manager-operator.179e33a1a8bd72e2
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6967"
    uid: 0f298109-d7a3-44b4-8b26-60154e110e5f
  reason: ServiceAccountCreated
  reportingComponent: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:19Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:19Z"
  message: Created ServiceAccount/pv-recycler-controller -n openshift-infra because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:22Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:22Z"
    name: kube-controller-manager-operator.179e33a1d855c994
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6975"
    uid: dffc239b-6cf6-46b5-8282-d0dd935cfaf4
  reason: ServiceAccountCreated
  reportingComponent: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:19Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:19Z"
  message: Created ClusterRoleBinding.rbac.authorization.k8s.io/system:openshift:operator:kube-controller-manager-recovery
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:23Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:23Z"
    name: kube-controller-manager-operator.179e33a1d8bc1b9e
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6992"
    uid: 790d01d8-0c3b-4d47-abd9-b394ddfe385f
  reason: ClusterRoleBindingCreated
  reportingComponent: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:20Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:20Z"
  message: Created ServiceAccount/localhost-recovery-client -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:23Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:23Z"
    name: kube-controller-manager-operator.179e33a207f8df97
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7007"
    uid: 1f85b04c-1cbd-47cd-a649-f1b87611e8cb
  reason: ServiceAccountCreated
  reportingComponent: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:20Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:20Z"
  message: Created Secret/localhost-recovery-client-token -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:23Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:23Z"
    name: kube-controller-manager-operator.179e33a213fa9897
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7012"
    uid: 14d0335b-7501-475d-a533-90650530ebf3
  reason: SecretCreated
  reportingComponent: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:20Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:20Z"
  message: Created ClusterRole.rbac.authorization.k8s.io/system:openshift:controller:cluster-csr-approver-controller
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:24Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:24Z"
    name: kube-controller-manager-operator.179e33a2143d52c5
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7018"
    uid: 8a2f0137-0e1d-4e75-90aa-a0f02511c016
  reason: ClusterRoleCreated
  reportingComponent: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:20Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:20Z"
  message: Created ClusterRoleBinding.rbac.authorization.k8s.io/system:openshift:controller:cluster-csr-approver-controller
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:24Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:24Z"
    name: kube-controller-manager-operator.179e33a2148bcd92
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7026"
    uid: 7773283f-5707-452e-9415-3d52ba643047
  reason: ClusterRoleBindingCreated
  reportingComponent: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:20Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:20Z"
  message: Created ClusterRole.rbac.authorization.k8s.io/system:openshift:kube-controller-manager:gce-cloud-provider
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:24Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:24Z"
    name: kube-controller-manager-operator.179e33a214dd13c9
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7033"
    uid: 15a460ac-f8e5-41f2-a0b0-6d5d3405992f
  reason: ClusterRoleCreated
  reportingComponent: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:20Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:20Z"
  message: Created ClusterRoleBinding.rbac.authorization.k8s.io/system:openshift:kube-controller-manager:gce-cloud-provider
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:25Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:25Z"
    name: kube-controller-manager-operator.179e33a21526075b
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7054"
    uid: 42232f00-6764-4ef0-bd53-c87e2627834a
  reason: ClusterRoleBindingCreated
  reportingComponent: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-kubecontrollermanagerstaticresources-kubecontrollermanagerstaticresources
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:20Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:20Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key,
    configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found" to "InstallerControllerDegraded: missing required resources: [configmaps:
    aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key,
    configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:21Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:21Z"
    name: kube-controller-manager-operator.179e33a221d94705
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "6861"
    uid: d7a1f424-ea07-455e-a473-23cc62522269
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 3
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:24Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:41Z"
  message: CloudProvider config file changed to /etc/kubernetes/static-pod-resources/configmaps/cloud-config/cloud.conf
  metadata:
    creationTimestamp: "2023-12-06T09:21:24Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:42Z"
    name: kube-controller-manager-operator.179e33a30fb37600
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7887"
    uid: ca574f32-b781-4253-b1b7-57f14b88c599
  reason: ObserveCloudProviderNamesChanges
  reportingComponent: kube-controller-manager-operator-config-observer-configobserver
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-config-observer-configobserver
  type: Normal
- apiVersion: v1
  count: 3
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:24Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:41Z"
  message: observed change in config
  metadata:
    creationTimestamp: "2023-12-06T09:21:27Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:43Z"
    name: kube-controller-manager-operator.179e33a30fb4f3d8
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7981"
    uid: 33df2f4c-17ce-49b1-9da2-c5406becc41e
  reason: ObserveServiceCAConfigMap
  reportingComponent: kube-controller-manager-operator-config-observer-configobserver
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-config-observer-configobserver
  type: Normal
- apiVersion: v1
  count: 3
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:24Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:41Z"
  message: "Writing updated observed config:   map[string]any{\n+ \t\"extendedArguments\":
    map[string]any{\n+ \t\t\"cloud-config\":   []any{string(\"/etc/kubernetes/static-pod-resources/configmaps/cloud-config/clo\"...)},\n+ \t\t\"cloud-provider\":
    []any{string(\"external\")},\n+ \t\t\"cluster-cidr\":   []any{string(\"10.128.0.0/14\")},\n+ \t\t\"cluster-name\":
    \  []any{string(\"ci-op-2j285qtr-234c7-w84r8\")},\n+ \t\t\"feature-gates\": []any{\n+ \t\t\tstring(\"AdminNetworkPolicy=false\"),
    string(\"AlibabaPlatform=true\"),\n+ \t\t\tstring(\"AutomatedEtcdBackup=false\"),
    string(\"AzureWorkloadIdentity=true\"),\n+ \t\t\tstring(\"BuildCSIVolumes=true\"),
    string(\"CSIDriverSharedResource=false\"),\n+ \t\t\tstring(\"CloudDualStackNodeIPs=true\"),
    string(\"ClusterAPIInstall=false\"), ...,\n+ \t\t},\n+ \t\t\"service-cluster-ip-range\":
    []any{string(\"172.30.0.0/16\")},\n+ \t},\n+ \t\"featureGates\": []any{\n+ \t\tstring(\"AdminNetworkPolicy=false\"),
    string(\"AlibabaPlatform=true\"),\n+ \t\tstring(\"AutomatedEtcdBackup=false\"),
    string(\"AzureWorkloadIdentity=true\"),\n+ \t\tstring(\"BuildCSIVolumes=true\"),
    string(\"CSIDriverSharedResource=false\"),\n+ \t\tstring(\"CloudDualStackNodeIPs=true\"),
    string(\"ClusterAPIInstall=false\"),\n+ \t\tstring(\"DNSNameResolver=false\"),\n+ \t\tstring(\"DisableKubeletCloudCredentialProviders=false\"),\n+ \t\tstring(\"DynamicResourceAllocation=false\"),
    string(\"EventedPLEG=false\"),\n+ \t\tstring(\"ExternalCloudProvider=true\"),\n+ \t\tstring(\"ExternalCloudProviderAzure=true\"),\n+ \t\tstring(\"ExternalCloudProviderExternal=true\"),\n+ \t\tstring(\"ExternalCloudProviderGCP=true\"),
    ...,\n+ \t},\n+ \t\"serviceServingCert\": map[string]any{\n+ \t\t\"certFile\":
    string(\"/etc/kubernetes/static-pod-resources/configmaps/service-ca/ca-bundle.crt\"),\n+ \t},\n+ \t\"servingInfo\":
    map[string]any{\n+ \t\t\"cipherSuites\": []any{\n+ \t\t\tstring(\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\"),\n+ \t\t\tstring(\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\"),\n+ \t\t\tstring(\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\"),\n+ \t\t\tstring(\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\"),\n+ \t\t\tstring(\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\"),\n+ \t\t\tstring(\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"),\n+ \t\t},\n+ \t\t\"minTLSVersion\":
    string(\"VersionTLS12\"),\n+ \t},\n  }\n"
  metadata:
    creationTimestamp: "2023-12-06T09:21:28Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:46Z"
    name: kube-controller-manager-operator.179e33a30fbc0daa
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8234"
    uid: 112e5fae-d9ed-4afc-a673-23c816e45386
  reason: ObservedConfigChanged
  reportingComponent: kube-controller-manager-operator-config-observer-configobserver
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-config-observer-configobserver
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:24Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:24Z"
  message: Created ConfigMap/aggregator-client-ca -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:25Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:25Z"
    name: kube-controller-manager-operator.179e33a30ffe7dee
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7047"
    uid: bb264b18-964b-4dd3-b412-2ad6bbc12b32
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-resource-sync-controller-resourcesynccontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-resource-sync-controller-resourcesynccontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:24Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:24Z"
  message: |-
    Updated ConfigMap/cloud-config -n openshift-kube-controller-manager:
    cause by changes in data.cloud.conf,data.config
  metadata:
    creationTimestamp: "2023-12-06T09:21:26Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:26Z"
    name: kube-controller-manager-operator.179e33a3102c5d08
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7093"
    uid: fed7148c-7c7c-4a7a-8966-8e59d34820ef
  reason: ConfigMapUpdated
  reportingComponent: kube-controller-manager-operator-resource-sync-controller-resourcesynccontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-resource-sync-controller-resourcesynccontroller
  type: Normal
- apiVersion: v1
  count: 5
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:25Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:32Z"
  message: 'configmaps: client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key,
    configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1'
  metadata:
    creationTimestamp: "2023-12-06T09:21:25Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:32Z"
    name: kube-controller-manager-operator.179e33a3280f73d3
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7410"
    uid: b4fac026-94c5-41f3-9df1-195aff2ac655
  reason: RequiredInstallerResourcesMissing
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:25Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:25Z"
  message: 'Failed to create revision 2: configmaps "revision-status-2" already exists'
  metadata:
    creationTimestamp: "2023-12-06T09:21:26Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:26Z"
    name: kube-controller-manager-operator.179e33a33f4c705e
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7103"
    uid: 28f075ad-73f1-4895-8ea4-baa453144942
  reason: RevisionCreateFailed
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:27Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:27Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key,
    configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found" to "InstallerControllerDegraded: missing required resources: [configmaps:
    client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps:
    cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:27Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:27Z"
    name: kube-controller-manager-operator.179e33a39f7e7394
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7163"
    uid: 6beaf743-9b3c-432a-9b17-ab426ff74593
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:30Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:30Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps:
    cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found" to "InstallerControllerDegraded: missing required resources: [configmaps:
    client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps:
    cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:30Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:30Z"
    name: kube-controller-manager-operator.179e33a4761090c2
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7326"
    uid: f0ae8ee6-6c86-4d4e-a6d2-5bb58ce94d3d
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:32Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:32Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps:
    cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found" to "InstallerControllerDegraded: missing required resources: [configmaps:
    client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps:
    cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: Operation cannot
    be fulfilled on kubecontrollermanagers.operator.openshift.io \"cluster\": the
    object has been modified; please apply your changes to the latest version and
    try again"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:32Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:32Z"
    name: kube-controller-manager-operator.179e33a4e184788a
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7413"
    uid: b1261dd3-48b8-48e9-aa12-c0ea40c985cf
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:32Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:32Z"
  message: Created Secret/kube-controller-manager-client-cert-key -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:32Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:32Z"
    name: kube-controller-manager-operator.179e33a4eb2833dd
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7420"
    uid: 5a40526c-a972-41c1-a92e-0bed3919bc14
  reason: SecretCreated
  reportingComponent: kube-controller-manager-operator-resource-sync-controller-resourcesynccontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-resource-sync-controller-resourcesynccontroller
  type: Normal
- apiVersion: v1
  count: 12
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:33Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:44Z"
  message: 'configmaps: client-ca, secrets: csr-signer, configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1'
  metadata:
    creationTimestamp: "2023-12-06T09:21:33Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:45Z"
    name: kube-controller-manager-operator.179e33a5344367d1
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8218"
    uid: 452b3c76-2087-45f2-9360-48d8d58582b5
  reason: RequiredInstallerResourcesMissing
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:33Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:33Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps:
    cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: Operation cannot
    be fulfilled on kubecontrollermanagers.operator.openshift.io \"cluster\": the
    object has been modified; please apply your changes to the latest version and
    try again" to "InstallerControllerDegraded: missing required resources: [configmaps:
    client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps:
    cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:33Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:33Z"
    name: kube-controller-manager-operator.179e33a534cf7a95
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7464"
    uid: 1638d9ef-49e3-4032-9f3d-e1e8007d640e
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:35Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:35Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps:
    cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found" to "InstallerControllerDegraded: missing required resources: [configmaps:
    client-ca, secrets: csr-signer, configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:35Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:35Z"
    name: kube-controller-manager-operator.179e33a594293ff4
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7542"
    uid: 1d9f3ad9-77c4-4118-b468-ad59e4e3e61f
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:38Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:38Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    client-ca, secrets: csr-signer, configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found" to "InstallerControllerDegraded: missing required resources: [configmaps:
    client-ca, secrets: csr-signer, configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:38Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:38Z"
    name: kube-controller-manager-operator.179e33a646ce753e
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7682"
    uid: 0d770b98-74b0-4d17-a621-fc43d9f7e83c
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:40Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:40Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    client-ca, secrets: csr-signer, configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]" to "InstallerControllerDegraded:
    missing required resources: [configmaps: client-ca, secrets: csr-signer, configmaps:
    cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:40Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:40Z"
    name: kube-controller-manager-operator.179e33a6a6441911
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7714"
    uid: 5f4fe47a-d0b3-4761-a3c1-9e37e1d69b51
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:40Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:40Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    client-ca, secrets: csr-signer, configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal]" to "InstallerControllerDegraded:
    missing required resources: [configmaps: client-ca, secrets: csr-signer, configmaps:
    cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:40Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:40Z"
    name: kube-controller-manager-operator.179e33a6d5f23715
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7738"
    uid: 3664696d-73d7-4386-9093-b88fea0a23ca
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:41Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:41Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    client-ca, secrets: csr-signer, configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]" to "InstallerControllerDegraded:
    missing required resources: [configmaps: client-ca, secrets: csr-signer, configmaps:
    cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]\nConfigObservationDegraded:
    error writing updated observed config: Operation cannot be fulfilled on kubecontrollermanagers.operator.openshift.io
    \"cluster\": the object has been modified; please apply your changes to the latest
    version and try again"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:42Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:42Z"
    name: kube-controller-manager-operator.179e33a7118613ff
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "7849"
    uid: 45ff5160-2521-447d-9925-873f2d465fb1
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:43Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:43Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    client-ca, secrets: csr-signer, configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    The master nodes not ready: node \"ip-10-0-94-160.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:19:02 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?), node \"ip-10-0-106-212.us-west-1.compute.internal\"
    not ready since 2023-12-06 09:18:38 +0000 UTC because KubeletNotReady (container
    runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network
    plugin returns error: No CNI configuration file in /etc/kubernetes/cni/net.d/.
    Has your network provider started?)\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]\nConfigObservationDegraded:
    error writing updated observed config: Operation cannot be fulfilled on kubecontrollermanagers.operator.openshift.io
    \"cluster\": the object has been modified; please apply your changes to the latest
    version and try again" to "InstallerControllerDegraded: missing required resources:
    [configmaps: client-ca, secrets: csr-signer, configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]\nConfigObservationDegraded:
    error writing updated observed config: Operation cannot be fulfilled on kubecontrollermanagers.operator.openshift.io
    \"cluster\": the object has been modified; please apply your changes to the latest
    version and try again"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:44Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:44Z"
    name: kube-controller-manager-operator.179e33a789fbb9ca
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8185"
    uid: 8fd9fbc5-f521-49ce-ab96-0e6d8d25e141
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:44Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:44Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    client-ca, secrets: csr-signer, configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]\nConfigObservationDegraded:
    error writing updated observed config: Operation cannot be fulfilled on kubecontrollermanagers.operator.openshift.io
    \"cluster\": the object has been modified; please apply your changes to the latest
    version and try again" to "InstallerControllerDegraded: missing required resources:
    [configmaps: client-ca, secrets: csr-signer, configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:45Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:45Z"
    name: kube-controller-manager-operator.179e33a7ad7717ca
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8215"
    uid: 4a3c61d0-8cd1-4783-a7d8-261f9024f1db
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:44Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:44Z"
  message: Created ConfigMap/config -n openshift-kube-controller-manager because it
    was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:45Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:45Z"
    name: kube-controller-manager-operator.179e33a7b6c3e84f
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8217"
    uid: 9213c51f-50a8-447f-b480-1477c73690e0
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-targetconfigcontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-targetconfigcontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:47Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:47Z"
  message: Created ConfigMap/cluster-policy-controller-config -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:47Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:47Z"
    name: kube-controller-manager-operator.179e33a85d397678
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8241"
    uid: 9558e63d-43cd-49fe-bac8-555d4ce380ab
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-targetconfigcontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-targetconfigcontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:49Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:49Z"
  message: Created ConfigMap/recycler-config -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:49Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:49Z"
    name: kube-controller-manager-operator.179e33a8bc9529d5
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8274"
    uid: 407c14d5-26aa-4b6d-b134-af2098770328
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-targetconfigcontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-targetconfigcontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:50Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:50Z"
  message: Created ConfigMap/client-ca -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:50Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:50Z"
    name: kube-controller-manager-operator.179e33a8f845b171
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8337"
    uid: de537c33-ea31-4158-89ce-ae6bac42c214
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-resource-sync-controller-resourcesynccontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-resource-sync-controller-resourcesynccontroller
  type: Normal
- apiVersion: v1
  count: 3
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:50Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:51Z"
  message: 'secrets: csr-signer, configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1'
  metadata:
    creationTimestamp: "2023-12-06T09:21:50Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:51Z"
    name: kube-controller-manager-operator.179e33a9120858c9
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8461"
    uid: 8414a5a6-4347-4b6e-88df-3f08a561ff53
  reason: RequiredInstallerResourcesMissing
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:50Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:50Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    client-ca, secrets: csr-signer, configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]" to "InstallerControllerDegraded:
    missing required resources: [configmaps: client-ca, secrets: csr-signer, configmaps:
    cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:50Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:50Z"
    name: kube-controller-manager-operator.179e33a9122d4618
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8357"
    uid: f5105afc-396d-430f-bcc2-06f3487634bd
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:50Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:50Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    client-ca, secrets: csr-signer, configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal]" to "InstallerControllerDegraded:
    missing required resources: [secrets: csr-signer, configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:50Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:50Z"
    name: kube-controller-manager-operator.179e33a91483d978
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8368"
    uid: bdd4f3d4-57d6-4ec3-8307-89b353441b67
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:51Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:51Z"
  message: Created ConfigMap/csr-signer-ca -n openshift-kube-controller-manager-operator
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:51Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:51Z"
    name: kube-controller-manager-operator.179e33a933ee6cc0
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8395"
    uid: 2b331a6f-47c9-40f2-bfac-c559c162834c
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-targetconfigcontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-targetconfigcontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:51Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:51Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [secrets:
    csr-signer, configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal]" to "InstallerControllerDegraded:
    missing required resources: [secrets: csr-signer, configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:21:51Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:51Z"
    name: kube-controller-manager-operator.179e33a9656f7336
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8464"
    uid: c1bc2a5b-81cd-4819-bb2c-f9a364701231
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:53Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:53Z"
  message: Created ConfigMap/csr-controller-ca -n openshift-kube-controller-manager-operator
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:53Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:53Z"
    name: kube-controller-manager-operator.179e33a9ab270e6d
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8504"
    uid: 0ca36ff3-f4b3-4603-b9f1-33f0a6a01534
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-targetconfigcontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-targetconfigcontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:54Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:54Z"
  message: Created ConfigMap/csr-controller-ca -n openshift-config-managed because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:54Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:54Z"
    name: kube-controller-manager-operator.179e33a9f2da3dc5
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8579"
    uid: 7b12ec6d-6619-4825-8160-9248887fcd51
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-resource-sync-controller-resourcesynccontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-resource-sync-controller-resourcesynccontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:55Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:55Z"
  message: Created Secret/csr-signer -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:55Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:55Z"
    name: kube-controller-manager-operator.179e33aa52a21f8b
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8674"
    uid: fd05d0e8-fb90-4457-96f7-89a4d4fc1363
  reason: SecretCreated
  reportingComponent: kube-controller-manager-operator-targetconfigcontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-targetconfigcontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:57Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:57Z"
  message: Created ConfigMap/serviceaccount-ca -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:57Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:57Z"
    name: kube-controller-manager-operator.179e33aaa5c40182
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8761"
    uid: 82c36342-d48f-4734-814a-ea961238fcf2
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-targetconfigcontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-targetconfigcontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:21:59Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:21:59Z"
  message: Created ConfigMap/controller-manager-kubeconfig -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:21:59Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:21:59Z"
    name: kube-controller-manager-operator.179e33ab10be54ce
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8875"
    uid: 79217dd5-81fd-4ffb-9c36-3efd3b6582e0
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-targetconfigcontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-targetconfigcontroller
  type: Normal
- apiVersion: v1
  count: 10
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:00Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:13Z"
  message: 'configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1'
  metadata:
    creationTimestamp: "2023-12-06T09:22:00Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:13Z"
    name: kube-controller-manager-operator.179e33ab6950618e
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9607"
    uid: 5faffa2f-b018-4b03-9d8e-91b6f43586a2
  reason: RequiredInstallerResourcesMissing
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:00Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:00Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [secrets:
    csr-signer, configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]" to "InstallerControllerDegraded:
    missing required resources: [secrets: csr-signer, configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:00Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:00Z"
    name: kube-controller-manager-operator.179e33ab6a4a4df3
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8975"
    uid: efe82f9c-c5e5-47bf-839d-dc0e5872e89c
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:00Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:00Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [secrets:
    csr-signer, configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal]" to "InstallerControllerDegraded:
    missing required resources: [configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:00Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:00Z"
    name: kube-controller-manager-operator.179e33ab6bab87b3
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "8981"
    uid: fef3f977-a1b6-49bb-84b9-7ef6123a2e1a
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 2
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:00Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:10Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal]" to "InstallerControllerDegraded:
    missing required resources: [configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:00Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:10Z"
    name: kube-controller-manager-operator.179e33ab75baa107
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9454"
    uid: cf020354-e1ca-47fa-bf89-094fb4e5bf00
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:00Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:00Z"
  message: Created ConfigMap/kube-controller-manager-pod -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:22:00Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:00Z"
    name: kube-controller-manager-operator.179e33ab7c10d845
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9016"
    uid: 1bdf0409-7acd-47ce-ac3f-902e50e437fb
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-targetconfigcontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-targetconfigcontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:00Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:00Z"
  message: new revision 2 triggered by "configmap \"kube-controller-manager-pod-1\"
    not found"
  metadata:
    creationTimestamp: "2023-12-06T09:22:00Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:00Z"
    name: kube-controller-manager-operator.179e33ab7c1db66a
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9013"
    uid: 004f7596-fe27-4e3b-9a54-3f2b918854cb
  reason: RevisionTriggered
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:02Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:02Z"
  message: |-
    Updated ConfigMap/revision-status-2 -n openshift-kube-controller-manager:
    cause by changes in data.reason
  metadata:
    creationTimestamp: "2023-12-06T09:22:02Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:02Z"
    name: kube-controller-manager-operator.179e33abc37f80c8
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9123"
    uid: fe50a6c1-a5f6-49ed-a60e-be505e7e57ac
  reason: ConfigMapUpdated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:03Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:03Z"
  message: Created ConfigMap/kube-controller-manager-pod-2 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:22:03Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:03Z"
    name: kube-controller-manager-operator.179e33ac0b43fd0a
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9195"
    uid: 872bc907-6534-4b97-8dd1-3f77550373cb
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:04Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:04Z"
  message: Created ConfigMap/config-2 -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:22:04Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:04Z"
    name: kube-controller-manager-operator.179e33ac52b7a9c1
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9234"
    uid: 69d7fbaa-e77f-467b-8d74-cf5e9dc20eb7
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:05Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:05Z"
  message: Created ConfigMap/cluster-policy-controller-config-2 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:22:05Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:05Z"
    name: kube-controller-manager-operator.179e33ac9a00f719
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9253"
    uid: 0db5735e-7297-4939-b448-f2b43e36f0fd
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:06Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:06Z"
  message: Created ConfigMap/controller-manager-kubeconfig-2 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:22:06Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:06Z"
    name: kube-controller-manager-operator.179e33acd5aaf68a
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9272"
    uid: 6e9cfb27-03c1-47b2-946a-877ba137db37
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:07Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:07Z"
  message: Created ConfigMap/cloud-config-2 -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:22:07Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:07Z"
    name: kube-controller-manager-operator.179e33ad11507c5e
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9298"
    uid: ce468a0e-2617-4f13-9e6c-68b05a99fd6e
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:08Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:08Z"
  message: Created ConfigMap/kube-controller-cert-syncer-kubeconfig-2 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:22:08Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:08Z"
    name: kube-controller-manager-operator.179e33ad40f9912b
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9333"
    uid: 4f3c08dd-04f7-4241-a87c-e8fa4e339b60
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:09Z"
  message: Created ConfigMap/serviceaccount-ca-2 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:22:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:09Z"
    name: kube-controller-manager-operator.179e33ad70c844b8
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9350"
    uid: c1787c12-c164-41da-a172-7eee87f554f3
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:09Z"
  message: Created ConfigMap/service-ca-2 -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:22:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:09Z"
    name: kube-controller-manager-operator.179e33ad946c4239
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9358"
    uid: 89595976-19cb-430f-b5eb-cee92676390e
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:10Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:10Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]" to "InstallerControllerDegraded:
    missing required resources: [configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:10Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:10Z"
    name: kube-controller-manager-operator.179e33adccf26491
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9445"
    uid: 6a91a496-5662-4c99-91d9-cafadf7af4f3
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:11Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:11Z"
  message: Created ConfigMap/recycler-config-2 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:22:11Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:11Z"
    name: kube-controller-manager-operator.179e33addbdfb480
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9465"
    uid: 5a19d814-e0f9-4e2c-99c7-4dfaf8aae77c
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:11Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:11Z"
  message: |-
    Updated ConfigMap/client-ca -n openshift-kube-controller-manager:
    cause by changes in data.ca-bundle.crt
  metadata:
    creationTimestamp: "2023-12-06T09:22:11Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:11Z"
    name: kube-controller-manager-operator.179e33adffb631f2
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9474"
    uid: 7892585f-2b42-4849-a1ca-af9ce7d1277b
  reason: ConfigMapUpdated
  reportingComponent: kube-controller-manager-operator-resource-sync-controller-resourcesynccontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-resource-sync-controller-resourcesynccontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:12Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:12Z"
  message: Created Secret/service-account-private-key-2 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:22:12Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:12Z"
    name: kube-controller-manager-operator.179e33ae2fcfd82d
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9511"
    uid: 6f0d53f2-7ee7-4d6d-98c7-8a1176db1acb
  reason: SecretCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:12Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:12Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]" to "InstallerControllerDegraded:
    missing required resources: [configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-21-63.us-west-1.compute.internal,
    Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:12Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:12Z"
    name: kube-controller-manager-operator.179e33ae33fb374f
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9527"
    uid: 23d7e99f-873f-4aa7-bec7-e7d056807ac4
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:13Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:13Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-21-63.us-west-1.compute.internal,
    Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal]" to "InstallerControllerDegraded:
    missing required resources: [configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:13Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:13Z"
    name: kube-controller-manager-operator.179e33ae5a2cd688
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9614"
    uid: 841ee3a2-9c87-4dd7-93c1-94dcbd295ab7
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:13Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:13Z"
  message: Created Secret/serving-cert-2 -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:22:13Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:13Z"
    name: kube-controller-manager-operator.179e33ae6b3f5e12
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9686"
    uid: 9d6ded8d-ae09-4a33-903d-193254c7d846
  reason: SecretCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:14Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:14Z"
  message: Created Secret/localhost-recovery-client-token-2 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:22:14Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:14Z"
    name: kube-controller-manager-operator.179e33aeb2eed81c
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9902"
    uid: 10fdf106-7d37-4bd2-a278-b635b9455afe
  reason: SecretCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:14Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:14Z"
  message: Revision 2 created because configmap "kube-controller-manager-pod-1" not
    found
  metadata:
    creationTimestamp: "2023-12-06T09:22:14Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:14Z"
    name: kube-controller-manager-operator.179e33aeb6bb6afd
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9910"
    uid: 5d14f82f-69a3-47d0-8c18-a10de25c9e8d
  reason: RevisionCreate
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:14Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:14Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: configmaps \"kube-controller-manager-pod\"
    not found\nGuardControllerDegraded: [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal]" to "InstallerControllerDegraded:
    missing required resources: [configmaps: cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nGuardControllerDegraded: [Missing operand on node
    ip-10-0-94-160.us-west-1.compute.internal, Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:14Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:14Z"
    name: kube-controller-manager-operator.179e33aeb7f193a9
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "9914"
    uid: cc04ac5e-9cc3-4941-909b-d43c6f9f35dc
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:15Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:15Z"
  message: Updating node "ip-10-0-21-63.us-west-1.compute.internal" from revision
    0 to 2 because node ip-10-0-21-63.us-west-1.compute.internal static pod not found
  metadata:
    creationTimestamp: "2023-12-06T09:22:15Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:15Z"
    name: kube-controller-manager-operator.179e33aefa96bd2a
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "10059"
    uid: 682e96e7-2678-4549-bc12-041de9b7a326
  reason: NodeTargetRevisionChanged
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:15Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:15Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Progressing
    message changed from "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes
    have achieved new revision 1" to "NodeInstallerProgressing: 3 nodes are at revision
    0; 0 nodes have achieved new revision 2",Available message changed from "StaticPodsAvailable:
    0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision
    1" to "StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes
    have achieved new revision 2"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:15Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:15Z"
    name: kube-controller-manager-operator.179e33aefbb3d33d
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "10065"
    uid: e66b4eba-93fa-4c60-9506-26f4d6bb0b4b
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:15Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:15Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "InstallerControllerDegraded: missing required resources: [configmaps:
    cluster-policy-controller-config-1,config-1,controller-manager-kubeconfig-1,kube-controller-cert-syncer-kubeconfig-1,kube-controller-manager-pod-1,recycler-config-1,service-ca-1,serviceaccount-ca-1,
    secrets: localhost-recovery-client-token-1,service-account-private-key-1]\nNodeControllerDegraded:
    All master nodes are ready\nGuardControllerDegraded: [Missing operand on node
    ip-10-0-94-160.us-west-1.compute.internal, Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal]" to "NodeControllerDegraded:
    All master nodes are ready\nGuardControllerDegraded: [Missing operand on node
    ip-10-0-94-160.us-west-1.compute.internal, Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-21-63.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:15Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:15Z"
    name: kube-controller-manager-operator.179e33aefe46684a
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "10088"
    uid: 8e1c4083-c89c-454a-92b5-e9acf0c3b7dc
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 3
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:16Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:27Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand on node ip-10-0-21-63.us-west-1.compute.internal]"
    to "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:16Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:27Z"
    name: kube-controller-manager-operator.179e33af339ee96e
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "10596"
    uid: cb314637-8cee-4ba7-8d5e-c829559c15e5
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:17Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:17Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]"
    to "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand on node ip-10-0-106-212.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:17Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:17Z"
    name: kube-controller-manager-operator.179e33af49cd04b6
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "10191"
    uid: 94808543-4ec9-4754-a2bf-0195d4dcc021
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:17Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:17Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand on node ip-10-0-106-212.us-west-1.compute.internal]"
    to "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:17Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:17Z"
    name: kube-controller-manager-operator.179e33af4bdb42cd
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "10193"
    uid: 06d323e6-0acb-42d5-8347-bd302b44c099
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:18Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:18Z"
  message: Created Pod/installer-2-ip-10-0-21-63.us-west-1.compute.internal -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:22:18Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:18Z"
    name: kube-controller-manager-operator.179e33af8941c42d
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "10258"
    uid: 4a4e2080-23a1-42ef-af87-2f3accfceabe
  reason: PodCreated
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 2
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:26Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:26Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]"
    to "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand on node ip-10-0-21-63.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:26Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:26Z"
    name: kube-controller-manager-operator.179e33b15b45d3e1
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "10568"
    uid: d77d55e7-8fbd-4bd8-aa92-d2d5c76ddc32
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:28Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:28Z"
  message: 'Failed to create installer pod for revision 2 count 0 on node "ip-10-0-21-63.us-west-1.compute.internal":
    client rate limiter Wait returned an error: context canceled'
  metadata:
    creationTimestamp: "2023-12-06T09:22:28Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:28Z"
    name: kube-controller-manager-operator.179e33b20b9d22ef
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "10683"
    uid: 39a3e881-432f-4496-a5ff-42cfa06ac50b
  reason: InstallerPodFailed
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:29Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:29Z"
  message: FeatureGates updated to featuregates.Features{Enabled:[]v1.FeatureGateName{"AlibabaPlatform",
    "AzureWorkloadIdentity", "BuildCSIVolumes", "CloudDualStackNodeIPs", "ExternalCloudProvider",
    "ExternalCloudProviderAzure", "ExternalCloudProviderExternal", "ExternalCloudProviderGCP",
    "OpenShiftPodSecurityAdmission", "PrivateHostedZoneAWS"}, Disabled:[]v1.FeatureGateName{"AdminNetworkPolicy",
    "AutomatedEtcdBackup", "CSIDriverSharedResource", "ClusterAPIInstall", "DNSNameResolver",
    "DisableKubeletCloudCredentialProviders", "DynamicResourceAllocation", "EventedPLEG",
    "GCPClusterHostedDNS", "GCPLabelsTags", "GatewayAPI", "InsightsConfigAPI", "InstallAlternateInfrastructureAWS",
    "MachineAPIOperatorDisableMachineHealthCheckController", "MachineAPIProviderOpenStack",
    "MachineConfigNodes", "ManagedBootImages", "MaxUnavailableStatefulSet", "MetricsServer",
    "MixedCPUsAllocation", "NetworkLiveMigration", "NodeSwap", "RouteExternalCertificate",
    "SigstoreImageVerification", "VSphereControlPlaneMachineSet", "VSphereStaticIPs",
    "ValidatingAdmissionPolicy"}}
  metadata:
    creationTimestamp: "2023-12-06T09:22:29Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:29Z"
    name: kube-controller-manager-operator.179e33b2459195b0
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "10760"
    uid: 73f05e53-e8de-44d2-bb3c-086c45e8fe45
  reason: FeatureGatesInitialized
  reportingComponent: kube-controller-manager-operator
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator
  type: Normal
- apiVersion: v1
  count: 3
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:31Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:41Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]"
    to "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand on node ip-10-0-106-212.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:31Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:41Z"
    name: kube-controller-manager-operator.179e33b2a08ab9a1
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "11218"
    uid: 79e78eb3-7bde-40eb-a7fc-72983f674ffd
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 2
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:31Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:36Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand on node ip-10-0-106-212.us-west-1.compute.internal]"
    to "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:31Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:36Z"
    name: kube-controller-manager-operator.179e33b2a589a4cb
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "11021"
    uid: 6c19d97f-39ca-4fde-97fb-20cfbcf10e50
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:33Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:33Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]"
    to "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand on node ip-10-0-21-63.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:33Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:33Z"
    name: kube-controller-manager-operator.179e33b30f771ae0
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "10860"
    uid: 5c88c216-efba-4ceb-b628-49bd9d97f3cf
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 2
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:33Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:46Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand on node ip-10-0-21-63.us-west-1.compute.internal]"
    to "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:33Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:46Z"
    name: kube-controller-manager-operator.179e33b31ab06471
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "11331"
    uid: 8ae17888-dc60-4607-be1b-e6a31c9a87b9
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 64
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:34Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:36:33Z"
  message: 'unexpected addresses: 10.0.154.98'
  metadata:
    creationTimestamp: "2023-12-06T09:22:34Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:36:33Z"
    name: kube-controller-manager-operator.179e33b358af26ba
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "28561"
    uid: dac06f3d-fd8b-4b08-be75-808385eaab23
  reason: SATokenSignerControllerStuck
  reportingComponent: kube-controller-manager-operator-satokensignercontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-satokensignercontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:43Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:43Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand on node ip-10-0-106-212.us-west-1.compute.internal]"
    to "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand on node ip-10-0-21-63.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:43Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:43Z"
    name: kube-controller-manager-operator.179e33b57f7d34ed
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "11284"
    uid: 4658f802-75b8-46b5-b24b-7736aff30e90
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:57Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:57Z"
  message: clusteroperator/kube-controller-manager version "kube-controller-manager"
    changed from "" to "1.28.4"
  metadata:
    creationTimestamp: "2023-12-06T09:22:57Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:57Z"
    name: kube-controller-manager-operator.179e33b897d9793c
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "11618"
    uid: 11c5979d-f003-43f1-ad5e-cffb580147cc
  reason: OperatorVersionChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:57Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:57Z"
  message: clusteroperator/kube-controller-manager version "operator" changed from
    "" to "4.15.0-0.ci.test-2023-12-06-090630-ci-op-2j285qtr-latest"
  metadata:
    creationTimestamp: "2023-12-06T09:22:57Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:57Z"
    name: kube-controller-manager-operator.179e33b897d99e3b
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "11620"
    uid: 1b143c55-f6ac-4f18-9375-5c23d04a1e76
  reason: OperatorVersionChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:57Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:57Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: status.versions
    changed from [{"raw-internal" "4.15.0-0.ci.test-2023-12-06-090630-ci-op-2j285qtr-latest"}]
    to [{"raw-internal" "4.15.0-0.ci.test-2023-12-06-090630-ci-op-2j285qtr-latest"}
    {"kube-controller-manager" "1.28.4"} {"operator" "4.15.0-0.ci.test-2023-12-06-090630-ci-op-2j285qtr-latest"}]'
  metadata:
    creationTimestamp: "2023-12-06T09:22:57Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:57Z"
    name: kube-controller-manager-operator.179e33b89843fbc6
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "11621"
    uid: 8ff819fa-4740-408a-b910-e3cfeca6fa7a
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:57Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:57Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]"
    to "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal, Missing PodIP in operand kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal
    on node ip-10-0-21-63.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:57Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:57Z"
    name: kube-controller-manager-operator.179e33b89974d2e5
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "11623"
    uid: 0284b6f7-9d4d-4bb5-92f0-e76aab9cd460
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:57Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:57Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal, Missing PodIP in operand kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal
    on node ip-10-0-21-63.us-west-1.compute.internal]" to "NodeControllerDegraded:
    All master nodes are ready\nGuardControllerDegraded: [Missing PodIP in operand
    kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal on node ip-10-0-21-63.us-west-1.compute.internal,
    Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:57Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:57Z"
    name: kube-controller-manager-operator.179e33b8a42254d8
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "11630"
    uid: a5879afd-4ff5-4ea9-a325-8e7e4c0450cb
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:22:57Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:22:57Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing PodIP in operand kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal
    on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal]" to "NodeControllerDegraded:
    All master nodes are ready\nGuardControllerDegraded: [Missing operand on node
    ip-10-0-94-160.us-west-1.compute.internal, Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing PodIP in operand kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal
    on node ip-10-0-21-63.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:22:57Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:22:57Z"
    name: kube-controller-manager-operator.179e33b8a90edf98
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "11638"
    uid: a34880db-0a36-4d11-a93f-d62e1adc20ab
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:00Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:23:00Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal, Missing PodIP in operand kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal
    on node ip-10-0-21-63.us-west-1.compute.internal]" to "StaticPodsDegraded: pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal
    container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating:
    \nNodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal, Missing PodIP in operand kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal
    on node ip-10-0-21-63.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:23:00Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:23:00Z"
    name: kube-controller-manager-operator.179e33b97f627ff9
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "11733"
    uid: 5f6be2cf-4537-4bf8-8f46-7bf882e9dcbd
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:01Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:23:01Z"
  message: Created Pod/kube-controller-manager-guard-ip-10-0-21-63.us-west-1.compute.internal
    -n openshift-kube-controller-manager because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:23:01Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:23:01Z"
    name: kube-controller-manager-operator.179e33b9ae27702f
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "11748"
    uid: cfd8ea5e-68e5-4e68-9e3b-a6a965fa5ce1
  reason: PodCreated
  reportingComponent: kube-controller-manager-operator-guardcontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-guardcontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:01Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:23:01Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "StaticPodsDegraded: pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal
    container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating:
    \nNodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal, Missing PodIP in operand kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal
    on node ip-10-0-21-63.us-west-1.compute.internal]" to "StaticPodsDegraded: pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal
    container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating:
    \nNodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:23:01Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:23:01Z"
    name: kube-controller-manager-operator.179e33b9b000e3ab
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "11752"
    uid: 377d1824-ca30-4914-a081-a6760d5fdb96
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:06Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:23:06Z"
  message: 'Failed to update Pod/kube-controller-manager-guard-ip-10-0-21-63.us-west-1.compute.internal
    -n openshift-kube-controller-manager: Operation cannot be fulfilled on pods "kube-controller-manager-guard-ip-10-0-21-63.us-west-1.compute.internal":
    the object has been modified; please apply your changes to the latest version
    and try again'
  metadata:
    creationTimestamp: "2023-12-06T09:23:06Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:23:06Z"
    name: kube-controller-manager-operator.179e33bac031e0d6
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "11865"
    uid: 49e26086-1301-4a1a-aeec-7754dfc4f47e
  reason: PodUpdateFailed
  reportingComponent: kube-controller-manager-operator-guardcontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-guardcontroller
  type: Warning
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:06Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:23:06Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "StaticPodsDegraded: pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal
    container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating:
    \nNodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]" to "StaticPodsDegraded: pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal
    container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating:
    \nNodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Unable to
    apply pod kube-controller-manager-guard-ip-10-0-21-63.us-west-1.compute.internal
    changes: Operation cannot be fulfilled on pods \"kube-controller-manager-guard-ip-10-0-21-63.us-west-1.compute.internal\":
    the object has been modified; please apply your changes to the latest version
    and try again, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:23:06Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:23:06Z"
    name: kube-controller-manager-operator.179e33bac148e740
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "11868"
    uid: ebc0fa69-80ac-4b62-872e-3dc1c5abc663
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:10Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:23:10Z"
  message: Updated Pod/kube-controller-manager-guard-ip-10-0-21-63.us-west-1.compute.internal
    -n openshift-kube-controller-manager because it changed
  metadata:
    creationTimestamp: "2023-12-06T09:23:10Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:23:10Z"
    name: kube-controller-manager-operator.179e33bbaebd410b
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "11947"
    uid: c46dc384-5dfa-4436-ac34-a7be7bacf11f
  reason: PodUpdated
  reportingComponent: kube-controller-manager-operator-guardcontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-guardcontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:10Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:23:10Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "StaticPodsDegraded: pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal
    container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating:
    \nNodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Unable to
    apply pod kube-controller-manager-guard-ip-10-0-21-63.us-west-1.compute.internal
    changes: Operation cannot be fulfilled on pods \"kube-controller-manager-guard-ip-10-0-21-63.us-west-1.compute.internal\":
    the object has been modified; please apply your changes to the latest version
    and try again, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]"
    to "StaticPodsDegraded: pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal
    container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating:
    \nNodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:23:10Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:23:10Z"
    name: kube-controller-manager-operator.179e33bbafed5f2b
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "11952"
    uid: 825baa04-74ab-4029-aee1-8b45bad85700
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:13Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:23:13Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "StaticPodsDegraded: pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal
    container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating:
    \nNodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]" to "StaticPodsDegraded: pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal
    container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating:
    \nNodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:23:13Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:23:13Z"
    name: kube-controller-manager-operator.179e33bc5677a3bd
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "12000"
    uid: b9ff32e6-1a8c-4574-8dc9-9b9d73675043
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:17Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:23:17Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "StaticPodsDegraded: pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal
    container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating:
    \nNodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal]" to "NodeControllerDegraded:
    All master nodes are ready\nGuardControllerDegraded: [Missing operand on node
    ip-10-0-94-160.us-west-1.compute.internal, Missing operand on node ip-10-0-106-212.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:23:17Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:23:17Z"
    name: kube-controller-manager-operator.179e33bd5dcfd91f
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "12170"
    uid: d9d0ac99-5153-4d34-ac44-9d04e4e4bc08
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:18Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:23:18Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal]" to "NodeControllerDegraded:
    All master nodes are ready\nGuardControllerDegraded: [Missing operand on node
    ip-10-0-106-212.us-west-1.compute.internal, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:23:18Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:23:18Z"
    name: kube-controller-manager-operator.179e33bd80bcdf32
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "12192"
    uid: 0f1819e0-2fcf-4b97-8ed7-747bdaa55eba
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:31Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:23:31Z"
  message: Updated node "ip-10-0-21-63.us-west-1.compute.internal" from revision 0
    to 2 because static pod is ready
  metadata:
    creationTimestamp: "2023-12-06T09:23:31Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:23:31Z"
    name: kube-controller-manager-operator.179e33c0872981da
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "13476"
    uid: f0805c3b-10a0-42f3-a98d-68d6d2101ada
  reason: NodeCurrentRevisionChanged
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:31Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:23:31Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Progressing
    message changed from "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes
    have achieved new revision 2" to "NodeInstallerProgressing: 2 nodes are at revision
    0; 1 nodes are at revision 2",Available changed from False to True ("StaticPodsAvailable:
    1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2")'
  metadata:
    creationTimestamp: "2023-12-06T09:23:31Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:23:31Z"
    name: kube-controller-manager-operator.179e33c087e145bf
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "13483"
    uid: cfdab517-d8b2-4bc7-8fd1-0437f24a2d18
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:33Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:23:33Z"
  message: Updating node "ip-10-0-94-160.us-west-1.compute.internal" from revision
    0 to 2 because node ip-10-0-94-160.us-west-1.compute.internal static pod not found
  metadata:
    creationTimestamp: "2023-12-06T09:23:34Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:23:34Z"
    name: kube-controller-manager-operator.179e33c12ec422ab
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "14106"
    uid: 0ad20679-3324-4897-aeb5-fd27b1d6cf7b
  reason: NodeTargetRevisionChanged
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:36Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:23:36Z"
  message: Created Pod/installer-2-ip-10-0-94-160.us-west-1.compute.internal -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:23:36Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:23:36Z"
    name: kube-controller-manager-operator.179e33c1c8c6efc9
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "14361"
    uid: c0fefa1d-1c45-4f4e-913e-47435625a3c2
  reason: PodCreated
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:37Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:23:37Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded changed
    from False to True ("GuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]")'
  metadata:
    creationTimestamp: "2023-12-06T09:23:37Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:23:37Z"
    name: kube-controller-manager-operator.179e33c1fa524916
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "14449"
    uid: df6052b5-d05b-4ba3-a10b-e19c6abdeb55
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 2
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:53Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:24:04Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]" to "GuardControllerDegraded:
    [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal, Missing operand
    on node ip-10-0-106-212.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:23:53Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:24:04Z"
    name: kube-controller-manager-operator.179e33c5ca775253
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "15694"
    uid: 0600fc30-bdc0-4dba-9c29-5af47bb7f994
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 2
  eventTime: null
  firstTimestamp: "2023-12-06T09:23:55Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:24:07Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-94-160.us-west-1.compute.internal,
    Missing operand on node ip-10-0-106-212.us-west-1.compute.internal]" to "GuardControllerDegraded:
    [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand
    on node ip-10-0-94-160.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:23:55Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:24:07Z"
    name: kube-controller-manager-operator.179e33c635a802f8
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "15747"
    uid: a73eb036-0e55-4141-98d4-1f65d84d3b80
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:24:15Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:24:15Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]" to "GuardControllerDegraded:
    [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing PodIP
    in operand kube-controller-manager-ip-10-0-94-160.us-west-1.compute.internal on
    node ip-10-0-94-160.us-west-1.compute.internal]"'
  metadata:
    creationTimestamp: "2023-12-06T09:24:15Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:24:15Z"
    name: kube-controller-manager-operator.179e33cad1e686d3
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "15873"
    uid: b321dc31-d249-4308-8486-fedee1bf301b
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:24:15Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:24:15Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing PodIP in operand kube-controller-manager-ip-10-0-94-160.us-west-1.compute.internal
    on node ip-10-0-94-160.us-west-1.compute.internal]" to "GuardControllerDegraded:
    [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing PodIP
    in operand kube-controller-manager-ip-10-0-94-160.us-west-1.compute.internal on
    node ip-10-0-94-160.us-west-1.compute.internal]\nStaticPodsDegraded: pod/kube-controller-manager-ip-10-0-94-160.us-west-1.compute.internal
    container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-94-160.us-west-1.compute.internal container
    \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-94-160.us-west-1.compute.internal container
    \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-94-160.us-west-1.compute.internal container
    \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating:
    "'
  metadata:
    creationTimestamp: "2023-12-06T09:24:15Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:24:15Z"
    name: kube-controller-manager-operator.179e33caf60fcce5
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "15901"
    uid: 6c17f0e3-7328-4af6-823d-c1cc115c1468
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:24:21Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:24:21Z"
  message: Created Pod/kube-controller-manager-guard-ip-10-0-94-160.us-west-1.compute.internal
    -n openshift-kube-controller-manager because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:24:21Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:24:21Z"
    name: kube-controller-manager-operator.179e33cc4e81a55f
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "15974"
    uid: 97d96558-c6c8-4480-9b17-9cd922a6a507
  reason: PodCreated
  reportingComponent: kube-controller-manager-operator-guardcontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-guardcontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:24:21Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:24:21Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal,
    Missing PodIP in operand kube-controller-manager-ip-10-0-94-160.us-west-1.compute.internal
    on node ip-10-0-94-160.us-west-1.compute.internal]\nStaticPodsDegraded: pod/kube-controller-manager-ip-10-0-94-160.us-west-1.compute.internal
    container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-94-160.us-west-1.compute.internal container
    \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-94-160.us-west-1.compute.internal container
    \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-94-160.us-west-1.compute.internal container
    \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating:
    " to "GuardControllerDegraded: Missing operand on node ip-10-0-106-212.us-west-1.compute.internal\nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-94-160.us-west-1.compute.internal container
    \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-94-160.us-west-1.compute.internal container
    \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-94-160.us-west-1.compute.internal container
    \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-94-160.us-west-1.compute.internal container
    \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating:
    "'
  metadata:
    creationTimestamp: "2023-12-06T09:24:21Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:24:21Z"
    name: kube-controller-manager-operator.179e33cc4ff7925b
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "15980"
    uid: 1826ca4a-6baa-49d7-bd33-eb122864d9da
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:24:25Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:24:25Z"
  message: Updated Pod/kube-controller-manager-guard-ip-10-0-94-160.us-west-1.compute.internal
    -n openshift-kube-controller-manager because it changed
  metadata:
    creationTimestamp: "2023-12-06T09:24:25Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:24:25Z"
    name: kube-controller-manager-operator.179e33cd3d1445a7
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "16023"
    uid: 0f1e0932-6303-4b01-8c48-6b1c49c059f6
  reason: PodUpdated
  reportingComponent: kube-controller-manager-operator-guardcontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-guardcontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:24:35Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:24:35Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "GuardControllerDegraded: Missing operand on node ip-10-0-106-212.us-west-1.compute.internal\nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-94-160.us-west-1.compute.internal container
    \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-94-160.us-west-1.compute.internal container
    \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-94-160.us-west-1.compute.internal container
    \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-94-160.us-west-1.compute.internal container
    \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating:
    " to "GuardControllerDegraded: Missing operand on node ip-10-0-106-212.us-west-1.compute.internal"'
  metadata:
    creationTimestamp: "2023-12-06T09:24:35Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:24:35Z"
    name: kube-controller-manager-operator.179e33cf6e31a547
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "16113"
    uid: abec104b-76a5-4242-9247-f10ed608c458
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:24:46Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:24:46Z"
  message: Updated node "ip-10-0-94-160.us-west-1.compute.internal" from revision
    0 to 2 because static pod is ready
  metadata:
    creationTimestamp: "2023-12-06T09:24:46Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:24:46Z"
    name: kube-controller-manager-operator.179e33d2211f39e0
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "16255"
    uid: 8be85b23-4ed0-4c5d-8847-110b17f12fc7
  reason: NodeCurrentRevisionChanged
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:24:46Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:24:46Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Progressing
    message changed from "NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes
    are at revision 2" to "NodeInstallerProgressing: 1 nodes are at revision 0; 2
    nodes are at revision 2",Available message changed from "StaticPodsAvailable:
    1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2" to "StaticPodsAvailable:
    2 nodes are active; 1 nodes are at revision 0; 2 nodes are at revision 2"'
  metadata:
    creationTimestamp: "2023-12-06T09:24:46Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:24:46Z"
    name: kube-controller-manager-operator.179e33d2220cb8b8
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "16256"
    uid: 1c3d39d9-4d34-4e22-aee8-458516906ba6
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:24:50Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:24:50Z"
  message: Updating node "ip-10-0-106-212.us-west-1.compute.internal" from revision
    0 to 2 because node ip-10-0-106-212.us-west-1.compute.internal static pod not
    found
  metadata:
    creationTimestamp: "2023-12-06T09:24:50Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:24:50Z"
    name: kube-controller-manager-operator.179e33d2f77f0d41
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "16304"
    uid: 0ea745c5-d4a2-4ac7-a5a9-3dcfddb88ff9
  reason: NodeTargetRevisionChanged
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:24:55Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:24:55Z"
  message: Created Pod/installer-2-ip-10-0-106-212.us-west-1.compute.internal -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:24:55Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:24:55Z"
    name: kube-controller-manager-operator.179e33d4452f0ecf
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "16451"
    uid: e26503af-7149-41e4-9894-c3739c518989
  reason: PodCreated
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:25:36Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:25:36Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "GuardControllerDegraded: Missing operand on node ip-10-0-106-212.us-west-1.compute.internal"
    to "GuardControllerDegraded: Missing operand on node ip-10-0-106-212.us-west-1.compute.internal\nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-106-212.us-west-1.compute.internal container
    \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-106-212.us-west-1.compute.internal container
    \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-106-212.us-west-1.compute.internal container
    \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-106-212.us-west-1.compute.internal container
    \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating:
    "'
  metadata:
    creationTimestamp: "2023-12-06T09:25:36Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:25:36Z"
    name: kube-controller-manager-operator.179e33dda1942717
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "18104"
    uid: 339dc3ac-3bad-4043-b6fe-defb0a1ed47c
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:25:39Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:25:39Z"
  message: Created Pod/kube-controller-manager-guard-ip-10-0-106-212.us-west-1.compute.internal
    -n openshift-kube-controller-manager because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:25:39Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:25:39Z"
    name: kube-controller-manager-operator.179e33de7769bfa3
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "18204"
    uid: dcae18b5-dcf3-4fdc-9b0c-ff1e59310f04
  reason: PodCreated
  reportingComponent: kube-controller-manager-operator-guardcontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-guardcontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:25:40Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:25:40Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "GuardControllerDegraded: Missing operand on node ip-10-0-106-212.us-west-1.compute.internal\nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-106-212.us-west-1.compute.internal container
    \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-106-212.us-west-1.compute.internal container
    \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-106-212.us-west-1.compute.internal container
    \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-106-212.us-west-1.compute.internal container
    \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating:
    " to "GuardControllerDegraded: Missing operand on node ip-10-0-106-212.us-west-1.compute.internal"'
  metadata:
    creationTimestamp: "2023-12-06T09:25:40Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:25:40Z"
    name: kube-controller-manager-operator.179e33de8512dfec
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "18228"
    uid: 74f5c8b8-32d3-4ed7-8373-0759bd475058
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:25:45Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:25:45Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded changed
    from True to False ("NodeControllerDegraded: All master nodes are ready")'
  metadata:
    creationTimestamp: "2023-12-06T09:25:45Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:25:45Z"
    name: kube-controller-manager-operator.179e33dfba3434ce
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "18363"
    uid: f66fa6ef-793b-42e0-9eb9-2ffa48ed6a5a
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:25:48Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:25:48Z"
  message: Updated Pod/kube-controller-manager-guard-ip-10-0-106-212.us-west-1.compute.internal
    -n openshift-kube-controller-manager because it changed
  metadata:
    creationTimestamp: "2023-12-06T09:25:48Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:25:48Z"
    name: kube-controller-manager-operator.179e33e09bd8adfe
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "18443"
    uid: b0b758e4-517f-414e-bde4-aa9e5b9f6fe3
  reason: PodUpdated
  reportingComponent: kube-controller-manager-operator-guardcontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-guardcontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:25:55Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:25:55Z"
  message: Updated node "ip-10-0-106-212.us-west-1.compute.internal" from revision
    0 to 2 because static pod is ready
  metadata:
    creationTimestamp: "2023-12-06T09:25:55Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:25:55Z"
    name: kube-controller-manager-operator.179e33e20dbc5240
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "18576"
    uid: d7f4578e-017b-4094-a797-084dbd522b99
  reason: NodeCurrentRevisionChanged
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:25:55Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:25:55Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Progressing
    changed from True to False ("NodeInstallerProgressing: 3 nodes are at revision
    2"),Available message changed from "StaticPodsAvailable: 2 nodes are active; 1
    nodes are at revision 0; 2 nodes are at revision 2" to "StaticPodsAvailable: 3
    nodes are active; 3 nodes are at revision 2"'
  metadata:
    creationTimestamp: "2023-12-06T09:25:55Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:25:55Z"
    name: kube-controller-manager-operator.179e33e20e4d118c
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "18578"
    uid: 01a17339-6f83-4b16-842e-e7a683cabd61
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:33:16Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:33:16Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready" to "StaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager-cert-syncer\" is terminated: Error: ecret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/secrets?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    E1206 09:32:07.780557       1 reflector.go:147] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/secrets?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    W1206 09:32:42.818872       1 reflector.go:535] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/configmaps?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    E1206 09:32:42.818900       1 reflector.go:147] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/configmaps?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    W1206 09:33:00.020259       1 reflector.go:535] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/secrets?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    E1206 09:33:00.020298       1 reflector.go:147] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/secrets?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    F1206 09:33:14.145244       1 base_controller.go:96] unable to sync caches for
    CertSyncController\nStaticPodsDegraded: \nNodeControllerDegraded: All master nodes
    are ready"'
  metadata:
    creationTimestamp: "2023-12-06T09:33:16Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:33:16Z"
    name: kube-controller-manager-operator.179e3448c8124f73
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "26097"
    uid: c26cea92-146a-4708-a5a0-60ef29a8f58a
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:33:22Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:33:22Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "StaticPodsDegraded: pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal
    container \"kube-controller-manager-cert-syncer\" is terminated: Error: ecret:
    Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/secrets?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    E1206 09:32:07.780557       1 reflector.go:147] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/secrets?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    W1206 09:32:42.818872       1 reflector.go:535] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/configmaps?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    E1206 09:32:42.818900       1 reflector.go:147] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/configmaps?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    W1206 09:33:00.020259       1 reflector.go:535] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/secrets?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    E1206 09:33:00.020298       1 reflector.go:147] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/secrets?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    F1206 09:33:14.145244       1 base_controller.go:96] unable to sync caches for
    CertSyncController\nStaticPodsDegraded: \nNodeControllerDegraded: All master nodes
    are ready" to "NodeControllerDegraded: All master nodes are ready"'
  metadata:
    creationTimestamp: "2023-12-06T09:33:22Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:33:22Z"
    name: kube-controller-manager-operator.179e344a519a509d
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "26163"
    uid: 3c4dbfc6-bcc9-4066-88b2-0dfa735d131d
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:16Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:16Z"
  message: found expected kube-apiserver endpoints
  metadata:
    creationTimestamp: "2023-12-06T09:37:16Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:16Z"
    name: kube-controller-manager-operator.179e3480950c422d
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29091"
    uid: d26d65f4-1290-4e10-8046-5a4496d15827
  reason: SATokenSignerControllerOK
  reportingComponent: kube-controller-manager-operator-satokensignercontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-satokensignercontroller
  type: Normal
- apiVersion: v1
  count: 2
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:16Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:27Z"
  message: new revision 3 triggered by "required secret/localhost-recovery-client-token
    has changed"
  metadata:
    creationTimestamp: "2023-12-06T09:37:16Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:27Z"
    name: kube-controller-manager-operator.179e3480bda677d7
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29559"
    uid: f26ec3ef-cfb3-4e6c-91d9-01cab4c47174
  reason: RevisionTriggered
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:17Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:17Z"
  message: Created ConfigMap/revision-status-3 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:37:17Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:17Z"
    name: kube-controller-manager-operator.179e3480db30dbcd
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29329"
    uid: 8ff0f02d-764c-42c5-97e1-0614fd970a6a
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:17Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:17Z"
  message: Created ConfigMap/kube-controller-manager-pod-3 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:37:17Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:17Z"
    name: kube-controller-manager-operator.179e3480fed74394
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29378"
    uid: d718f129-744c-40f6-85ff-ea23e73a9e7c
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:18Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:18Z"
  message: Created ConfigMap/config-3 -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:37:18Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:18Z"
    name: kube-controller-manager-operator.179e34812e88fa79
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29411"
    uid: c0cf06ee-02d0-4f34-9fb0-4113ebff2ff1
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:19Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:19Z"
  message: Created Secret/next-service-account-private-key -n openshift-kube-controller-manager-operator
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:37:19Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:19Z"
    name: kube-controller-manager-operator.179e3481527b2bc5
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29420"
    uid: b711fd76-c6c6-4445-b860-e2964e3ed581
  reason: SecretCreated
  reportingComponent: kube-controller-manager-operator-satokensignercontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-satokensignercontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:19Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:19Z"
  message: Created ConfigMap/cluster-policy-controller-config-3 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:37:19Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:19Z"
    name: kube-controller-manager-operator.179e34816a2e29cd
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29428"
    uid: 2a5c34fe-ed74-41a5-b725-8c86a9744a71
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:20Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:20Z"
  message: |-
    Updated ConfigMap/sa-token-signing-certs -n openshift-config-managed:
    cause by changes in data.service-account-002.pub
  metadata:
    creationTimestamp: "2023-12-06T09:37:20Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:20Z"
    name: kube-controller-manager-operator.179e34818de159f6
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29440"
    uid: 842e0572-0301-4f4d-bdaa-e3114d10e3ea
  reason: ConfigMapUpdated
  reportingComponent: kube-controller-manager-operator-satokensignercontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-satokensignercontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:20Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:20Z"
  message: Created ConfigMap/controller-manager-kubeconfig-3 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:37:20Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:20Z"
    name: kube-controller-manager-operator.179e3481a5a80d5a
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29447"
    uid: e7089c13-7cd2-4a4e-9d1f-56fac6a04d7d
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:21Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:21Z"
  message: Created ConfigMap/cloud-config-3 -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:37:21Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:21Z"
    name: kube-controller-manager-operator.179e3481d55dc437
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29471"
    uid: f9bf0477-035e-44da-84b2-aac2f44cd7ca
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:22Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:22Z"
  message: Created ConfigMap/kube-controller-cert-syncer-kubeconfig-3 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:37:22Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:22Z"
    name: kube-controller-manager-operator.179e3482051d6210
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29483"
    uid: 5b2f3fb6-8598-483b-b7a1-27f8944bd441
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:23Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:23Z"
  message: Created ConfigMap/serviceaccount-ca-3 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:37:23Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:23Z"
    name: kube-controller-manager-operator.179e348234d71d31
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29502"
    uid: 57dea270-75f1-4aa6-90d7-41b80149c5e1
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:23Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:23Z"
  message: Created ConfigMap/service-ca-3 -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:37:23Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:23Z"
    name: kube-controller-manager-operator.179e348264a07a7e
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29519"
    uid: c4cdb4df-ab67-4abc-a7a4-7b24be38b1dc
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:24Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:24Z"
  message: Created ConfigMap/recycler-config-3 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:37:24Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:24Z"
    name: kube-controller-manager-operator.179e34829414137c
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29527"
    uid: 98d7b6e2-93a5-4c98-b768-a7de6426a6c7
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:25Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:25Z"
  message: Created Secret/service-account-private-key-3 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:37:25Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:25Z"
    name: kube-controller-manager-operator.179e3482c3e14b68
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29535"
    uid: 8f2bad33-78f0-456e-86e9-2efd81957523
  reason: SecretCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:26Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:26Z"
  message: Created Secret/serving-cert-3 -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:37:26Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:26Z"
    name: kube-controller-manager-operator.179e3482f3bff9de
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29546"
    uid: 2a1c1532-c740-4800-82d2-fb8cd3e5af0b
  reason: SecretCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:27Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:27Z"
  message: Created Secret/localhost-recovery-client-token-3 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:37:27Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:27Z"
    name: kube-controller-manager-operator.179e3483234aa05e
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29556"
    uid: a9a3bddd-cb2a-4556-bc4b-9fe13f87ee45
  reason: SecretCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:27Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:27Z"
  message: Revision 3 created because required secret/localhost-recovery-client-token
    has changed
  metadata:
    creationTimestamp: "2023-12-06T09:37:27Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:27Z"
    name: kube-controller-manager-operator.179e348323eb5487
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29558"
    uid: c0c81cdf-8202-40a6-8047-e225e83793b9
  reason: RevisionCreate
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:27Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:27Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready" to "NodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: conflicting latestAvailableRevision
    3"'
  metadata:
    creationTimestamp: "2023-12-06T09:37:27Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:27Z"
    name: kube-controller-manager-operator.179e348326bb61e6
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29563"
    uid: 8d9c1699-97f6-4c82-8158-a5ef61d10e87
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:27Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:27Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready\nRevisionControllerDegraded:
    conflicting latestAvailableRevision 3" to "NodeControllerDegraded: All master
    nodes are ready"'
  metadata:
    creationTimestamp: "2023-12-06T09:37:27Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:27Z"
    name: kube-controller-manager-operator.179e348328e77f66
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29565"
    uid: 27f665de-5216-42ca-a1ed-19ae4f7bf368
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:33Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:33Z"
  message: Updating node "ip-10-0-21-63.us-west-1.compute.internal" from revision
    2 to 3 because node ip-10-0-21-63.us-west-1.compute.internal with revision 2 is
    the oldest
  metadata:
    creationTimestamp: "2023-12-06T09:37:33Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:33Z"
    name: kube-controller-manager-operator.179e3484acff7b4f
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29660"
    uid: 47b6a04e-fd62-4678-990c-e18bd82e10d4
  reason: NodeTargetRevisionChanged
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:33Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:33Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Progressing
    changed from False to True ("NodeInstallerProgressing: 3 nodes are at revision
    2; 0 nodes have achieved new revision 3"),Available message changed from "StaticPodsAvailable:
    3 nodes are active; 3 nodes are at revision 2" to "StaticPodsAvailable: 3 nodes
    are active; 3 nodes are at revision 2; 0 nodes have achieved new revision 3"'
  metadata:
    creationTimestamp: "2023-12-06T09:37:33Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:33Z"
    name: kube-controller-manager-operator.179e3484adadce13
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29662"
    uid: ad42ba50-3fdb-439e-a39c-00d7cf829555
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:37:35Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:37:35Z"
  message: Created Pod/installer-3-ip-10-0-21-63.us-west-1.compute.internal -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:37:35Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:37:35Z"
    name: kube-controller-manager-operator.179e34852fe05aec
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "29681"
    uid: 1be0e2fa-d61f-4251-94ad-228f95ead2e2
  reason: PodCreated
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:38:34Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:38:34Z"
  message: Updated node "ip-10-0-21-63.us-west-1.compute.internal" from revision 2
    to 3 because static pod is ready
  metadata:
    creationTimestamp: "2023-12-06T09:38:34Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:38:34Z"
    name: kube-controller-manager-operator.179e3492edbe6b35
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "31144"
    uid: 37fd8e1c-6d7a-4b52-ad4b-f3cca812ddff
  reason: NodeCurrentRevisionChanged
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:38:34Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:38:34Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Progressing
    message changed from "NodeInstallerProgressing: 3 nodes are at revision 2; 0 nodes
    have achieved new revision 3" to "NodeInstallerProgressing: 2 nodes are at revision
    2; 1 nodes are at revision 3",Available message changed from "StaticPodsAvailable:
    3 nodes are active; 3 nodes are at revision 2; 0 nodes have achieved new revision
    3" to "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 2; 1 nodes
    are at revision 3"'
  metadata:
    creationTimestamp: "2023-12-06T09:38:34Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:38:34Z"
    name: kube-controller-manager-operator.179e3492ee8ac384
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "31146"
    uid: 98d9f79e-cd3f-4b2d-8019-565a7942939b
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:38:41Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:38:41Z"
  message: Updating node "ip-10-0-94-160.us-west-1.compute.internal" from revision
    2 to 3 because node ip-10-0-94-160.us-west-1.compute.internal with revision 2
    is the oldest
  metadata:
    creationTimestamp: "2023-12-06T09:38:41Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:38:41Z"
    name: kube-controller-manager-operator.179e34945f60f432
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "31271"
    uid: b2ff074e-023b-45e1-8230-a369e7d6d3de
  reason: NodeTargetRevisionChanged
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:38:48Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:38:48Z"
  message: Created Pod/installer-3-ip-10-0-94-160.us-west-1.compute.internal -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:38:48Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:38:48Z"
    name: kube-controller-manager-operator.179e34960be8f13b
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "31359"
    uid: b56f3c73-8dd2-4351-9525-fec4921f797d
  reason: PodCreated
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:40:46Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:40:46Z"
  message: Updated node "ip-10-0-94-160.us-west-1.compute.internal" from revision
    2 to 3 because static pod is ready
  metadata:
    creationTimestamp: "2023-12-06T09:40:46Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:40:46Z"
    name: kube-controller-manager-operator.179e34b1952a6a5a
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "35175"
    uid: 4c24bc2f-d5f0-484d-b800-0a76f8eb0801
  reason: NodeCurrentRevisionChanged
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:40:46Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:40:46Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Progressing
    message changed from "NodeInstallerProgressing: 2 nodes are at revision 2; 1 nodes
    are at revision 3" to "NodeInstallerProgressing: 1 nodes are at revision 2; 2
    nodes are at revision 3",Available message changed from "StaticPodsAvailable:
    3 nodes are active; 2 nodes are at revision 2; 1 nodes are at revision 3" to "StaticPodsAvailable:
    3 nodes are active; 1 nodes are at revision 2; 2 nodes are at revision 3"'
  metadata:
    creationTimestamp: "2023-12-06T09:40:46Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:40:46Z"
    name: kube-controller-manager-operator.179e34b195cacaa3
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "35177"
    uid: 21d56d55-8906-4046-b6c8-028df601d951
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:40:51Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:40:51Z"
  message: Updating node "ip-10-0-106-212.us-west-1.compute.internal" from revision
    2 to 3 because node ip-10-0-106-212.us-west-1.compute.internal with revision 2
    is the oldest
  metadata:
    creationTimestamp: "2023-12-06T09:40:51Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:40:51Z"
    name: kube-controller-manager-operator.179e34b2d712d304
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "35303"
    uid: fddb6f6c-68ee-4ee4-a8dc-afc5542a0628
  reason: NodeTargetRevisionChanged
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:40:53Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:40:53Z"
  message: Created Pod/installer-3-ip-10-0-106-212.us-west-1.compute.internal -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:40:53Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:40:53Z"
    name: kube-controller-manager-operator.179e34b34e2e1aa0
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "35331"
    uid: 0e7b1b17-7d2c-43aa-8eb6-8d0d1ce23c30
  reason: PodCreated
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 2
  eventTime: null
  firstTimestamp: "2023-12-06T09:41:42Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:41:58Z"
  message: new revision 4 triggered by "required secret/localhost-recovery-client-token
    has changed"
  metadata:
    creationTimestamp: "2023-12-06T09:41:42Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:41:58Z"
    name: kube-controller-manager-operator.179e34be9d25f6d6
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "36710"
    uid: 9481d92c-594b-44b5-9b0e-d35ebbccae66
  reason: RevisionTriggered
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:41:43Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:41:43Z"
  message: Created ConfigMap/revision-status-4 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:41:43Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:41:43Z"
    name: kube-controller-manager-operator.179e34bee452b449
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "36459"
    uid: 459c14b3-da18-4580-a46d-7f33a0efe6aa
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:41:44Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:41:44Z"
  message: Created ConfigMap/kube-controller-manager-pod-4 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:41:44Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:41:44Z"
    name: kube-controller-manager-operator.179e34bf2bf3e55f
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "36503"
    uid: 3be4fab9-6d30-4570-8402-c780e13c9316
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:41:46Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:41:46Z"
  message: Created ConfigMap/config-4 -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:41:46Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:41:46Z"
    name: kube-controller-manager-operator.179e34bf73e884d0
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "36526"
    uid: 56c65784-8f1b-488e-a57b-8ab9c98a1c23
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:41:47Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:41:47Z"
  message: Created ConfigMap/cluster-policy-controller-config-4 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:41:47Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:41:47Z"
    name: kube-controller-manager-operator.179e34bfc6e4f03a
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "36536"
    uid: 2ee95902-4a06-42d4-910f-75490b34564c
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:41:48Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:41:48Z"
  message: Created ConfigMap/controller-manager-kubeconfig-4 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:41:48Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:41:48Z"
    name: kube-controller-manager-operator.179e34c00e7e5c35
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "36549"
    uid: 3692f360-c6eb-48a2-b81d-63a62da85e36
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:41:49Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:41:49Z"
  message: Created ConfigMap/cloud-config-4 -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:41:49Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:41:49Z"
    name: kube-controller-manager-operator.179e34c055efde7c
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "36574"
    uid: 1648bd42-bada-4b37-8596-234260d995e0
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:41:51Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:41:51Z"
  message: Created ConfigMap/kube-controller-cert-syncer-kubeconfig-4 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:41:51Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:41:51Z"
    name: kube-controller-manager-operator.179e34c09dd06262
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "36606"
    uid: 8ac13290-3987-42b3-aea7-db67c0cbbdba
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:41:52Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:41:52Z"
  message: Created ConfigMap/serviceaccount-ca-4 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:41:52Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:41:52Z"
    name: kube-controller-manager-operator.179e34c0f10bac79
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "36619"
    uid: dedf7e22-ac24-42ea-8d18-b4ad2b55cb68
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:41:53Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:41:53Z"
  message: Created ConfigMap/service-ca-4 -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:41:53Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:41:53Z"
    name: kube-controller-manager-operator.179e34c1444d5a44
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "36634"
    uid: a60d468f-f737-4757-810a-53024e96f24d
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:41:54Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:41:54Z"
  message: Updated node "ip-10-0-106-212.us-west-1.compute.internal" from revision
    2 to 3 because static pod is ready
  metadata:
    creationTimestamp: "2023-12-06T09:41:54Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:41:54Z"
    name: kube-controller-manager-operator.179e34c15113e903
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "36637"
    uid: b1815119-1e21-4333-9df6-441bb0096f55
  reason: NodeCurrentRevisionChanged
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:41:54Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:41:54Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Progressing
    changed from True to False ("NodeInstallerProgressing: 3 nodes are at revision
    3"),Available message changed from "StaticPodsAvailable: 3 nodes are active; 1
    nodes are at revision 2; 2 nodes are at revision 3" to "StaticPodsAvailable: 3
    nodes are active; 3 nodes are at revision 3"'
  metadata:
    creationTimestamp: "2023-12-06T09:41:54Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:41:54Z"
    name: kube-controller-manager-operator.179e34c152179972
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "36638"
    uid: 112bd686-a049-484b-98ce-61376c093574
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:41:55Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:41:55Z"
  message: Created ConfigMap/recycler-config-4 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:41:55Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:41:55Z"
    name: kube-controller-manager-operator.179e34c18be031b6
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "36652"
    uid: b4be34c5-d9a5-4f9c-ba9f-e9e3b2c8f4f1
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:41:56Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:41:56Z"
  message: Created Secret/service-account-private-key-4 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:41:56Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:41:56Z"
    name: kube-controller-manager-operator.179e34c1df544a2c
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "36680"
    uid: 56056c8c-d148-4413-acfb-a5bb5a72d8d0
  reason: SecretCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:41:57Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:41:57Z"
  message: Created Secret/serving-cert-4 -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:41:57Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:41:57Z"
    name: kube-controller-manager-operator.179e34c2270f26a1
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "36689"
    uid: 91b725b4-f6af-4cd3-93c8-55f635623f5e
  reason: SecretCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:41:58Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:41:58Z"
  message: Created Secret/localhost-recovery-client-token-4 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:41:58Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:41:58Z"
    name: kube-controller-manager-operator.179e34c26e642b2d
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "36707"
    uid: f8d77db6-51ba-408b-a05f-11ac27b3085e
  reason: SecretCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:41:58Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:41:58Z"
  message: Revision 4 created because required secret/localhost-recovery-client-token
    has changed
  metadata:
    creationTimestamp: "2023-12-06T09:41:58Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:41:58Z"
    name: kube-controller-manager-operator.179e34c26f1e8b73
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "36709"
    uid: bbeacf55-ed21-45a1-baf4-f08935170010
  reason: RevisionCreate
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:41:58Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:41:58Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready" to "NodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: conflicting latestAvailableRevision
    4"'
  metadata:
    creationTimestamp: "2023-12-06T09:41:58Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:41:58Z"
    name: kube-controller-manager-operator.179e34c271b77809
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "36714"
    uid: d96edd2e-322a-4895-b777-d16059f89cd8
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:41:58Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:41:58Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready\nRevisionControllerDegraded:
    conflicting latestAvailableRevision 4" to "NodeControllerDegraded: All master
    nodes are ready"'
  metadata:
    creationTimestamp: "2023-12-06T09:41:58Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:41:58Z"
    name: kube-controller-manager-operator.179e34c272fac8a2
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "36717"
    uid: 8b57b1bc-4cf1-470e-ba43-3e6b2cc3c039
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:01Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:01Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Progressing
    changed from False to True ("NodeInstallerProgressing: 3 nodes are at revision
    3; 0 nodes have achieved new revision 4"),Available message changed from "StaticPodsAvailable:
    3 nodes are active; 3 nodes are at revision 3" to "StaticPodsAvailable: 3 nodes
    are active; 3 nodes are at revision 3; 0 nodes have achieved new revision 4"'
  metadata:
    creationTimestamp: "2023-12-06T09:42:01Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:01Z"
    name: kube-controller-manager-operator.179e34c30b54b889
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "36761"
    uid: a67fdcec-1f5c-4aa2-94eb-f276c3c00c32
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:07Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:07Z"
  message: Updating node "ip-10-0-21-63.us-west-1.compute.internal" from revision
    3 to 4 because node ip-10-0-21-63.us-west-1.compute.internal with revision 3 is
    the oldest
  metadata:
    creationTimestamp: "2023-12-06T09:42:07Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:07Z"
    name: kube-controller-manager-operator.179e34c4639e607b
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "36820"
    uid: 10cc02fc-5f1e-467e-91bb-ab52bf1a2648
  reason: NodeTargetRevisionChanged
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:09Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:09Z"
  message: Created Pod/installer-4-ip-10-0-21-63.us-west-1.compute.internal -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:42:09Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:09Z"
    name: kube-controller-manager-operator.179e34c4da5d2a92
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "36847"
    uid: 1757abe4-62b9-4b2f-8048-65c7deaefd1e
  reason: PodCreated
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:29Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:29Z"
  message: Updated Secret/service-account-private-key -n openshift-kube-controller-manager
    because it changed
  metadata:
    creationTimestamp: "2023-12-06T09:42:29Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:29Z"
    name: kube-controller-manager-operator.179e34c9809624bd
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "37090"
    uid: 110054c1-4f48-42c1-a8a7-c6d01c0bda8c
  reason: SecretUpdated
  reportingComponent: kube-controller-manager-operator-satokensignercontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-satokensignercontroller
  type: Normal
- apiVersion: v1
  count: 2
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:29Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:41Z"
  message: new revision 5 triggered by "required secret/service-account-private-key
    has changed"
  metadata:
    creationTimestamp: "2023-12-06T09:42:29Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:41Z"
    name: kube-controller-manager-operator.179e34c980b16a39
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "37306"
    uid: 8acce464-9f3f-4d96-9b9a-8564c117b825
  reason: RevisionTriggered
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:29Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:29Z"
  message: Created ConfigMap/revision-status-5 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:42:29Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:29Z"
    name: kube-controller-manager-operator.179e34c98275839d
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "37093"
    uid: 75b78e62-77eb-488e-af0e-b9f838b2127f
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:29Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:29Z"
  message: Created ConfigMap/kube-controller-manager-pod-5 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:42:29Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:29Z"
    name: kube-controller-manager-operator.179e34c99a55f23f
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "37104"
    uid: 5a68a353-25eb-4f41-b3d4-75cf82256c55
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:30Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:30Z"
  message: Created ConfigMap/config-5 -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:42:30Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:30Z"
    name: kube-controller-manager-operator.179e34c9c9f7b8bf
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "37122"
    uid: 783c23ce-ec68-407a-890b-94a36929618f
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:31Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:31Z"
  message: Created ConfigMap/cluster-policy-controller-config-5 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:42:31Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:31Z"
    name: kube-controller-manager-operator.179e34ca0595b0e3
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "37152"
    uid: a9d55ae8-ea6c-4f9e-bdbd-3c8f16eac88e
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:33Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:33Z"
  message: Created ConfigMap/controller-manager-kubeconfig-5 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:42:33Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:33Z"
    name: kube-controller-manager-operator.179e34ca64d7c021
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "37167"
    uid: 79262481-6e71-49b7-a4e2-82f0a8335793
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:34Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:34Z"
  message: Created ConfigMap/cloud-config-5 -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:42:34Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:34Z"
    name: kube-controller-manager-operator.179e34cac42b5a92
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "37183"
    uid: 2124f5d5-7859-4316-9475-8f9e7510ffe7
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:35Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:35Z"
  message: Created ConfigMap/kube-controller-cert-syncer-kubeconfig-5 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:42:35Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:35Z"
    name: kube-controller-manager-operator.179e34cafff53622
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "37192"
    uid: 0f04d0cf-6e95-403c-a350-ea9ad4607c7d
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:36Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:36Z"
  message: Created ConfigMap/serviceaccount-ca-5 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:42:36Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:36Z"
    name: kube-controller-manager-operator.179e34cb3b73d340
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "37214"
    uid: b333de3f-849b-4fa0-9ea2-3d584a492d12
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:37Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:37Z"
  message: Created ConfigMap/service-ca-5 -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:42:37Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:37Z"
    name: kube-controller-manager-operator.179e34cb77108113
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "37223"
    uid: 94a60b54-37bd-4112-8e6a-c94bb13c97f7
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:38Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:38Z"
  message: Created ConfigMap/recycler-config-5 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:42:38Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:38Z"
    name: kube-controller-manager-operator.179e34cbb2a58d4c
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "37238"
    uid: 667a1662-36e1-43c4-9454-ba1afc88a0a7
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:39Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:39Z"
  message: Created Secret/service-account-private-key-5 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:42:39Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:39Z"
    name: kube-controller-manager-operator.179e34cbe29f38d1
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "37258"
    uid: e302de24-dcb4-4691-997f-d3af2bdcffe6
  reason: SecretCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:40Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:40Z"
  message: Created Secret/serving-cert-5 -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:42:40Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:40Z"
    name: kube-controller-manager-operator.179e34cc122193fd
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "37279"
    uid: cead8a58-7730-4aca-8f9e-ae6692a08fb4
  reason: SecretCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:41Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:41Z"
  message: Created Secret/localhost-recovery-client-token-5 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:42:41Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:41Z"
    name: kube-controller-manager-operator.179e34cc42023a02
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "37300"
    uid: cc0cd02d-99a1-4732-8929-72be5479a5e0
  reason: SecretCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:41Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:41Z"
  message: Revision 5 created because required secret/service-account-private-key
    has changed
  metadata:
    creationTimestamp: "2023-12-06T09:42:41Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:41Z"
    name: kube-controller-manager-operator.179e34cc43eed224
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "37305"
    uid: 94542c4a-782d-40ed-b51c-0293d5c93443
  reason: RevisionCreate
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:41Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:41Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready" to "NodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: conflicting latestAvailableRevision
    5"'
  metadata:
    creationTimestamp: "2023-12-06T09:42:41Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:41Z"
    name: kube-controller-manager-operator.179e34cc48355820
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "37311"
    uid: f0391d00-d904-4fef-bbde-e22020fc60f8
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:41Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:41Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready\nRevisionControllerDegraded:
    conflicting latestAvailableRevision 5" to "NodeControllerDegraded: All master
    nodes are ready"'
  metadata:
    creationTimestamp: "2023-12-06T09:42:41Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:41Z"
    name: kube-controller-manager-operator.179e34cc4b403949
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "37313"
    uid: 07cb4258-2efd-4885-8ed2-7ea49ae17e7a
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:43Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:43Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Progressing
    message changed from "NodeInstallerProgressing: 3 nodes are at revision 3; 0 nodes
    have achieved new revision 4" to "NodeInstallerProgressing: 3 nodes are at revision
    3; 0 nodes have achieved new revision 5",Available message changed from "StaticPodsAvailable:
    3 nodes are active; 3 nodes are at revision 3; 0 nodes have achieved new revision
    4" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 3; 0 nodes
    have achieved new revision 5"'
  metadata:
    creationTimestamp: "2023-12-06T09:42:43Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:43Z"
    name: kube-controller-manager-operator.179e34ccbb78ea5c
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "37340"
    uid: c8814613-d4f1-449c-90f7-9194d8a39f3e
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:42:45Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:42:45Z"
  message: Created Pod/installer-5-ip-10-0-21-63.us-west-1.compute.internal -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:42:45Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:42:45Z"
    name: kube-controller-manager-operator.179e34cd602205b9
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "37373"
    uid: e39ebb36-1b4f-47a6-bec8-bc412be7f190
  reason: PodCreated
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:43:44Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:43:44Z"
  message: Updated node "ip-10-0-21-63.us-west-1.compute.internal" from revision 3
    to 5 because static pod is ready
  metadata:
    creationTimestamp: "2023-12-06T09:43:44Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:43:44Z"
    name: kube-controller-manager-operator.179e34db20feb054
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "38074"
    uid: 2831567e-7f89-4c07-a7d2-e0cdb6da96c4
  reason: NodeCurrentRevisionChanged
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:43:44Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:43:44Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Progressing
    message changed from "NodeInstallerProgressing: 3 nodes are at revision 3; 0 nodes
    have achieved new revision 5" to "NodeInstallerProgressing: 2 nodes are at revision
    3; 1 nodes are at revision 5",Available message changed from "StaticPodsAvailable:
    3 nodes are active; 3 nodes are at revision 3; 0 nodes have achieved new revision
    5" to "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 3; 1 nodes
    are at revision 5"'
  metadata:
    creationTimestamp: "2023-12-06T09:43:44Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:43:44Z"
    name: kube-controller-manager-operator.179e34db21cb4f2a
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "38076"
    uid: accdfa33-c247-4c17-b1c0-6924b0de1340
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:43:53Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:43:53Z"
  message: Updating node "ip-10-0-94-160.us-west-1.compute.internal" from revision
    3 to 5 because node ip-10-0-94-160.us-west-1.compute.internal with revision 3
    is the oldest
  metadata:
    creationTimestamp: "2023-12-06T09:43:53Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:43:53Z"
    name: kube-controller-manager-operator.179e34dd09d32e78
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "38176"
    uid: 34a66997-c092-491c-9d12-e50145e05b6f
  reason: NodeTargetRevisionChanged
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:43:55Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:43:55Z"
  message: Created Pod/installer-5-ip-10-0-94-160.us-west-1.compute.internal -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:43:55Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:43:55Z"
    name: kube-controller-manager-operator.179e34dd8ccb148d
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "38190"
    uid: cdd01c46-b83e-48ba-8ba6-37a3562159f8
  reason: PodCreated
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:44:54Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:44:54Z"
  message: Updated node "ip-10-0-94-160.us-west-1.compute.internal" from revision
    3 to 5 because static pod is ready
  metadata:
    creationTimestamp: "2023-12-06T09:44:54Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:44:54Z"
    name: kube-controller-manager-operator.179e34eb34e8517c
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "38846"
    uid: 77573713-fb2a-48c3-9dbc-27134b4631cf
  reason: NodeCurrentRevisionChanged
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:44:54Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:44:54Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Progressing
    message changed from "NodeInstallerProgressing: 2 nodes are at revision 3; 1 nodes
    are at revision 5" to "NodeInstallerProgressing: 1 nodes are at revision 3; 2
    nodes are at revision 5",Available message changed from "StaticPodsAvailable:
    3 nodes are active; 2 nodes are at revision 3; 1 nodes are at revision 5" to "StaticPodsAvailable:
    3 nodes are active; 1 nodes are at revision 3; 2 nodes are at revision 5"'
  metadata:
    creationTimestamp: "2023-12-06T09:44:54Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:44:54Z"
    name: kube-controller-manager-operator.179e34eb3610c269
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "38848"
    uid: f818ecec-638d-4ae1-b481-16053b0b7234
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:45:02Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:45:02Z"
  message: Updating node "ip-10-0-106-212.us-west-1.compute.internal" from revision
    3 to 5 because node ip-10-0-106-212.us-west-1.compute.internal with revision 3
    is the oldest
  metadata:
    creationTimestamp: "2023-12-06T09:45:02Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:45:02Z"
    name: kube-controller-manager-operator.179e34ed11c11b0a
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "38952"
    uid: 91afdf99-9ef6-4346-9028-a9e984575356
  reason: NodeTargetRevisionChanged
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:45:04Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:45:04Z"
  message: Created Pod/installer-5-ip-10-0-106-212.us-west-1.compute.internal -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:45:04Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:45:04Z"
    name: kube-controller-manager-operator.179e34ed94d68e6b
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "38981"
    uid: a0d30297-a7dc-4327-a56d-911943ba8208
  reason: PodCreated
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:45:51Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:45:51Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready" to "NodeControllerDegraded:
    All master nodes are ready\nGuardControllerDegraded: Missing PodIP in operand
    kube-controller-manager-ip-10-0-106-212.us-west-1.compute.internal on node ip-10-0-106-212.us-west-1.compute.internal"'
  metadata:
    creationTimestamp: "2023-12-06T09:45:51Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:45:51Z"
    name: kube-controller-manager-operator.179e34f887e1ccad
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "39564"
    uid: 279f8d44-af68-4572-98ac-3651e9b0474c
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:45:56Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:45:56Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded:
    Missing PodIP in operand kube-controller-manager-ip-10-0-106-212.us-west-1.compute.internal
    on node ip-10-0-106-212.us-west-1.compute.internal" to "NodeControllerDegraded:
    All master nodes are ready"'
  metadata:
    creationTimestamp: "2023-12-06T09:45:56Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:45:56Z"
    name: kube-controller-manager-operator.179e34f9c99a7ece
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "39625"
    uid: ad3ad177-424b-4bfd-ac22-1d3d92e6bec8
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:45:59Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:45:59Z"
  message: Updated node "ip-10-0-106-212.us-west-1.compute.internal" from revision
    3 to 5 because static pod is ready
  metadata:
    creationTimestamp: "2023-12-06T09:45:59Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:45:59Z"
    name: kube-controller-manager-operator.179e34fa8746d51a
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "39665"
    uid: a5371bac-4d53-42d6-90b7-eb13261ca76e
  reason: NodeCurrentRevisionChanged
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:45:59Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:45:59Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Progressing
    changed from True to False ("NodeInstallerProgressing: 3 nodes are at revision
    5"),Available message changed from "StaticPodsAvailable: 3 nodes are active; 1
    nodes are at revision 3; 2 nodes are at revision 5" to "StaticPodsAvailable: 3
    nodes are active; 3 nodes are at revision 5"'
  metadata:
    creationTimestamp: "2023-12-06T09:45:59Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:45:59Z"
    name: kube-controller-manager-operator.179e34fa88005591
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "39667"
    uid: 285c05ae-88c8-4688-9ef4-b1bf5c251891
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:53:35Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:53:35Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready" to "StaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal container
    \"kube-controller-manager-cert-syncer\" is terminated: Error: https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/configmaps?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    E1206 09:52:29.370451       1 reflector.go:147] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/configmaps?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    W1206 09:53:14.666024       1 reflector.go:535] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/configmaps?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    E1206 09:53:14.666063       1 reflector.go:147] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/configmaps?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    W1206 09:53:24.175098       1 reflector.go:535] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/secrets?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    E1206 09:53:24.175176       1 reflector.go:147] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/secrets?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    F1206 09:53:32.773964       1 base_controller.go:96] unable to sync caches for
    CertSyncController\nStaticPodsDegraded: \nNodeControllerDegraded: All master nodes
    are ready"'
  metadata:
    creationTimestamp: "2023-12-06T09:53:35Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:53:35Z"
    name: kube-controller-manager-operator.179e35649c952683
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "44255"
    uid: bcf6bef3-5567-4ae4-99e5-98e8747cf040
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:53:38Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:53:38Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "StaticPodsDegraded: pod/kube-controller-manager-ip-10-0-21-63.us-west-1.compute.internal
    container \"kube-controller-manager-cert-syncer\" is terminated: Error: https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/configmaps?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    E1206 09:52:29.370451       1 reflector.go:147] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/configmaps?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    W1206 09:53:14.666024       1 reflector.go:535] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/configmaps?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    E1206 09:53:14.666063       1 reflector.go:147] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/configmaps?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    W1206 09:53:24.175098       1 reflector.go:535] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/secrets?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    E1206 09:53:24.175176       1 reflector.go:147] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/secrets?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    F1206 09:53:32.773964       1 base_controller.go:96] unable to sync caches for
    CertSyncController\nStaticPodsDegraded: \nNodeControllerDegraded: All master nodes
    are ready" to "NodeControllerDegraded: All master nodes are ready"'
  metadata:
    creationTimestamp: "2023-12-06T09:53:38Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:53:38Z"
    name: kube-controller-manager-operator.179e35654f83cdba
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "44289"
    uid: f70787bc-4631-43d5-a7d8-fd0755ca0be9
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:54:44Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:54:44Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready" to "StaticPodsDegraded:
    pod/kube-controller-manager-ip-10-0-94-160.us-west-1.compute.internal container
    \"kube-controller-manager-cert-syncer\" is terminated: Error: ecret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/secrets?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    E1206 09:53:30.753982       1 reflector.go:147] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/secrets?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    W1206 09:54:08.076091       1 reflector.go:535] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/secrets?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    E1206 09:54:08.076144       1 reflector.go:147] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/secrets?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    W1206 09:54:16.676348       1 reflector.go:535] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/configmaps?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    E1206 09:54:16.676399       1 reflector.go:147] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/configmaps?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    F1206 09:54:42.267476       1 base_controller.go:96] unable to sync caches for
    CertSyncController\nStaticPodsDegraded: \nNodeControllerDegraded: All master nodes
    are ready"'
  metadata:
    creationTimestamp: "2023-12-06T09:54:44Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:54:44Z"
    name: kube-controller-manager-operator.179e3574a9471d37
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "45130"
    uid: 9dd7749e-212b-47f9-9b93-c1f1e7cd5d4f
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:54:47Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:54:47Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "StaticPodsDegraded: pod/kube-controller-manager-ip-10-0-94-160.us-west-1.compute.internal
    container \"kube-controller-manager-cert-syncer\" is terminated: Error: ecret:
    Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/secrets?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    E1206 09:53:30.753982       1 reflector.go:147] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/secrets?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    W1206 09:54:08.076091       1 reflector.go:535] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/secrets?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    E1206 09:54:08.076144       1 reflector.go:147] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/secrets?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    W1206 09:54:16.676348       1 reflector.go:535] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/configmaps?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    E1206 09:54:16.676399       1 reflector.go:147] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229:
    Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-controller-manager/configmaps?limit=500&resourceVersion=0\":
    tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded:
    F1206 09:54:42.267476       1 base_controller.go:96] unable to sync caches for
    CertSyncController\nStaticPodsDegraded: \nNodeControllerDegraded: All master nodes
    are ready" to "NodeControllerDegraded: All master nodes are ready"'
  metadata:
    creationTimestamp: "2023-12-06T09:54:47Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:54:47Z"
    name: kube-controller-manager-operator.179e35755c58b168
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "45165"
    uid: eeeb4893-b087-4f2c-a4ea-404a538a10b1
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 2
  eventTime: null
  firstTimestamp: "2023-12-06T09:57:26Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:57:37Z"
  message: new revision 6 triggered by "required secret/localhost-recovery-client-token
    has changed"
  metadata:
    creationTimestamp: "2023-12-06T09:57:26Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:57:37Z"
    name: kube-controller-manager-operator.179e359a6c717234
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "47522"
    uid: deaed2f2-b217-4128-8455-be9cd76e4c7e
  reason: RevisionTriggered
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:57:26Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:57:26Z"
  message: Created ConfigMap/revision-status-6 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:57:26Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:57:26Z"
    name: kube-controller-manager-operator.179e359a7fed1e4c
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "47244"
    uid: 0fb89a27-6f52-4457-ad2b-d78ae81c27a7
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:57:27Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:57:27Z"
  message: Created ConfigMap/kube-controller-manager-pod-6 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:57:27Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:57:27Z"
    name: kube-controller-manager-operator.179e359aa2d3b6ff
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "47310"
    uid: ef23ec5a-1d63-4fc2-bd1a-642aa44f1384
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:57:28Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:57:28Z"
  message: Created ConfigMap/config-6 -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:57:28Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:57:28Z"
    name: kube-controller-manager-operator.179e359ad293263c
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "47334"
    uid: fd4315f9-5d74-460d-b56f-83bb812030c5
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:57:29Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:57:29Z"
  message: Created ConfigMap/cluster-policy-controller-config-6 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:57:29Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:57:29Z"
    name: kube-controller-manager-operator.179e359b02322e3d
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "47347"
    uid: 5e709595-256f-4d06-b7e7-c667c09f69f4
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:57:29Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:57:29Z"
  message: Created ConfigMap/controller-manager-kubeconfig-6 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:57:29Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:57:29Z"
    name: kube-controller-manager-operator.179e359b31d48f19
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "47386"
    uid: 891b3ed6-e0e8-41cc-9092-5e54356f5086
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:57:30Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:57:30Z"
  message: Created ConfigMap/cloud-config-6 -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:57:30Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:57:30Z"
    name: kube-controller-manager-operator.179e359b6d801346
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "47414"
    uid: eb4d5513-6030-4b6f-bd4d-5780c863f1a4
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:57:31Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:57:31Z"
  message: Created ConfigMap/kube-controller-cert-syncer-kubeconfig-6 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:57:31Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:57:31Z"
    name: kube-controller-manager-operator.179e359b9d327309
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "47421"
    uid: 13ba67d3-24d1-4b2a-b1f7-0bc20b885d44
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:57:33Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:57:33Z"
  message: Created ConfigMap/serviceaccount-ca-6 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:57:33Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:57:33Z"
    name: kube-controller-manager-operator.179e359bf09d9a83
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "47448"
    uid: 718ebf3d-94e3-419d-b237-a26055a36124
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:57:34Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:57:34Z"
  message: Created ConfigMap/service-ca-6 -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:57:34Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:57:34Z"
    name: kube-controller-manager-operator.179e359c2c454c9e
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "47468"
    uid: 25919a5c-025e-4f5a-9c51-de5f6e84740a
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:57:35Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:57:35Z"
  message: Created ConfigMap/recycler-config-6 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:57:35Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:57:35Z"
    name: kube-controller-manager-operator.179e359c67ddc130
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "47476"
    uid: 143719c8-ec95-424f-a89d-e69dbda3e323
  reason: ConfigMapCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:57:35Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:57:35Z"
  message: Created Secret/service-account-private-key-6 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:57:35Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:57:35Z"
    name: kube-controller-manager-operator.179e359c979f8065
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "47488"
    uid: ab37c834-5dea-4596-b30a-efbf6112b012
  reason: SecretCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:57:36Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:57:36Z"
  message: Created Secret/serving-cert-6 -n openshift-kube-controller-manager because
    it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:57:36Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:57:36Z"
    name: kube-controller-manager-operator.179e359cc751be8f
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "47498"
    uid: 50e39a0f-0193-4a84-97b1-f8169c915bbc
  reason: SecretCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:57:37Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:57:37Z"
  message: Created Secret/localhost-recovery-client-token-6 -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:57:37Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:57:37Z"
    name: kube-controller-manager-operator.179e359cf7281c87
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "47519"
    uid: b2303697-1211-4d18-9e0d-c2e5b43c6d70
  reason: SecretCreated
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:57:37Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:57:37Z"
  message: Revision 6 created because required secret/localhost-recovery-client-token
    has changed
  metadata:
    creationTimestamp: "2023-12-06T09:57:37Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:57:37Z"
    name: kube-controller-manager-operator.179e359cf8068d26
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "47521"
    uid: b5e7050e-e975-4502-a7f9-0ed69a78af6a
  reason: RevisionCreate
  reportingComponent: kube-controller-manager-operator-revisioncontroller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-revisioncontroller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:57:37Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:57:37Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready" to "NodeControllerDegraded:
    All master nodes are ready\nRevisionControllerDegraded: conflicting latestAvailableRevision
    6"'
  metadata:
    creationTimestamp: "2023-12-06T09:57:37Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:57:37Z"
    name: kube-controller-manager-operator.179e359cfa2338a4
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "47526"
    uid: 4977840d-8922-444b-b4ea-2a4994a47561
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:57:37Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:57:37Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Degraded message
    changed from "NodeControllerDegraded: All master nodes are ready\nRevisionControllerDegraded:
    conflicting latestAvailableRevision 6" to "NodeControllerDegraded: All master
    nodes are ready"'
  metadata:
    creationTimestamp: "2023-12-06T09:57:37Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:57:37Z"
    name: kube-controller-manager-operator.179e359cfb684b14
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "47528"
    uid: cd583e3e-ade1-41a9-bc79-c38e9c8510f0
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:57:43Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:57:43Z"
  message: Updating node "ip-10-0-21-63.us-west-1.compute.internal" from revision
    5 to 6 because node ip-10-0-21-63.us-west-1.compute.internal with revision 5 is
    the oldest
  metadata:
    creationTimestamp: "2023-12-06T09:57:43Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:57:43Z"
    name: kube-controller-manager-operator.179e359e68fceb5d
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "47597"
    uid: 8e2f590c-30f5-4a56-a591-b79e4dd4a409
  reason: NodeTargetRevisionChanged
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:57:43Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:57:43Z"
  message: 'Status for clusteroperator/kube-controller-manager changed: Progressing
    changed from False to True ("NodeInstallerProgressing: 3 nodes are at revision
    5; 0 nodes have achieved new revision 6"),Available message changed from "StaticPodsAvailable:
    3 nodes are active; 3 nodes are at revision 5" to "StaticPodsAvailable: 3 nodes
    are active; 3 nodes are at revision 5; 0 nodes have achieved new revision 6"'
  metadata:
    creationTimestamp: "2023-12-06T09:57:43Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:57:43Z"
    name: kube-controller-manager-operator.179e359e6a5005c3
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "47599"
    uid: 6264dbb6-2aca-45c5-86ff-ae3c7fe084f6
  reason: OperatorStatusChanged
  reportingComponent: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-status-controller-statussyncer_kube-controller-manager
  type: Normal
- apiVersion: v1
  count: 1
  eventTime: null
  firstTimestamp: "2023-12-06T09:57:45Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: kube-controller-manager-operator
    namespace: openshift-kube-controller-manager-operator
    uid: e7f4b67a-a84e-43ce-b618-74fc67cca1c6
  kind: Event
  lastTimestamp: "2023-12-06T09:57:45Z"
  message: Created Pod/installer-6-ip-10-0-21-63.us-west-1.compute.internal -n openshift-kube-controller-manager
    because it was missing
  metadata:
    creationTimestamp: "2023-12-06T09:57:45Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:count: {}
        f:firstTimestamp: {}
        f:involvedObject: {}
        f:lastTimestamp: {}
        f:message: {}
        f:reason: {}
        f:reportingComponent: {}
        f:source:
          f:component: {}
        f:type: {}
      manager: cluster-kube-controller-manager-operator
      operation: Update
      time: "2023-12-06T09:57:45Z"
    name: kube-controller-manager-operator.179e359eec0097de
    namespace: openshift-kube-controller-manager-operator
    resourceVersion: "47618"
    uid: d226469c-e97d-4a08-88e7-e1bab34e81f7
  reason: PodCreated
  reportingComponent: kube-controller-manager-operator-installer-controller
  reportingInstance: ""
  source:
    component: kube-controller-manager-operator-installer-controller
  type: Normal
kind: EventList
metadata:
  resourceVersion: "47885"
