---
apiVersion: config.openshift.io/v1
kind: ClusterOperator
metadata:
  annotations:
    exclude.release.openshift.io/internal-openshift-hosted: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
  creationTimestamp: "2023-12-06T09:17:48Z"
  generation: 1
  managedFields:
  - apiVersion: config.openshift.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:exclude.release.openshift.io/internal-openshift-hosted: {}
          f:include.release.openshift.io/self-managed-high-availability: {}
          f:include.release.openshift.io/single-node-developer: {}
        f:ownerReferences:
          .: {}
          k:{"uid":"a7216904-f9ea-48f0-a63e-5356267ade74"}: {}
      f:spec: {}
    manager: cluster-version-operator
    operation: Update
    time: "2023-12-06T09:17:48Z"
  - apiVersion: config.openshift.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:status:
        .: {}
        f:extension: {}
    manager: cluster-version-operator
    operation: Update
    subresource: status
    time: "2023-12-06T09:17:48Z"
  - apiVersion: config.openshift.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:status:
        f:conditions: {}
        f:relatedObjects: {}
        f:versions: {}
    manager: cluster-kube-scheduler-operator
    operation: Update
    subresource: status
    time: "2023-12-06T09:57:39Z"
  name: kube-scheduler
  ownerReferences:
  - apiVersion: config.openshift.io/v1
    controller: true
    kind: ClusterVersion
    name: version
    uid: a7216904-f9ea-48f0-a63e-5356267ade74
  resourceVersion: "47543"
  uid: db7ee830-462a-4154-8abc-95965820d867
spec: {}
status:
  conditions:
  - lastTransitionTime: "2023-12-06T09:23:09Z"
    message: |-
      GuardControllerDegraded: [Missing operand on node ip-10-0-106-212.us-west-1.compute.internal, Missing operand on node ip-10-0-21-63.us-west-1.compute.internal, Missing operand on node ip-10-0-94-160.us-west-1.compute.internal]
      InstallerControllerDegraded: missing required resources: [configmaps: config-1,kube-scheduler-cert-syncer-kubeconfig-1,kube-scheduler-pod-1,scheduler-kubeconfig-1,serviceaccount-ca-1, secrets: localhost-recovery-client-token-1]
      RevisionControllerDegraded: ConfigMap "kube-scheduler-pod-2" is invalid: metadata.ownerReferences.uid: Invalid value: "": uid must not be empty
    reason: GuardController_SyncError::InstallerController_Error::RevisionController_ContentCreationError
    status: "True"
    type: Degraded
  - lastTransitionTime: "2023-12-06T09:21:16Z"
    message: 'NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved
      new revision 1'
    reason: NodeInstaller
    status: "True"
    type: Progressing
  - lastTransitionTime: "2023-12-06T09:21:16Z"
    message: 'StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0;
      0 nodes have achieved new revision 1'
    reason: StaticPods_ZeroNodesActive
    status: "False"
    type: Available
  - lastTransitionTime: "2023-12-06T09:21:10Z"
    message: All is well
    reason: AsExpected
    status: "True"
    type: Upgradeable
  extension: null
  relatedObjects:
  - group: operator.openshift.io
    name: cluster
    resource: kubeschedulers
  - group: config.openshift.io
    name: ""
    resource: schedulers
  - group: ""
    name: openshift-config
    resource: namespaces
  - group: ""
    name: openshift-config-managed
    resource: namespaces
  - group: ""
    name: openshift-kube-scheduler
    resource: namespaces
  - group: ""
    name: openshift-kube-scheduler-operator
    resource: namespaces
  - group: controlplane.operator.openshift.io
    name: ""
    namespace: openshift-kube-apiserver
    resource: podnetworkconnectivitychecks
  versions:
  - name: raw-internal
    version: 4.15.0-0.ci.test-2023-12-06-090630-ci-op-2j285qtr-latest
